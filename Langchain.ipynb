{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPptfde+7PfngjirorkQx9b",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ryyhan/RandomCodes/blob/main/Langchain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Basic Chat\n"
      ],
      "metadata": {
        "id": "WLjYXD-eDade"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade langchain langchain_core langchain_openai langchain_community chromadb openai tiktoken"
      ],
      "metadata": {
        "collapsed": true,
        "id": "3iYxf6v9DsFW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2MmHoZ6hApFa",
        "outputId": "d4f07b66-3889-4557-8eb3-4c11a6676f47"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Full result:\n",
            "content='81 divided by 9 equals 9.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 16, 'total_tokens': 26, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_dbaca60df0', 'id': 'chatcmpl-BXVSvUKgCw1lYZf9wHyiqGLlWezQL', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='run--b8f45d5b-e19e-4a7b-a1af-cc19dfb8cd3a-0' usage_metadata={'input_tokens': 16, 'output_tokens': 10, 'total_tokens': 26, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
            "Content only:\n",
            "81 divided by 9 equals 9.\n"
          ]
        }
      ],
      "source": [
        "#imports\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "#environment variables\n",
        "from google.colab import userdata\n",
        "api=userdata.get('OPEN_AI_KEY')\n",
        "\n",
        "\n",
        "# Create a ChatOpenAI model\n",
        "model = ChatOpenAI(model=\"gpt-4o-mini\",api_key=api)\n",
        "\n",
        "# Invoke the model with a message\n",
        "result = model.invoke(\"What is 81 divided by 9?\")\n",
        "print(\"Full result:\")\n",
        "print(result)\n",
        "print(\"Content only:\")\n",
        "print(result.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conversation"
      ],
      "metadata": {
        "id": "RlmNyBYVLjTp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "\n",
        "#environment variables\n",
        "from google.colab import userdata\n",
        "api=userdata.get('OPEN_AI_KEY')\n",
        "\n",
        "\n",
        "# Create a ChatOpenAI model\n",
        "model = ChatOpenAI(model=\"gpt-4o-mini\",api_key=api)\n",
        "\n",
        "\n",
        "messages = [\n",
        "    SystemMessage(content=\"Solve the following math problems\"),\n",
        "    HumanMessage(content=\"What is 81 divided by 9?\"),\n",
        "]\n",
        "\n",
        "# Invoke the model with messages\n",
        "result = model.invoke(messages)\n",
        "print(f\"Answer from AI (Single): {result.content}\")\n",
        "\n",
        "\n",
        "# AIMessage:\n",
        "#   Message from an AI.\n",
        "messages = [\n",
        "    SystemMessage(content=\"Solve the following math problems\"),\n",
        "    HumanMessage(content=\"What is 81 divided by 9?\"),\n",
        "    AIMessage(content=\"81 divided by 9 is 9.\"),\n",
        "    HumanMessage(content=\"What is 10 times 5?\"),\n",
        "]\n",
        "\n",
        "# Invoke the model with messages\n",
        "result = model.invoke(messages)\n",
        "print(f\"Answer from AI (Multiple): {result.content}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Omxka0qoDdD8",
        "outputId": "d4ab8568-6bd6-4f72-c2c5-bfa3dc420e4f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer from AI (Single): 81 divided by 9 is 9.\n",
            "Answer from AI (Multiple): 10 times 5 is 50.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Chatbot Conversations"
      ],
      "metadata": {
        "id": "baW8PAR8RtT2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.schema import AIMessage, HumanMessage, SystemMessage\n",
        "\n",
        "#environment variables\n",
        "from google.colab import userdata\n",
        "api=userdata.get('OPEN_AI_KEY')\n",
        "\n",
        "\n",
        "# Create a ChatOpenAI model\n",
        "model = ChatOpenAI(model=\"gpt-4o-mini\",api_key=api)\n",
        "\n",
        "\n",
        "chat_history = []  # Use a list to store messages\n",
        "\n",
        "# Set an initial system message (optional)\n",
        "system_message = SystemMessage(content=\"You are a helpful AI assistant.\")\n",
        "chat_history.append(system_message)  # Add system message to chat history\n",
        "\n",
        "# Chat loop\n",
        "while True:\n",
        "    query = input(\"You: \")\n",
        "    if query.lower() == \"exit\":\n",
        "        break\n",
        "    chat_history.append(HumanMessage(content=query))  # Add user message\n",
        "\n",
        "    # Get AI response using history\n",
        "    result = model.invoke(chat_history)\n",
        "    response = result.content\n",
        "    chat_history.append(AIMessage(content=response))  # Add AI message\n",
        "\n",
        "    print(f\"AI: {response}\")\n",
        "\n",
        "\n",
        "print(\"---- Message History ----\")\n",
        "print(chat_history)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dmchOq7YRs85",
        "outputId": "89696008-d1d8-4013-e71a-c734fb43222c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You: Who is GOAT in UFC?\n",
            "AI: The term \"GOAT\" stands for \"Greatest of All Time,\" and in the context of UFC (Ultimate Fighting Championship), it can vary depending on personal opinions and criteria. However, several fighters are commonly mentioned in debates about the greatest in UFC history:\n",
            "\n",
            "1. **Jon Jones** - Known for his remarkable skill set and fight IQ, Jones has held the UFC Light Heavyweight Championship multiple times and is often cited for his undefeated record in fights that counted (his only loss was due to a disqualification).\n",
            "\n",
            "2. **Georges St-Pierre (GSP)** - GSP is a two-division champion (welterweight and middleweight) and is celebrated for his well-rounded skill set, wrestling, and striking. He is considered one of the pioneers in MMA.\n",
            "\n",
            "3. **Khabib Nurmagomedov** - Retiring with an undefeated record of 29-0, Khabib is noted for his dominance in the lightweight division and his grappling prowess. He also holds victories over several former champions.\n",
            "\n",
            "4. **Anderson Silva** - A former middleweight champion, Silva had one of the longest title reigns in UFC history and is known for his striking ability and finishing fights.\n",
            "\n",
            "5. **Amanda Nunes** - Widely regarded as the greatest female fighter, Nunes has held titles in two divisions (bantamweight and featherweight) and has notable victories over many top fighters.\n",
            "\n",
            "The debate regarding the GOAT can be subjective and depends on various factors such as fight records, championship titles, skill sets, and impact on the sport.\n",
            "You: What about Makhachev?\n",
            "AI: Islam Makhachev is a prominent fighter in the UFC, especially known for his accomplishments in the lightweight division. As of my last update, he is the reigning UFC Lightweight Champion and has gained recognition for his wrestling ability, grappling skills, and striking improvements.\n",
            "\n",
            "Makhachev is a protégé of Khabib Nurmagomedov, and he has continued the legacy of Dagestani fighters in the UFC. He has an impressive fighting style characterized by a strong grappling base and effective striking, and he has showcased his skills against high-level competition.\n",
            "\n",
            "While Makhachev is often discussed as one of the top fighters in the lightweight division and has significant potential to be considered among the all-time greats, it's still early in his career relative to some of the established names often mentioned in discussions of the \"GOAT.\" If he continues to defend his title and accumulate notable victories, he could strengthen his case in future GOAT discussions. His career achievements and performances will be closely watched as he progresses.\n",
            "You: exit\n",
            "---- Message History ----\n",
            "[SystemMessage(content='You are a helpful AI assistant.', additional_kwargs={}, response_metadata={}), HumanMessage(content='Who is GOAT in UFC?', additional_kwargs={}, response_metadata={}), AIMessage(content='The term \"GOAT\" stands for \"Greatest of All Time,\" and in the context of UFC (Ultimate Fighting Championship), it can vary depending on personal opinions and criteria. However, several fighters are commonly mentioned in debates about the greatest in UFC history:\\n\\n1. **Jon Jones** - Known for his remarkable skill set and fight IQ, Jones has held the UFC Light Heavyweight Championship multiple times and is often cited for his undefeated record in fights that counted (his only loss was due to a disqualification).\\n\\n2. **Georges St-Pierre (GSP)** - GSP is a two-division champion (welterweight and middleweight) and is celebrated for his well-rounded skill set, wrestling, and striking. He is considered one of the pioneers in MMA.\\n\\n3. **Khabib Nurmagomedov** - Retiring with an undefeated record of 29-0, Khabib is noted for his dominance in the lightweight division and his grappling prowess. He also holds victories over several former champions.\\n\\n4. **Anderson Silva** - A former middleweight champion, Silva had one of the longest title reigns in UFC history and is known for his striking ability and finishing fights.\\n\\n5. **Amanda Nunes** - Widely regarded as the greatest female fighter, Nunes has held titles in two divisions (bantamweight and featherweight) and has notable victories over many top fighters.\\n\\nThe debate regarding the GOAT can be subjective and depends on various factors such as fight records, championship titles, skill sets, and impact on the sport.', additional_kwargs={}, response_metadata={}), HumanMessage(content='What about Makhachev?', additional_kwargs={}, response_metadata={}), AIMessage(content='Islam Makhachev is a prominent fighter in the UFC, especially known for his accomplishments in the lightweight division. As of my last update, he is the reigning UFC Lightweight Champion and has gained recognition for his wrestling ability, grappling skills, and striking improvements.\\n\\nMakhachev is a protégé of Khabib Nurmagomedov, and he has continued the legacy of Dagestani fighters in the UFC. He has an impressive fighting style characterized by a strong grappling base and effective striking, and he has showcased his skills against high-level competition.\\n\\nWhile Makhachev is often discussed as one of the top fighters in the lightweight division and has significant potential to be considered among the all-time greats, it\\'s still early in his career relative to some of the established names often mentioned in discussions of the \"GOAT.\" If he continues to defend his title and accumulate notable victories, he could strengthen his case in future GOAT discussions. His career achievements and performances will be closely watched as he progresses.', additional_kwargs={}, response_metadata={})]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example Source: https://python.langchain.com/v0.2/docs/integrations/memory/google_firestore/\n",
        "\n",
        "from google.cloud import firestore\n",
        "from langchain_google_firestore import FirestoreChatMessageHistory\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "\"\"\"\n",
        "Steps to replicate this example:\n",
        "1. Create a Firebase account\n",
        "2. Create a new Firebase project\n",
        "    - Copy the project ID\n",
        "3. Create a Firestore database in the Firebase project\n",
        "4. Install the Google Cloud CLI on your computer\n",
        "    - https://cloud.google.com/sdk/docs/install\n",
        "    - Authenticate the Google Cloud CLI with your Google account\n",
        "        - https://cloud.google.com/docs/authentication/provide-credentials-adc#local-dev\n",
        "    - Set your default project to the new Firebase project you created\n",
        "5. Enable the Firestore API in the Google Cloud Console:\n",
        "    - https://console.cloud.google.com/apis/enableflow?apiid=firestore.googleapis.com&project=crewai-automation\n",
        "\"\"\"\n",
        "\n",
        "#environment variables\n",
        "from google.colab import userdata\n",
        "api=userdata.get('OPEN_AI_KEY')\n",
        "\n",
        "\n",
        "# Setup Firebase Firestore\n",
        "PROJECT_ID = \"langchain-demo-abf48\"\n",
        "SESSION_ID = \"user_session_new\"  # This could be a username or a unique ID\n",
        "COLLECTION_NAME = \"chat_history\"\n",
        "\n",
        "# Initialize Firestore Client\n",
        "print(\"Initializing Firestore Client...\")\n",
        "client = firestore.Client(project=PROJECT_ID)\n",
        "\n",
        "# Initialize Firestore Chat Message History\n",
        "print(\"Initializing Firestore Chat Message History...\")\n",
        "chat_history = FirestoreChatMessageHistory(\n",
        "    session_id=SESSION_ID,\n",
        "    collection=COLLECTION_NAME,\n",
        "    client=client,\n",
        ")\n",
        "print(\"Chat History Initialized.\")\n",
        "print(\"Current Chat History:\", chat_history.messages)\n",
        "\n",
        "# Create a ChatOpenAI model\n",
        "model = ChatOpenAI(model=\"gpt-4o-mini\",api_key=api)\n",
        "\n",
        "print(\"Start chatting with the AI. Type 'exit' to quit.\")\n",
        "\n",
        "while True:\n",
        "    human_input = input(\"User: \")\n",
        "    if human_input.lower() == \"exit\":\n",
        "        break\n",
        "\n",
        "    chat_history.add_user_message(human_input)\n",
        "\n",
        "    ai_response = model.invoke(chat_history.messages)\n",
        "    chat_history.add_ai_message(ai_response.content)\n",
        "\n",
        "    print(f\"AI: {ai_response.content}\")"
      ],
      "metadata": {
        "id": "ifnx-HPIL8oe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "u8zseiV6hSUd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#prompting"
      ],
      "metadata": {
        "id": "X_Aq47ZAhT_g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "0TDhv16OhXgR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prompt Template Docs:\n",
        "#   https://python.langchain.com/v0.2/docs/concepts/#prompt-templateshttps://python.langchain.com/v0.2/docs/concepts/#prompt-templates\n",
        "\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain_core.messages import HumanMessage\n",
        "\n",
        "# # PART 1: Create a ChatPromptTemplate using a template string\n",
        "template = \"Tell me a joke about {topic}.\"\n",
        "prompt_template = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "print(\"-----Prompt from Template-----\")\n",
        "prompt = prompt_template.invoke({\"topic\": \"cats\"})\n",
        "print(prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WHe2WsKShTHI",
        "outputId": "549dda59-5777-4569-d668-5549d6e38c63"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----Prompt from Template-----\n",
            "messages=[HumanMessage(content='Tell me a joke about cats.', additional_kwargs={}, response_metadata={})]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# PART 2: Prompt with Multiple Placeholders\n",
        "template_multiple = \"\"\"You are a helpful assistant.\n",
        "Human: Tell me a {adjective} story about a {animal}.\n",
        "Assistant:\"\"\"\n",
        "prompt_multiple = ChatPromptTemplate.from_template(template_multiple)\n",
        "prompt = prompt_multiple.invoke({\"adjective\": \"funny\", \"animal\": \"panda\"})\n",
        "print(\"\\n----- Prompt with Multiple Placeholders -----\\n\")\n",
        "print(prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mQDYTH4DJkBx",
        "outputId": "1a544e9b-60ef-4580-a572-4859d1e52f9f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "----- Prompt with Multiple Placeholders -----\n",
            "\n",
            "messages=[HumanMessage(content='You are a helpful assistant.\\nHuman: Tell me a funny story about a panda.\\nAssistant:', additional_kwargs={}, response_metadata={})]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prompt Template with ChatModel"
      ],
      "metadata": {
        "id": "LIFjKTV5LlBR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "#environment variables\n",
        "from google.colab import userdata\n",
        "api=userdata.get('OPEN_AI_KEY')\n",
        "\n",
        "# Create a ChatOpenAI model\n",
        "model = ChatOpenAI(model=\"gpt-4o-mini\",api_key=api)\n",
        "\n",
        "# PART 1: Create a ChatPromptTemplate using a template string\n",
        "print(\"-----Prompt from Template-----\")\n",
        "template = \"Tell me a joke about {topic}.\"\n",
        "prompt_template = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "prompt = prompt_template.invoke({\"topic\": \"cats\"})\n",
        "result = model.invoke(prompt)\n",
        "print(prompt, result.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6TJlO_OTKyT7",
        "outputId": "31e53fb7-f05a-41fe-d706-792c34069bf3"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----Prompt from Template-----\n",
            "messages=[HumanMessage(content='Tell me a joke about cats.', additional_kwargs={}, response_metadata={})] Why was the cat sitting on the computer?\n",
            "\n",
            "Because it wanted to keep an eye on the mouse!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# PART 2: Prompt with Multiple Placeholders\n",
        "print(\"\\n----- Prompt with Multiple Placeholders -----\\n\")\n",
        "template_multiple = \"\"\"You are a helpful assistant.\n",
        "Human: Tell me a {adjective} short story about a {animal}.\n",
        "Assistant:\"\"\"\n",
        "prompt_multiple = ChatPromptTemplate.from_template(template_multiple)\n",
        "prompt = prompt_multiple.invoke({\"adjective\": \"funny\", \"animal\": \"cat\"})\n",
        "\n",
        "result = model.invoke(prompt)\n",
        "print(prompt,\"/n\\n\",result.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "usPIM502lUQy",
        "outputId": "9a6719ca-c397-4391-e062-12c6b654cf3d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "----- Prompt with Multiple Placeholders -----\n",
            "\n",
            "messages=[HumanMessage(content='You are a helpful assistant.\\nHuman: Tell me a funny short story about a cat.\\nAssistant:', additional_kwargs={}, response_metadata={})] /n\n",
            " Once upon a time in a cozy little house, there lived a cat named Mr. Whiskers. Mr. Whiskers was not your ordinary feline; he had a fondness for mischief and a talent for getting into trouble. \n",
            "\n",
            "One sunny afternoon, the owner of the house decided to bake a batch of delicious chocolate chip cookies. The delightful smell wafted through the air, eventually reaching Mr. Whiskers, who was napping lazily on the windowsill. His eyes popped open, and with his nose twitching, he sprang into action.\n",
            "\n",
            "Determined to taste the cookies, Mr. Whiskers concocted a plan. He stealthily crept into the kitchen, where the owner had left the bowl of cookie dough on the counter. With a leap that would make a gymnast proud, he bounded onto the counter. Just as he was about to take a swipe at the gooey dough, he lost his balance and knocked over the bowl!\n",
            "\n",
            "Cookie dough flew everywhere, like a scene from a food fight movie. It landed in the flour, stuck to the walls, and covered Mr. Whiskers from head to tail. For a moment, he froze in shock, his big green eyes wide open, resembling a doughy monster. \n",
            "\n",
            "At that same instant, the owner walked in, gasped, and burst into laughter. There stood Mr. Whiskers, a comical sight of flour and cookie dough, looking both guilty and bewildered. Instead of scolding him, the owner grabbed the camera for a quick snapshot of the \"Cookie Catastrophe.\"\n",
            "\n",
            "From that day on, Mr. Whiskers became a local legend. Every time the owner baked cookies, they would declare, \"It's time for the Cookie Cat to strike!\" He never quite managed to steal a cookie again, but he did become the star of many funny stories—and his picture was plastered on the fridge as \"The Great Cookie Bandit.\"\n",
            "\n",
            "And so, with his messy reputation secured, Mr. Whiskers continued his reign as the mischievous master of the house—always planning his next great adventure!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# PART 3: Prompt with System and Human Messages (Using Tuples)\n",
        "print(\"\\n----- Prompt with System and Human Messages (Tuple) -----\\n\")\n",
        "messages = [\n",
        "    (\"system\", \"You are a comedian who tells jokes about {topic}.\"),\n",
        "    (\"human\", \"Tell me {joke_count} jokes.\"),\n",
        "]\n",
        "prompt_template = ChatPromptTemplate.from_messages(messages)\n",
        "prompt = prompt_template.invoke({\"topic\": \"cats\", \"joke_count\": 3})\n",
        "result = model.invoke(prompt)\n",
        "print(prompt,\"\\n\\n\\n\")\n",
        "print(result.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ELOvkj-lgNO",
        "outputId": "ad66ff24-f921-477e-825b-e864d36fb84f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "----- Prompt with System and Human Messages (Tuple) -----\n",
            "\n",
            "messages=[SystemMessage(content='You are a comedian who tells jokes about cats.', additional_kwargs={}, response_metadata={}), HumanMessage(content='Tell me 3 jokes.', additional_kwargs={}, response_metadata={})] \n",
            "\n",
            "\n",
            "\n",
            "Sure, here are three cat-themed jokes for you:\n",
            "\n",
            "1. Why was the cat sitting on the computer?  \n",
            "   Because it wanted to keep an eye on the mouse!\n",
            "\n",
            "2. What do you call a pile of cats?  \n",
            "   A meowtain!\n",
            "\n",
            "3. Why don’t cats play poker in the jungle?  \n",
            "   Too many cheetahs!  \n",
            "\n",
            "Hope these made you chuckle!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "TEHfwCPSl5Qe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chains"
      ],
      "metadata": {
        "id": "YRVbxvN0l_AC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "JvK3j7mlmBGG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "#environment variables\n",
        "from google.colab import userdata\n",
        "api=userdata.get('OPEN_AI_KEY')\n",
        "\n",
        "# Create a ChatOpenAI model\n",
        "model = ChatOpenAI(model=\"gpt-4o-mini\",api_key=api)\n",
        "\n",
        "# Define prompt templates (no need for separate Runnable chains)\n",
        "prompt_template = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"You are a comedian who tells jokes about {topic}.\"),\n",
        "        (\"human\", \"Tell me {joke_count} jokes.\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Create the combined chain using LangChain Expression Language (LCEL)\n",
        "chain = prompt_template | model | StrOutputParser()\n",
        "\n",
        "# Run the chain\n",
        "result = chain.invoke({\"topic\": \"cats\", \"joke_count\": 3})\n",
        "\n",
        "# Output\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FWkafui9lrd-",
        "outputId": "5ea2d1d1-e34b-4bfd-f178-e7aae340a643"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sure! Here are three cat-themed jokes for you:\n",
            "\n",
            "1. Why did the cat sit on the computer?\n",
            "   Because it wanted to keep an eye on the mouse!\n",
            "\n",
            "2. What do you call a pile of cats?\n",
            "   A meow-tain!\n",
            "\n",
            "3. Why was the cat so good at video games?\n",
            "   Because it had nine lives and never gets game over!\n",
            "\n",
            "Hope these made you smile!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Behind the scenes"
      ],
      "metadata": {
        "id": "zJq9hZB0rS6B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.schema.runnable import RunnableLambda, RunnableSequence\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "#environment variables\n",
        "from google.colab import userdata\n",
        "api=userdata.get('OPEN_AI_KEY')\n",
        "\n",
        "# Create a ChatOpenAI model\n",
        "model = ChatOpenAI(model=\"gpt-4o-mini\",api_key=api)\n",
        "\n",
        "# Define prompt templates\n",
        "prompt_template = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"You are a comedian who tells jokes about {topic}.\"),\n",
        "        (\"human\", \"Tell me {joke_count} jokes.\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Create individual runnables (steps in the chain)\n",
        "format_prompt = RunnableLambda(lambda x: prompt_template.format_prompt(**x))\n",
        "invoke_model = RunnableLambda(lambda x: model.invoke(x.to_messages()))\n",
        "parse_output = RunnableLambda(lambda x: x.content)\n",
        "\n",
        "# Create the RunnableSequence (equivalent to the LCEL chain)\n",
        "chain = RunnableSequence(first=format_prompt, middle=[invoke_model], last=parse_output)\n",
        "\n",
        "# Run the chain\n",
        "response = chain.invoke({\"topic\": \"cats\", \"joke_count\": 3})\n",
        "\n",
        "# Output\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K9a2ZF8CrSrO",
        "outputId": "409e21ed-057c-4edf-f4a5-cbabb68e119e"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sure, here are three cat-themed jokes for you!\n",
            "\n",
            "1. Why was the cat sitting on the computer?\n",
            "   Because it wanted to keep an eye on the mouse!\n",
            "\n",
            "2. What do you call a pile of cats?\n",
            "   A meowtain!\n",
            "\n",
            "3. Why did the cat join Instagram?\n",
            "   Because it wanted to be a purr-fect influencer!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Custom runnnable/Chain\n"
      ],
      "metadata": {
        "id": "YXQedniysApn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.schema.output_parser import StrOutputParser\n",
        "from langchain.schema.runnable import RunnableLambda\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "#environment variables\n",
        "from google.colab import userdata\n",
        "api=userdata.get('OPEN_AI_KEY')\n",
        "\n",
        "# Create a ChatOpenAI model\n",
        "model = ChatOpenAI(model=\"gpt-4o-mini\",api_key=api)\n",
        "\n",
        "# Define prompt templates\n",
        "prompt_template = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"You are a comedian who tells jokes about {topic}.\"),\n",
        "        (\"human\", \"Tell me {joke_count} jokes.\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Define additional processing steps using RunnableLambda\n",
        "uppercase_output = RunnableLambda(lambda x: x.upper())\n",
        "count_words = RunnableLambda(lambda x: f\"Word count: {len(x.split())}\\n{x}\")\n",
        "\n",
        "# Create the combined chain using LangChain Expression Language (LCEL)\n",
        "chain = prompt_template | model | StrOutputParser() | uppercase_output | count_words\n",
        "\n",
        "# Run the chain\n",
        "result = chain.invoke({\"topic\": \"cats\", \"joke_count\": 3})\n",
        "\n",
        "# Output\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RXISKPg4o_RC",
        "outputId": "625a20ed-cb4f-4683-d0cd-e79ef1ea9f1f"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word count: 56\n",
            "SURE, HERE ARE THREE CAT-THEMED JOKES FOR YOU!\n",
            "\n",
            "1. WHY DID THE CAT SIT ON THE COMPUTER?  \n",
            "   BECAUSE IT WANTED TO KEEP AN EYE ON THE MOUSE!\n",
            "\n",
            "2. WHAT DO YOU CALL A PILE OF CATS?  \n",
            "   A MEOW-TAIN!\n",
            "\n",
            "3. WHY WAS THE CAT SO GOOD AT VIDEO GAMES?  \n",
            "   BECAUSE IT HAD NINE LIVES TO PLAY WITH!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Parallel Chain"
      ],
      "metadata": {
        "id": "sDyYqRgssKCZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.schema.output_parser import StrOutputParser\n",
        "from langchain.schema.runnable import RunnableParallel, RunnableLambda\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "#environment variables\n",
        "from google.colab import userdata\n",
        "api=userdata.get('OPEN_AI_KEY')\n",
        "\n",
        "# Create a ChatOpenAI model\n",
        "model = ChatOpenAI(model=\"gpt-4o-mini\",api_key=api)\n",
        "\n",
        "# Define prompt template\n",
        "prompt_template = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"You are an expert product reviewer.\"),\n",
        "        (\"human\", \"List the main features of the product {product_name}.\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "\n",
        "# Define pros analysis step\n",
        "def analyze_pros(features):\n",
        "    pros_template = ChatPromptTemplate.from_messages(\n",
        "        [\n",
        "            (\"system\", \"You are an expert product reviewer.\"),\n",
        "            (\n",
        "                \"human\",\n",
        "                \"Given these features: {features}, list the pros of these features.\",\n",
        "            ),\n",
        "        ]\n",
        "    )\n",
        "    return pros_template.format_prompt(features=features)\n",
        "\n",
        "\n",
        "# Define cons analysis step\n",
        "def analyze_cons(features):\n",
        "    cons_template = ChatPromptTemplate.from_messages(\n",
        "        [\n",
        "            (\"system\", \"You are an expert product reviewer.\"),\n",
        "            (\n",
        "                \"human\",\n",
        "                \"Given these features: {features}, list the cons of these features.\",\n",
        "            ),\n",
        "        ]\n",
        "    )\n",
        "    return cons_template.format_prompt(features=features)\n",
        "\n",
        "\n",
        "# Combine pros and cons into a final review\n",
        "def combine_pros_cons(pros, cons):\n",
        "    return f\"Pros:\\n{pros}\\n\\nCons:\\n{cons}\"\n",
        "\n",
        "\n",
        "# Simplify branches with LCEL\n",
        "pros_branch_chain = (\n",
        "    RunnableLambda(lambda x: analyze_pros(x)) | model | StrOutputParser()\n",
        ")\n",
        "\n",
        "cons_branch_chain = (\n",
        "    RunnableLambda(lambda x: analyze_cons(x)) | model | StrOutputParser()\n",
        ")\n",
        "\n",
        "# Create the combined chain using LangChain Expression Language (LCEL)\n",
        "chain = (\n",
        "    prompt_template\n",
        "    | model\n",
        "    | StrOutputParser()\n",
        "    | RunnableParallel(branches={\"pros\": pros_branch_chain, \"cons\": cons_branch_chain})\n",
        "    | RunnableLambda(lambda x: combine_pros_cons(x[\"branches\"][\"pros\"], x[\"branches\"][\"cons\"]))\n",
        ")\n",
        "\n",
        "# Run the chain\n",
        "result = chain.invoke({\"product_name\": \"MacBook Pro\"})\n",
        "\n",
        "# Output\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GoTXss2bsGS3",
        "outputId": "1aa35df8-3cb0-4a94-94fd-47a65a47745c"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pros:\n",
            "Based on the features of the latest MacBook Pro models (14-inch and 16-inch versions), here are the pros associated with each feature:\n",
            "\n",
            "1. **Apple Silicon**:\n",
            "   - **Performance and Efficiency**: The M1 Pro and M1 Max chips deliver incredible processing power and graphics performance, enabling seamless multitasking and efficient running of demanding applications. This results in faster rendering times for creative work, making it ideal for professionals.\n",
            "\n",
            "2. **Display**:\n",
            "   - **Visual Quality**: The Retina display with True Tone technology provides sharp visuals and accurate color reproduction, enhancing the overall viewing experience. ProMotion technology ensures smooth scrolling and responsiveness, while high brightness levels improve visibility in various lighting conditions.\n",
            "\n",
            "3. **Battery Life**:\n",
            "   - **Endurance**: Up to 17 hours of web browsing and 21 hours of video playback means users can work and enjoy media for extended periods without needing to recharge, making the laptop highly portable and convenient for travel.\n",
            "\n",
            "4. **Storage and Memory**:\n",
            "   - **Customization**: The extensive range of memory (16GB to 64GB) and storage options (512GB to 8TB) allows users to customize their MacBook Pro to meet their specific needs, whether for professional software, large multimedia files, or general use.\n",
            "\n",
            "5. **Ports**:\n",
            "   - **Versatile Connectivity**: Multiple Thunderbolt 4 ports, along with HDMI and an SDXC card slot, provide flexibility for connecting various peripherals, making it easier to work with external devices and media.\n",
            "\n",
            "6. **Audio**:\n",
            "   - **Immersive Sound**: The six-speaker sound system and high-quality microphone array deliver an exceptional audio experience for music, video, and conferencing applications, making it suitable for creative professionals and content creators.\n",
            "\n",
            "7. **Keyboard and Trackpad**:\n",
            "   - **User Experience**: The Magic Keyboard offers a comfortable typing experience, while Touch ID provides secure authentication easily. The large Force Touch trackpad enhances navigation and responsiveness, making it efficient for productivity tasks.\n",
            "\n",
            "8. **Operating System**:\n",
            "   - **Ecosystem Integration**: macOS provides a user-friendly interface, robust productivity features, and seamless integration with other Apple devices, ensuring a smooth workflow for users invested in the Apple ecosystem.\n",
            "\n",
            "9. **Design**:\n",
            "   - **Portability and Aesthetics**: The sleek design and durable aluminum chassis make the MacBook Pro not only visually appealing but also practical for on-the-go use, reinforcing its suitability for professionals who need a reliable machine.\n",
            "\n",
            "10. **FaceTime HD Camera**:\n",
            "    - **Enhanced Communication**: The improved 1080p camera ensures that video calls are clearer and more lifelike, making it great for remote work, virtual meetings, and socializing, especially in an era where video communication is prevalent.\n",
            "\n",
            "These pros highlight the MacBook Pro's capabilities as a high-performance laptop that meets the demands of a variety of users, particularly those in creative and professional fields.\n",
            "\n",
            "Cons:\n",
            "While the latest MacBook Pro models possess impressive features, there are some potential drawbacks and cons associated with them:\n",
            "\n",
            "1. **Apple Silicon**:\n",
            "   - **Limited Compatibility**: Although many applications have been optimized for Apple Silicon, some legacy software or specific programs may not be fully compatible, potentially leading to performance issues or a need for workarounds.\n",
            "\n",
            "2. **Display**:\n",
            "   - **Cost of Replacement**: The high-resolution Retina display is exceptional but could be expensive to repair or replace if damaged.\n",
            "   - **Glossy Finish**: The glossy screen may be reflective in bright environments, which can be distracting for some users.\n",
            "\n",
            "3. **Battery Life**:\n",
            "   - **Variable Performance**: The stated battery life can vary significantly based on usage, and heavy workloads could lead to reduced time between charges.\n",
            "   - **Non-Replaceable Battery**: In case the battery degrades over time, replacement can be costly and cumbersome compared to easily replaceable batteries in other laptops.\n",
            "\n",
            "4. **Storage and Memory**:\n",
            "   - **High Cost for Upgrades**: Configuring additional RAM or storage can significantly increase the price, and these components are not user-upgradable post-purchase.\n",
            "   - **Limited Entry-Level Configurations**: The base models come with lower storage and memory options that may not suffice for power users.\n",
            "\n",
            "5. **Ports**:\n",
            "   - **Dongle Dependency**: While the inclusion of various ports is beneficial, some users may still require adapters or dongles for legacy devices and peripherals, which can be inconvenient.\n",
            "   - **Limited Non-Thunderbolt Connectivity**: The exclusive use of Thunderbolt 4/USB-C ports may be limiting for users with older USB-A devices.\n",
            "\n",
            "6. **Audio**:\n",
            "   - **Mic Quality with Background Noise**: Despite being studio-quality, the built-in microphones can pick up background noise, which may not be ideal for recordings in noisy environments.\n",
            "\n",
            "7. **Keyboard and Trackpad**:\n",
            "   - **Magic Keyboard Sensitivity**: Some users may find the Magic Keyboard's key travel and response less satisfying compared to previous models with butterfly keyboard mechanisms.\n",
            "   - **Trackpad Size**: While large trackpads are generally great, some users may find it overly sensitive or prone to accidental taps.\n",
            "\n",
            "8. **Operating System**:\n",
            "   - **macOS Learning Curve**: Users who are accustomed to other operating systems, like Windows, may face a learning curve when transitioning to macOS.\n",
            "   - **Software Restrictions**: Certain software titles and applications that are only available on Windows may limit usage for some users.\n",
            "\n",
            "9. **Design**:\n",
            "   - **Durability vs. Weight**: While the aluminum chassis is sleek and durable, it can add weight compared to some competitor models, potentially affecting portability for on-the-go users.\n",
            "\n",
            "10. **FaceTime HD Camera**:\n",
            "    - **1080p Limitations**: Although improved, the camera may still struggle in low-light conditions compared to dedicated external webcams, which might be a downside for frequent video callers.\n",
            "\n",
            "Overall, while the MacBook Pro offers a range of superior features, potential buyers should consider these cons based on their specific use cases and personal preferences.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Branched Chains"
      ],
      "metadata": {
        "id": "0AEfkQue3CSV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.schema.output_parser import StrOutputParser\n",
        "from langchain.schema.runnable import RunnableBranch\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "#environment variables\n",
        "from google.colab import userdata\n",
        "api=userdata.get('OPEN_AI_KEY')\n",
        "\n",
        "# Create a ChatOpenAI model\n",
        "model = ChatOpenAI(model=\"gpt-4o-mini\",api_key=api)\n",
        "\n",
        "\n",
        "# Define prompt templates for different feedback types\n",
        "positive_feedback_template = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"You are a helpful assistant.\"),\n",
        "        (\"human\",\n",
        "         \"Generate a thank you note for this positive feedback: {feedback}.\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "negative_feedback_template = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"You are a helpful assistant.\"),\n",
        "        (\"human\",\n",
        "         \"Generate a response addressing this negative feedback: {feedback}.\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "neutral_feedback_template = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"You are a helpful assistant.\"),\n",
        "        (\n",
        "            \"human\",\n",
        "            \"Generate a request for more details for this neutral feedback: {feedback}.\",\n",
        "        ),\n",
        "    ]\n",
        ")\n",
        "\n",
        "escalate_feedback_template = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"You are a helpful assistant.\"),\n",
        "        (\n",
        "            \"human\",\n",
        "            \"Generate a message to escalate this feedback to a human agent: {feedback}.\",\n",
        "        ),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Define the feedback classification template\n",
        "classification_template = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"You are a helpful assistant.\"),\n",
        "        (\"human\",\n",
        "         \"Classify the sentiment of this feedback as positive, negative, neutral, or escalate: {feedback}.\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Define the runnable branches for handling feedback\n",
        "branches = RunnableBranch(\n",
        "    (\n",
        "        lambda x: \"positive\" in x,\n",
        "        positive_feedback_template | model | StrOutputParser()  # Positive feedback chain\n",
        "    ),\n",
        "    (\n",
        "        lambda x: \"negative\" in x,\n",
        "        negative_feedback_template | model | StrOutputParser()  # Negative feedback chain\n",
        "    ),\n",
        "    (\n",
        "        lambda x: \"neutral\" in x,\n",
        "        neutral_feedback_template | model | StrOutputParser()  # Neutral feedback chain\n",
        "    ),\n",
        "    escalate_feedback_template | model | StrOutputParser()\n",
        ")\n",
        "\n",
        "# Create the classification chain\n",
        "classification_chain = classification_template | model | StrOutputParser()\n",
        "\n",
        "# Combine classification and response generation into one chain\n",
        "chain = classification_chain | branches\n",
        "\n",
        "# Run the chain with an example review\n",
        "# Good review - \"The product is excellent. I really enjoyed using it and found it very helpful.\"\n",
        "# Bad review - \"The product is terrible. It broke after just one use and the quality is very poor.\"\n",
        "# Neutral review - \"The product is okay. It works as expected but nothing exceptional.\"\n",
        "# Default - \"I'm not sure about the product yet. Can you tell me more about its features and benefits?\"\n",
        "\n",
        "review = \"The product is terrible. It broke after just one use and the quality is very poor.\"\n",
        "result = chain.invoke({\"feedback\": review})\n",
        "\n",
        "# Output the result\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qtafj7wAtR0N",
        "outputId": "46b5fa86-50bc-4de5-c81d-590aa91dbf7f"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dear [Name],\n",
            "\n",
            "Thank you for taking the time to share your feedback. I’m sorry to hear that your experience did not meet your expectations. Your concerns are important to us, and we're committed to addressing them.\n",
            "\n",
            "We strive to provide the best possible experience for our customers, and it’s clear we fell short in this instance. I appreciate your insights as they help us identify areas for improvement. If you're open to it, I would love the opportunity to discuss this further and understand more about your experience. \n",
            "\n",
            "Please feel free to reach out directly at [contact information], and let’s find a way to make this right.\n",
            "\n",
            "Thank you again for your feedback.\n",
            "\n",
            "Best regards,\n",
            "\n",
            "[Your Name]  \n",
            "[Your Position]  \n",
            "[Your Company]  \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "xxzmDedfB67k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RAG - Ingestion"
      ],
      "metadata": {
        "id": "o5gOHpHKB-hB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "NQ05tvOmCCF8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pypdf\n",
        "import os\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain_community.document_loaders import TextLoader\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "\n",
        "# Define file path (Assuming it's in the current directory)\n",
        "file_path = '/content/us_bill_of_rights.txt'\n",
        "persistent_directory = '/content/db/chroma_db'\n",
        "\n",
        "# Check if the Chroma vector store already exists\n",
        "if not os.path.exists(persistent_directory):\n",
        "    print(\"Persistent directory does not exist. Initializing vector store...\")\n",
        "\n",
        "    # Ensure the text file exists\n",
        "    if not os.path.exists(file_path):\n",
        "        raise FileNotFoundError(\n",
        "            f\"The file {file_path} does not exist. Please check the path.\"\n",
        "        )\n",
        "\n",
        "    #Read the text content from the file\n",
        "    loader = TextLoader(file_path)\n",
        "    documents = loader.load()\n",
        "\n",
        "    # Split the document into chunks\n",
        "    text_splitter = CharacterTextSplitter(chunk_size=200, chunk_overlap=5)\n",
        "    docs = text_splitter.split_documents(documents)\n",
        "\n",
        "    # Display information about the split documents\n",
        "    print(\"\\n--- Document Chunks Information ---\")\n",
        "    print(f\"Number of document chunks: {len(docs)}\")\n",
        "    print(f\"Sample chunk:\\n{docs[0].page_content}\\n\")\n",
        "\n",
        "    # Create embeddings\n",
        "    print(\"\\n--- Creating embeddings ---\")\n",
        "    embeddings = OpenAIEmbeddings(\n",
        "        model=\"text-embedding-ada-002\", openai_api_key=api\n",
        "    )\n",
        "    print(\"\\n--- Finished creating embeddings ---\")\n",
        "\n",
        "    # Create the vector store and persist it automatically\n",
        "    print(\"\\n--- Creating vector store ---\")\n",
        "    db = Chroma.from_documents(\n",
        "        docs, embeddings, persist_directory=persistent_directory)\n",
        "    print(\"\\n--- Finished creating vector store ---\")\n",
        "\n",
        "else:\n",
        "    print(\"Vector store already exists. No need to initialize.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QyuoKqbO6B_S",
        "outputId": "c72431f8-f8a2-4355-eba0-fc552756a0d7"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pypdf in /usr/local/lib/python3.11/dist-packages (5.5.0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_text_splitters.base:Created a chunk of size 512, which is longer than the specified 200\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 272, which is longer than the specified 200\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 332, which is longer than the specified 200\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 588, which is longer than the specified 200\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 478, which is longer than the specified 200\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 271, which is longer than the specified 200\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1184, which is longer than the specified 200\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 354, which is longer than the specified 200\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 674, which is longer than the specified 200\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 581, which is longer than the specified 200\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1045, which is longer than the specified 200\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 563, which is longer than the specified 200\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 374, which is longer than the specified 200\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1132, which is longer than the specified 200\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 418, which is longer than the specified 200\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 287, which is longer than the specified 200\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 737, which is longer than the specified 200\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1674, which is longer than the specified 200\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 359, which is longer than the specified 200\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 634, which is longer than the specified 200\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 808, which is longer than the specified 200\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 800, which is longer than the specified 200\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 273, which is longer than the specified 200\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 483, which is longer than the specified 200\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 682, which is longer than the specified 200\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 297, which is longer than the specified 200\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 595, which is longer than the specified 200\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 447, which is longer than the specified 200\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 252, which is longer than the specified 200\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 424, which is longer than the specified 200\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 506, which is longer than the specified 200\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 245, which is longer than the specified 200\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 249, which is longer than the specified 200\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 271, which is longer than the specified 200\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 271, which is longer than the specified 200\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 252, which is longer than the specified 200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Persistent directory does not exist. Initializing vector store...\n",
            "\n",
            "--- Document Chunks Information ---\n",
            "Number of document chunks: 63\n",
            "Sample chunk:\n",
            "﻿The Project Gutenberg eBook of The United States Bill of Rights\n",
            "    \n",
            "This ebook is for the use of anyone anywhere in the United States and\n",
            "most other parts of the world at no cost and with almost no restrictions\n",
            "whatsoever. You may copy it, give it away or re-use it under the terms\n",
            "of the Project Gutenberg License included with this ebook or online\n",
            "at www.gutenberg.org. If you are not located in the United States,\n",
            "you will have to check the laws of the country where you are located\n",
            "before using this eBook.\n",
            "\n",
            "\n",
            "--- Creating embeddings ---\n",
            "\n",
            "--- Finished creating embeddings ---\n",
            "\n",
            "--- Creating vector store ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-16-d66df501ffe7>:37: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import OpenAIEmbeddings``.\n",
            "  embeddings = OpenAIEmbeddings(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Finished creating vector store ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Retrieval"
      ],
      "metadata": {
        "id": "9CTWnAA_yaku"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "# Define the persistent directory\n",
        "persistent_directory = '/content/db/chroma_db'\n",
        "\n",
        "# Define the embedding model\n",
        "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\",openai_api_key=api)\n",
        "\n",
        "# Load the existing vector store with the embedding function\n",
        "db = Chroma(persist_directory=persistent_directory,\n",
        "            embedding_function=embeddings)\n",
        "\n",
        "# Define the user's question\n",
        "query = \"The Project Gutenberg eBook of The United States Bill of Rights\"\n",
        "\n",
        "# Retrieve relevant documents based on the query\n",
        "retriever = db.as_retriever(\n",
        "    search_type=\"similarity_score_threshold\",\n",
        "    search_kwargs={\"k\": 3, \"score_threshold\": 0.1},\n",
        ")\n",
        "relevant_docs = retriever.invoke(query)\n",
        "\n",
        "# Display the relevant results with metadata\n",
        "print(\"\\n--- Relevant Documents ---\")\n",
        "for i, doc in enumerate(relevant_docs, 1):\n",
        "    print(f\"Document {i}:\\n{doc.page_content}\\n\")\n",
        "    if doc.metadata:\n",
        "        print(f\"Source: {doc.metadata.get('source', 'Unknown')}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iZ78bHc2_lJw",
        "outputId": "f0506864-e16a-4d9a-a53e-43d75eb3b751"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/langchain_core/vectorstores/base.py:1082: UserWarning: Relevance scores must be between 0 and 1, got [(Document(metadata={'source': '/content/us_bill_of_rights.txt'}, page_content='III\\nNo soldier shall, in time of peace be quartered in any house,\\nwithout the consent of the owner, nor in time of war,\\nbut in a manner to be prescribed by law.\\n\\n\\nIV'), -0.39944083908893546), (Document(metadata={'source': '/content/us_bill_of_rights.txt'}, page_content='1.F.'), -0.40599543267565275), (Document(metadata={'source': '/content/us_bill_of_rights.txt'}, page_content='In all criminal prosecutions, the accused shall enjoy the right to a\\nspeedy and public trial, by an impartial jury of the State and district\\nwherein the crime shall have been committed, which district shall have\\nbeen previously ascertained by law, and to be informed of the nature\\nand cause of the accusation; to be confronted with the witnesses against him;\\nto have compulsory process for obtaining witnesses in his favor,\\nand to have the assistance of counsel for his defense.'), -0.4061260036123395)]\n",
            "  self.vectorstore.similarity_search_with_relevance_scores(\n",
            "WARNING:langchain_core.vectorstores.base:No relevant docs were retrieved using the relevance score threshold 0.1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Relevant Documents ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## retrieval methods types"
      ],
      "metadata": {
        "id": "FHEGPGkGxYGK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "# Define the persistent directory\n",
        "persistent_directory = '/content/db/chroma_db'\n",
        "\n",
        "# Define the embedding model\n",
        "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\", api_key=api)\n",
        "\n",
        "# Load the existing vector store with the embedding function\n",
        "db = Chroma(persist_directory=persistent_directory,\n",
        "            embedding_function=embeddings)\n",
        "\n",
        "# Function to query a vector store with different search types and parameters\n",
        "def query_vector_store(\n",
        "    store_name, query, embedding_function, search_type, search_kwargs\n",
        "):\n",
        "    if os.path.exists(persistent_directory):\n",
        "        print(f\"\\n--- Querying the Vector Store {store_name} ---\")\n",
        "        db = Chroma(\n",
        "            persist_directory=persistent_directory,\n",
        "            embedding_function=embedding_function,\n",
        "        )\n",
        "        retriever = db.as_retriever(\n",
        "            search_type=search_type,\n",
        "            search_kwargs=search_kwargs,\n",
        "        )\n",
        "        relevant_docs = retriever.invoke(query)\n",
        "        # Display the relevant results with metadata\n",
        "        print(f\"\\n--- Relevant Documents for {store_name} ---\")\n",
        "        for i, doc in enumerate(relevant_docs, 1):\n",
        "            print(f\"Document {i}:\\n{doc.page_content}\\n\")\n",
        "            if doc.metadata:\n",
        "                print(f\"Source: {doc.metadata.get('source', 'Unknown')}\\n\")\n",
        "    else:\n",
        "        print(f\"Vector store {store_name} does not exist.\")\n",
        "\n",
        "\n",
        "# Define the user's question\n",
        "query = \"The Project Gutenberg eBook of The United States Bill of Rights?\"\n",
        "\n",
        "# Showcase different retrieval methods\n",
        "\n",
        "# 1. Similarity Search\n",
        "# This method retrieves documents based on vector similarity.\n",
        "# It finds the most similar documents to the query vector based on cosine similarity.\n",
        "# Use this when you want to retrieve the top k most similar documents.\n",
        "print(\"\\n--- Using Similarity Search ---\")\n",
        "query_vector_store(\"chroma_db_with_metadata\", query,\n",
        "                   embeddings, \"similarity\", {\"k\": 3})\n",
        "\n",
        "# 2. Max Marginal Relevance (MMR)\n",
        "# This method balances between selecting documents that are relevant to the query and diverse among themselves.\n",
        "# 'fetch_k' specifies the number of documents to initially fetch based on similarity.\n",
        "# 'lambda_mult' controls the diversity of the results: 1 for minimum diversity, 0 for maximum.\n",
        "# Use this when you want to avoid redundancy and retrieve diverse yet relevant documents.\n",
        "# Note: Relevance measures how closely documents match the query.\n",
        "# Note: Diversity ensures that the retrieved documents are not too similar to each other,\n",
        "#       providing a broader range of information.\n",
        "print(\"\\n--- Using Max Marginal Relevance (MMR) ---\")\n",
        "query_vector_store(\n",
        "    \"chroma_db_with_metadata\",\n",
        "    query,\n",
        "    embeddings,\n",
        "    \"mmr\",\n",
        "    {\"k\": 3, \"fetch_k\": 20, \"lambda_mult\": 0.5},\n",
        ")\n",
        "\n",
        "# 3. Similarity Score Threshold\n",
        "# This method retrieves documents that exceed a certain similarity score threshold.\n",
        "# 'score_threshold' sets the minimum similarity score a document must have to be considered relevant.\n",
        "# Use this when you want to ensure that only highly relevant documents are retrieved, filtering out less relevant ones.\n",
        "print(\"\\n--- Using Similarity Score Threshold ---\")\n",
        "query_vector_store(\n",
        "    \"chroma_db_with_metadata\",\n",
        "    query,\n",
        "    embeddings,\n",
        "    \"similarity_score_threshold\",\n",
        "    {\"k\": 3, \"score_threshold\": 0.1},\n",
        ")\n",
        "\n",
        "print(\"Querying demonstrations with different search types completed.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hFbqIEAdJOXL",
        "outputId": "c815f58e-a029-473f-ec8e-1d7d3847a9ab"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Using Similarity Search ---\n",
            "\n",
            "--- Querying the Vector Store chroma_db_with_metadata ---\n",
            "\n",
            "--- Relevant Documents for chroma_db_with_metadata ---\n",
            "Document 1:\n",
            "III\n",
            "No soldier shall, in time of peace be quartered in any house,\n",
            "without the consent of the owner, nor in time of war,\n",
            "but in a manner to be prescribed by law.\n",
            "\n",
            "\n",
            "IV\n",
            "\n",
            "Source: /content/us_bill_of_rights.txt\n",
            "\n",
            "Document 2:\n",
            "1.F.\n",
            "\n",
            "Source: /content/us_bill_of_rights.txt\n",
            "\n",
            "Document 3:\n",
            "In all criminal prosecutions, the accused shall enjoy the right to a\n",
            "speedy and public trial, by an impartial jury of the State and district\n",
            "wherein the crime shall have been committed, which district shall have\n",
            "been previously ascertained by law, and to be informed of the nature\n",
            "and cause of the accusation; to be confronted with the witnesses against him;\n",
            "to have compulsory process for obtaining witnesses in his favor,\n",
            "and to have the assistance of counsel for his defense.\n",
            "\n",
            "Source: /content/us_bill_of_rights.txt\n",
            "\n",
            "\n",
            "--- Using Max Marginal Relevance (MMR) ---\n",
            "\n",
            "--- Querying the Vector Store chroma_db_with_metadata ---\n",
            "\n",
            "--- Relevant Documents for chroma_db_with_metadata ---\n",
            "Document 1:\n",
            "III\n",
            "No soldier shall, in time of peace be quartered in any house,\n",
            "without the consent of the owner, nor in time of war,\n",
            "but in a manner to be prescribed by law.\n",
            "\n",
            "\n",
            "IV\n",
            "\n",
            "Source: /content/us_bill_of_rights.txt\n",
            "\n",
            "Document 2:\n",
            "V\n",
            "\n",
            "Source: /content/us_bill_of_rights.txt\n",
            "\n",
            "Document 3:\n",
            "• You pay a royalty fee of 20% of the gross profits you derive from\n",
            "        the use of Project Gutenberg™ works calculated using the method\n",
            "        you already use to calculate your applicable taxes. The fee is owed\n",
            "        to the owner of the Project Gutenberg™ trademark, but he has\n",
            "        agreed to donate royalties under this paragraph to the Project\n",
            "        Gutenberg Literary Archive Foundation. Royalty payments must be paid\n",
            "        within 60 days following each date on which you prepare (or are\n",
            "        legally required to prepare) your periodic tax returns. Royalty\n",
            "        payments should be clearly marked as such and sent to the Project\n",
            "        Gutenberg Literary Archive Foundation at the address specified in\n",
            "        Section 4, “Information about donations to the Project Gutenberg\n",
            "        Literary Archive Foundation.”\n",
            "    \n",
            "    • You provide a full refund of any money paid by a user who notifies\n",
            "        you in writing (or by e-mail) within 30 days of receipt that s/he\n",
            "        does not agree to the terms of the full Project Gutenberg™\n",
            "        License. You must require such a user to return or destroy all\n",
            "        copies of the works possessed in a physical medium and discontinue\n",
            "        all use of and all access to other copies of Project Gutenberg™\n",
            "        works.\n",
            "    \n",
            "    • You provide, in accordance with paragraph 1.F.3, a full refund of\n",
            "        any money paid for a work or a replacement copy, if a defect in the\n",
            "        electronic work is discovered and reported to you within 90 days of\n",
            "        receipt of the work.\n",
            "    \n",
            "    • You comply with all other terms of this agreement for free\n",
            "        distribution of Project Gutenberg™ works.\n",
            "\n",
            "Source: /content/us_bill_of_rights.txt\n",
            "\n",
            "\n",
            "--- Using Similarity Score Threshold ---\n",
            "\n",
            "--- Querying the Vector Store chroma_db_with_metadata ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/langchain_core/vectorstores/base.py:1082: UserWarning: Relevance scores must be between 0 and 1, got [(Document(metadata={'source': '/content/us_bill_of_rights.txt'}, page_content='III\\nNo soldier shall, in time of peace be quartered in any house,\\nwithout the consent of the owner, nor in time of war,\\nbut in a manner to be prescribed by law.\\n\\n\\nIV'), -0.3806597819239961), (Document(metadata={'source': '/content/us_bill_of_rights.txt'}, page_content='1.F.'), -0.3853126254122048), (Document(metadata={'source': '/content/us_bill_of_rights.txt'}, page_content='In all criminal prosecutions, the accused shall enjoy the right to a\\nspeedy and public trial, by an impartial jury of the State and district\\nwherein the crime shall have been committed, which district shall have\\nbeen previously ascertained by law, and to be informed of the nature\\nand cause of the accusation; to be confronted with the witnesses against him;\\nto have compulsory process for obtaining witnesses in his favor,\\nand to have the assistance of counsel for his defense.'), -0.385383010649218)]\n",
            "  self.vectorstore.similarity_search_with_relevance_scores(\n",
            "WARNING:langchain_core.vectorstores.base:No relevant docs were retrieved using the relevance score threshold 0.1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Relevant Documents for chroma_db_with_metadata ---\n",
            "Querying demonstrations with different search types completed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Naive RAG"
      ],
      "metadata": {
        "id": "iEAJAvuyxf6y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_core.messages import HumanMessage, SystemMessage\n",
        "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
        "\n",
        "\n",
        "# Define the embedding model\n",
        "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\", api_key=api)\n",
        "\n",
        "# Load the existing vector store with the embedding function\n",
        "db = Chroma(persist_directory=persistent_directory,\n",
        "            embedding_function=embeddings)\n",
        "\n",
        "# Define the user's question\n",
        "query = \"The Project Gutenberg eBook of The United States Bill of Rights?\"\n",
        "\n",
        "# Retrieve relevant documents based on the query\n",
        "retriever = db.as_retriever(\n",
        "    search_type=\"similarity\",\n",
        "    search_kwargs={\"k\": 1},\n",
        ")\n",
        "relevant_docs = retriever.invoke(query)\n",
        "\n",
        "# Display the relevant results with metadata\n",
        "print(\"\\n--- Relevant Documents ---\")\n",
        "for i, doc in enumerate(relevant_docs, 1):\n",
        "    print(f\"Document {i}:\\n{doc.page_content}\\n\")\n",
        "\n",
        "# Combine the query and the relevant document contents\n",
        "combined_input = (\n",
        "    \"Here are some documents that might help answer the question: \"\n",
        "    + query\n",
        "    + \"\\n\\nRelevant Documents:\\n\"\n",
        "    + \"\\n\\n\".join([doc.page_content for doc in relevant_docs])\n",
        "    + \"\\n\\nPlease provide an answer based only on the provided documents. If the answer is not found in the documents, respond with 'I'm not sure'.\"\n",
        ")\n",
        "\n",
        "# Create a ChatOpenAI model\n",
        "model = ChatOpenAI(model=\"gpt-4o-mini\", api_key=api)\n",
        "\n",
        "# Define the messages for the model\n",
        "messages = [\n",
        "    SystemMessage(content=\"You are a helpful assistant.\"),\n",
        "    HumanMessage(content=combined_input),\n",
        "]\n",
        "\n",
        "# Invoke the model with the combined input\n",
        "result = model.invoke(messages)\n",
        "\n",
        "# Display the full result and content only\n",
        "print(\"\\n--- Generated Response ---\")\n",
        "# print(\"Full result:\")\n",
        "# print(result)\n",
        "print(\"Content only:\")\n",
        "print(result.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kDx8dXO_xfpG",
        "outputId": "6950f95a-3f7c-493c-fdf1-3d9038b59d31"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Relevant Documents ---\n",
            "Document 1:\n",
            "III\n",
            "No soldier shall, in time of peace be quartered in any house,\n",
            "without the consent of the owner, nor in time of war,\n",
            "but in a manner to be prescribed by law.\n",
            "\n",
            "\n",
            "IV\n",
            "\n",
            "\n",
            "--- Generated Response ---\n",
            "Content only:\n",
            "The provided documents include the Third and Fourth Amendments of the United States Bill of Rights. The Third Amendment states that no soldier shall be quartered in any house during times of peace without the consent of the owner, and during times of war, it must be done in a manner prescribed by law. However, there is no content provided about the Fourth Amendment in your message.\n",
            "\n",
            "If you have any specific questions regarding these amendments, please let me know, and I'll do my best to answer based on the information from the documents. If you are looking for information regarding the Fourth Amendment or any other specific question, I can assist with that as well. If there is no information related to your question in the provided documents, I will respond with 'I'm not sure.'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conversational History RAG"
      ],
      "metadata": {
        "id": "x0ai04ImswTl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "from langchain.chains import create_history_aware_retriever, create_retrieval_chain\n",
        "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_core.messages import HumanMessage, SystemMessage\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
        "\n",
        "\n",
        "# Define the embedding model\n",
        "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\", api_key=api)\n",
        "\n",
        "# Load the existing vector store with the embedding function\n",
        "db = Chroma(persist_directory=persistent_directory, embedding_function=embeddings)\n",
        "\n",
        "# Create a retriever for querying the vector store\n",
        "# `search_type` specifies the type of search (e.g., similarity)\n",
        "# `search_kwargs` contains additional arguments for the search (e.g., number of results to return)\n",
        "retriever = db.as_retriever(\n",
        "    search_type=\"similarity\",\n",
        "    search_kwargs={\"k\": 3},\n",
        ")\n",
        "\n",
        "# Create a ChatOpenAI model\n",
        "llm = ChatOpenAI(model=\"gpt-4o\", api_key=api)\n",
        "\n",
        "# Contextualize question prompt\n",
        "# This system prompt helps the AI understand that it should reformulate the question\n",
        "# based on the chat history to make it a standalone question\n",
        "contextualize_q_system_prompt = (\n",
        "    \"Given a chat history and the latest user question \"\n",
        "    \"which might reference context in the chat history, \"\n",
        "    \"formulate a standalone question which can be understood \"\n",
        "    \"without the chat history. Do NOT answer the question, just \"\n",
        "    \"reformulate it if needed and otherwise return it as is.\"\n",
        ")\n",
        "\n",
        "# Create a prompt template for contextualizing questions\n",
        "contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", contextualize_q_system_prompt),\n",
        "        MessagesPlaceholder(\"chat_history\"),\n",
        "        (\"human\", \"{input}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Create a history-aware retriever\n",
        "# This uses the LLM to help reformulate the question based on chat history\n",
        "history_aware_retriever = create_history_aware_retriever(\n",
        "    llm, retriever, contextualize_q_prompt\n",
        ")\n",
        "\n",
        "# Answer question prompt\n",
        "# This system prompt helps the AI understand that it should provide concise answers\n",
        "# based on the retrieved context and indicates what to do if the answer is unknown\n",
        "qa_system_prompt = (\n",
        "    \"You are an assistant for question-answering tasks. Use \"\n",
        "    \"the following pieces of retrieved context to answer the \"\n",
        "    \"question. If you don't know the answer, just say that you \"\n",
        "    \"don't know. Use three sentences maximum and keep the answer \"\n",
        "    \"concise.\"\n",
        "    \"\\n\\n\"\n",
        "    \"{context}\"\n",
        ")\n",
        "\n",
        "# Create a prompt template for answering questions\n",
        "qa_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", qa_system_prompt),\n",
        "        MessagesPlaceholder(\"chat_history\"),\n",
        "        (\"human\", \"{input}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Create a chain to combine documents for question answering\n",
        "# `create_stuff_documents_chain` feeds all retrieved context into the LLM\n",
        "question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)\n",
        "\n",
        "# Create a retrieval chain that combines the history-aware retriever and the question answering chain\n",
        "rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)\n",
        "\n",
        "\n",
        "# Function to simulate a continual chat\n",
        "def continual_chat():\n",
        "    print(\"Start chatting with the AI! Type 'exit' to end the conversation.\")\n",
        "    chat_history = []  # Collect chat history here (a sequence of messages)\n",
        "    while True:\n",
        "        query = input(\"You: \")\n",
        "        if query.lower() == \"exit\":\n",
        "            break\n",
        "        # Process the user's query through the retrieval chain\n",
        "        result = rag_chain.invoke({\"input\": query, \"chat_history\": chat_history})\n",
        "        # Display the AI's response\n",
        "        print(f\"AI: {result['answer']}\")\n",
        "        # Update the chat history\n",
        "        chat_history.append(HumanMessage(content=query))\n",
        "        chat_history.append(SystemMessage(content=result[\"answer\"]))"
      ],
      "metadata": {
        "id": "qlWnpaz5smNd"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "yPzQlUe21fwc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tools and Agents"
      ],
      "metadata": {
        "id": "ZJM2KM1Wr3f8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "NS1iuJUNsGds"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Simple"
      ],
      "metadata": {
        "id": "stS_a4HSsNl7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import hub\n",
        "from langchain.agents import (\n",
        "    AgentExecutor,\n",
        "    create_react_agent,\n",
        ")\n",
        "from langchain_core.tools import Tool\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "\n",
        "# Define a very simple tool function that returns the current time\n",
        "def get_current_time(*args, **kwargs):\n",
        "    \"\"\"Returns the current time in H:MM AM/PM format.\"\"\"\n",
        "    import datetime\n",
        "\n",
        "    now = datetime.datetime.now()  # Get current time\n",
        "    return now.strftime(\"%I:%M %p\")  # Format time in H:MM AM/PM format\n",
        "\n",
        "\n",
        "# List of tools available to the agent\n",
        "tools = [\n",
        "    Tool(\n",
        "        name=\"Time\",  # Name of the tool\n",
        "        func=get_current_time,  # Function that the tool will execute\n",
        "        # Description of the tool\n",
        "        description=\"Useful for when you need to know the current time\",\n",
        "    ),\n",
        "]\n",
        "\n",
        "# Pull the prompt template from the hub\n",
        "# ReAct = Reason and Action\n",
        "# https://smith.langchain.com/hub/hwchase17/react\n",
        "prompt = hub.pull(\"hwchase17/react\")\n",
        "\n",
        "# Initialize a ChatOpenAI model\n",
        "llm = ChatOpenAI(\n",
        "    model=\"gpt-4o\", temperature=0, api_key = api,\n",
        ")\n",
        "\n",
        "# Create the ReAct agent using the create_react_agent function\n",
        "agent = create_react_agent(\n",
        "    llm=llm,\n",
        "    tools=tools,\n",
        "    prompt=prompt,\n",
        "    stop_sequence=True,\n",
        ")\n",
        "\n",
        "# Create an agent executor from the agent and tools\n",
        "agent_executor = AgentExecutor.from_agent_and_tools(\n",
        "    agent=agent,\n",
        "    tools=tools,\n",
        "    verbose=True,\n",
        ")\n",
        "\n",
        "# Run the agent with a test query\n",
        "response = agent_executor.invoke({\"input\": \"What time is it?\"})\n",
        "\n",
        "# Print the response from the agent\n",
        "print(\"response:\", response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x_ztTjhDrTvs",
        "outputId": "c0cc5da8-6ff5-400c-e0cb-14ef022980b2"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/langsmith/client.py:272: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mTo answer this question, I need to find out the current time. \n",
            "Action: Time\n",
            "Action Input: None\u001b[0m\u001b[36;1m\u001b[1;3m04:22 PM\u001b[0m\u001b[32;1m\u001b[1;3mI now know the final answer. \n",
            "Final Answer: The current time is 04:22 PM.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "response: {'input': 'What time is it?', 'output': 'The current time is 04:22 PM.'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Custom Tool"
      ],
      "metadata": {
        "id": "BNuhud2FsbvX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import hub\n",
        "from langchain.agents import AgentExecutor, create_tool_calling_agent\n",
        "from langchain.pydantic_v1 import BaseModel, Field\n",
        "from langchain_core.tools import StructuredTool, Tool\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "\n",
        "# Functions for the tools\n",
        "def greet_user(name: str) -> str:\n",
        "    \"\"\"Greets the user by name.\"\"\"\n",
        "    return f\"Hello, {name}!\"\n",
        "\n",
        "\n",
        "def reverse_string(text: str) -> str:\n",
        "    \"\"\"Reverses the given string.\"\"\"\n",
        "    return text[::-1]\n",
        "\n",
        "\n",
        "def concatenate_strings(a: str, b: str) -> str:\n",
        "    \"\"\"Concatenates two strings.\"\"\"\n",
        "    return a + b\n",
        "\n",
        "\n",
        "# Pydantic model for tool arguments\n",
        "class ConcatenateStringsArgs(BaseModel):\n",
        "    a: str = Field(description=\"First string\")\n",
        "    b: str = Field(description=\"Second string\")\n",
        "\n",
        "\n",
        "# Create tools using the Tool and StructuredTool constructor approach\n",
        "tools = [\n",
        "    # Use Tool for simpler functions with a single input parameter.\n",
        "    # This is straightforward and doesn't require an input schema.\n",
        "    Tool(\n",
        "        name=\"GreetUser\",  # Name of the tool\n",
        "        func=greet_user,  # Function to execute\n",
        "        description=\"Greets the user by name.\",  # Description of the tool\n",
        "    ),\n",
        "    # Use Tool for another simple function with a single input parameter.\n",
        "    Tool(\n",
        "        name=\"ReverseString\",  # Name of the tool\n",
        "        func=reverse_string,  # Function to execute\n",
        "        description=\"Reverses the given string.\",  # Description of the tool\n",
        "    ),\n",
        "    # Use StructuredTool for more complex functions that require multiple input parameters.\n",
        "    # StructuredTool allows us to define an input schema using Pydantic, ensuring proper validation and description.\n",
        "    StructuredTool.from_function(\n",
        "        func=concatenate_strings,  # Function to execute\n",
        "        name=\"ConcatenateStrings\",  # Name of the tool\n",
        "        description=\"Concatenates two strings.\",  # Description of the tool\n",
        "        args_schema=ConcatenateStringsArgs,  # Schema defining the tool's input arguments\n",
        "    ),\n",
        "]\n",
        "\n",
        "# Initialize a ChatOpenAI model\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\",api_key=api)\n",
        "\n",
        "# Pull the prompt template from the hub\n",
        "prompt = hub.pull(\"hwchase17/openai-tools-agent\")\n",
        "\n",
        "# Create the ReAct agent using the create_tool_calling_agent function\n",
        "agent = create_tool_calling_agent(\n",
        "    llm=llm,  # Language model to use\n",
        "    tools=tools,  # List of tools available to the agent\n",
        "    prompt=prompt,  # Prompt template to guide the agent's responses\n",
        ")\n",
        "\n",
        "# Create the agent executor\n",
        "agent_executor = AgentExecutor.from_agent_and_tools(\n",
        "    agent=agent,  # The agent to execute\n",
        "    tools=tools,  # List of tools available to the agent\n",
        "    verbose=True,  # Enable verbose logging\n",
        "    handle_parsing_errors=True,  # Handle parsing errors gracefully\n",
        ")\n",
        "\n",
        "# Test the agent with sample queries\n",
        "response = agent_executor.invoke({\"input\": \"Greet Alice\"})\n",
        "print(\"Response for 'Greet Alice':\", response)\n",
        "\n",
        "response = agent_executor.invoke({\"input\": \"Reverse the string 'hello'\"})\n",
        "print(\"Response for 'Reverse the string hello':\", response)\n",
        "\n",
        "response = agent_executor.invoke({\"input\": \"Concatenate 'hello' and 'world'\"})\n",
        "print(\"Response for 'Concatenate hello and world':\", response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3mUAL8PJsdDC",
        "outputId": "03f61d4e-cb32-4472-ec71-6caf7ea7c9f2"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py:3553: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
            "\n",
            "For example, replace imports like: `from langchain.pydantic_v1 import BaseModel`\n",
            "with: `from pydantic import BaseModel`\n",
            "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
            "\n",
            "  exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "/usr/local/lib/python3.11/dist-packages/langsmith/client.py:272: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m\n",
            "Invoking: `GreetUser` with `Alice`\n",
            "\n",
            "\n",
            "\u001b[0m\u001b[36;1m\u001b[1;3mHello, Alice!\u001b[0m\u001b[32;1m\u001b[1;3mHello, Alice!\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Response for 'Greet Alice': {'input': 'Greet Alice', 'output': 'Hello, Alice!'}\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m\n",
            "Invoking: `ReverseString` with `hello`\n",
            "\n",
            "\n",
            "\u001b[0m\u001b[33;1m\u001b[1;3molleh\u001b[0m\u001b[32;1m\u001b[1;3mThe reversed string is 'olleh'.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Response for 'Reverse the string hello': {'input': \"Reverse the string 'hello'\", 'output': \"The reversed string is 'olleh'.\"}\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m\n",
            "Invoking: `ConcatenateStrings` with `{'a': 'hello', 'b': 'world'}`\n",
            "\n",
            "\n",
            "\u001b[0m\u001b[38;5;200m\u001b[1;3mhelloworld\u001b[0m\u001b[32;1m\u001b[1;3mThe concatenated result of 'hello' and 'world' is **helloworld**.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Response for 'Concatenate hello and world': {'input': \"Concatenate 'hello' and 'world'\", 'output': \"The concatenated result of 'hello' and 'world' is **helloworld**.\"}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ReACT Agent"
      ],
      "metadata": {
        "id": "sr26DW0CseFE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wikipedia"
      ],
      "metadata": {
        "id": "y2coCEM5wkNU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import hub\n",
        "from langchain.agents import AgentExecutor, create_structured_chat_agent\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage\n",
        "from langchain_core.tools import Tool\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "\n",
        "# Define Tools\n",
        "def get_current_time(*args, **kwargs):\n",
        "    \"\"\"Returns the current time in H:MM AM/PM format.\"\"\"\n",
        "    import datetime\n",
        "\n",
        "    now = datetime.datetime.now()\n",
        "    return now.strftime(\"%I:%M %p\")\n",
        "\n",
        "\n",
        "def search_wikipedia(query):\n",
        "    \"\"\"Searches Wikipedia and returns the summary of the first result.\"\"\"\n",
        "    from wikipedia import summary\n",
        "\n",
        "    try:\n",
        "        # Limit to two sentences for brevity\n",
        "        return summary(query, sentences=2)\n",
        "    except:\n",
        "        return \"I couldn't find any information on that.\"\n",
        "\n",
        "\n",
        "# Define the tools that the agent can use\n",
        "tools = [\n",
        "    Tool(\n",
        "        name=\"Time\",\n",
        "        func=get_current_time,\n",
        "        description=\"Useful for when you need to know the current time.\",\n",
        "    ),\n",
        "    Tool(\n",
        "        name=\"Wikipedia\",\n",
        "        func=search_wikipedia,\n",
        "        description=\"Useful for when you need to know information about a topic.\",\n",
        "    ),\n",
        "]\n",
        "\n",
        "# Load the correct JSON Chat Prompt from the hub\n",
        "prompt = hub.pull(\"hwchase17/structured-chat-agent\")\n",
        "\n",
        "# Initialize a ChatOpenAI model\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\", api_key=api)\n",
        "\n",
        "# Create a structured Chat Agent with Conversation Buffer Memory\n",
        "# ConversationBufferMemory stores the conversation history, allowing the agent to maintain context across interactions\n",
        "memory = ConversationBufferMemory(\n",
        "    memory_key=\"chat_history\", return_messages=True)\n",
        "\n",
        "# create_structured_chat_agent initializes a chat agent designed to interact using a structured prompt and tools\n",
        "# It combines the language model (llm), tools, and prompt to create an interactive agent\n",
        "agent = create_structured_chat_agent(llm=llm, tools=tools, prompt=prompt)\n",
        "\n",
        "# AgentExecutor is responsible for managing the interaction between the user input, the agent, and the tools\n",
        "# It also handles memory to ensure context is maintained throughout the conversation\n",
        "agent_executor = AgentExecutor.from_agent_and_tools(\n",
        "    agent=agent,\n",
        "    tools=tools,\n",
        "    verbose=True,\n",
        "    memory=memory,  # Use the conversation memory to maintain context\n",
        "    handle_parsing_errors=True,  # Handle any parsing errors gracefully\n",
        ")\n",
        "\n",
        "# Initial system message to set the context for the chat\n",
        "# SystemMessage is used to define a message from the system to the agent, setting initial instructions or context\n",
        "initial_message = \"You are an AI assistant that can provide helpful answers using available tools.\\nIf you are unable to answer, you can use the following tools: Time and Wikipedia.\"\n",
        "memory.chat_memory.add_message(SystemMessage(content=initial_message))\n",
        "\n",
        "# Chat Loop to interact with the user\n",
        "while True:\n",
        "    user_input = input(\"User: \")\n",
        "    if user_input.lower() == \"exit\":\n",
        "        break\n",
        "\n",
        "    # Add the user's message to the conversation memory\n",
        "    memory.chat_memory.add_message(HumanMessage(content=user_input))\n",
        "\n",
        "    # Invoke the agent with the user input and the current chat history\n",
        "    response = agent_executor.invoke({\"input\": user_input})\n",
        "    print(\"Bot:\", response[\"output\"])\n",
        "\n",
        "    # Add the agent's response to the conversation memory\n",
        "    memory.chat_memory.add_message(AIMessage(content=response[\"output\"]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nWvacmyEsrDU",
        "outputId": "a2aad3aa-9b59-4954-f0b5-270cc9527e2a"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/langsmith/client.py:272: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
            "  warnings.warn(\n",
            "<ipython-input-33-de727de38326>:51: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
            "  memory = ConversationBufferMemory(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "User: How are you?\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m```\n",
            "{\n",
            "  \"action\": \"Final Answer\",\n",
            "  \"action_input\": \"I'm just a program, but I'm here to help you! How can I assist you today?\"\n",
            "}\n",
            "```\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Bot: I'm just a program, but I'm here to help you! How can I assist you today?\n",
            "User: Do you know Islam Makhachev is?\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mThought: I need to find information about Islam Makhachev to respond accurately.\n",
            "Action:\n",
            "```\n",
            "{\n",
            "  \"action\": \"Wikipedia\",\n",
            "  \"action_input\": {\"type\": \"Islam Makhachev\"}\n",
            "}\n",
            "```\u001b[0m\u001b[33;1m\u001b[1;3mIslam Ramazanovich Makhachev (Russian: Ислам Рамазанович Махачев; born 27 October 1991) is a Russian professional mixed martial artist and former sambo competitor. He currently competes in the Lightweight division of the Ultimate Fighting Championship (UFC), where he is the current UFC Lightweight Champion.\u001b[0m\u001b[32;1m\u001b[1;3mThought: I have the information about Islam Makhachev and can now provide a summary.\n",
            "Action:\n",
            "```\n",
            "{\n",
            "  \"action\": \"Final Answer\",\n",
            "  \"action_input\": \"Islam Makhachev is a Russian professional mixed martial artist and former sambo competitor, born on October 27, 1991. He competes in the Lightweight division of the Ultimate Fighting Championship (UFC) and is the current UFC Lightweight Champion.\"\n",
            "}\n",
            "```\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Bot: Islam Makhachev is a Russian professional mixed martial artist and former sambo competitor, born on October 27, 1991. He competes in the Lightweight division of the Ultimate Fighting Championship (UFC) and is the current UFC Lightweight Champion.\n",
            "User: how many title defenses does islam have?\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mThought: I need to find information regarding Islam Makhachev's title defenses in the UFC. \n",
            "Action:\n",
            "```\n",
            "{\n",
            "  \"action\": \"Wikipedia\",\n",
            "  \"action_input\": {\"type\": \"Islam Makhachev title defenses\"}\n",
            "}\n",
            "```\u001b[0m\u001b[33;1m\u001b[1;3mIslam Ramazanovich Makhachev (Russian: Ислам Рамазанович Махачев; born 27 October 1991) is a Russian professional mixed martial artist and former sambo competitor. He currently competes in the Lightweight division of the Ultimate Fighting Championship (UFC), where he is the current UFC Lightweight Champion.\u001b[0m\u001b[32;1m\u001b[1;3mThought: I need to find the specific number of title defenses Islam Makhachev has as the UFC Lightweight Champion.\n",
            "Action:\n",
            "```\n",
            "{\n",
            "  \"action\": \"Wikipedia\",\n",
            "  \"action_input\": \"Islam Makhachev title defenses\"\n",
            "}\n",
            "```\u001b[0m\u001b[33;1m\u001b[1;3mIslam Ramazanovich Makhachev (Russian: Ислам Рамазанович Махачев; born 27 October 1991) is a Russian professional mixed martial artist and former sambo competitor. He currently competes in the Lightweight division of the Ultimate Fighting Championship (UFC), where he is the current UFC Lightweight Champion.\u001b[0m\u001b[32;1m\u001b[1;3mThought: I need to find specific information about how many title defenses Islam Makhachev has in the UFC. \n",
            "Action:\n",
            "```\n",
            "{\n",
            "  \"action\": \"Wikipedia\",\n",
            "  \"action_input\": \"Islam Makhachev\"\n",
            "}\n",
            "```\u001b[0m\u001b[33;1m\u001b[1;3mIslam Ramazanovich Makhachev (Russian: Ислам Рамазанович Махачев; born 27 October 1991) is a Russian professional mixed martial artist and former sambo competitor. He currently competes in the Lightweight division of the Ultimate Fighting Championship (UFC), where he is the current UFC Lightweight Champion.\u001b[0m\u001b[32;1m\u001b[1;3mI need to find the specific number of title defenses Islam Makhachev has as the UFC Lightweight Champion. \n",
            "Action:\n",
            "```\n",
            "{\n",
            "  \"action\": \"Wikipedia\",\n",
            "  \"action_input\": \"Islam Makhachev title defenses\"\n",
            "}\n",
            "```\u001b[0m\u001b[33;1m\u001b[1;3mIslam Ramazanovich Makhachev (Russian: Ислам Рамазанович Махачев; born 27 October 1991) is a Russian professional mixed martial artist and former sambo competitor. He currently competes in the Lightweight division of the Ultimate Fighting Championship (UFC), where he is the current UFC Lightweight Champion.\u001b[0m\u001b[32;1m\u001b[1;3mI need to find specific information about how many title defenses Islam Makhachev has in the UFC. \n",
            "\n",
            "Action:\n",
            "```\n",
            "{\n",
            "  \"action\": \"Wikipedia\",\n",
            "  \"action_input\": \"Islam Makhachev\"\n",
            "}\n",
            "```\u001b[0m\u001b[33;1m\u001b[1;3mIslam Ramazanovich Makhachev (Russian: Ислам Рамазанович Махачев; born 27 October 1991) is a Russian professional mixed martial artist and former sambo competitor. He currently competes in the Lightweight division of the Ultimate Fighting Championship (UFC), where he is the current UFC Lightweight Champion.\u001b[0m\u001b[32;1m\u001b[1;3mI need to find specific information about how many title defenses Islam Makhachev has in the UFC. \n",
            "\n",
            "Action:\n",
            "```\n",
            "{\n",
            "  \"action\": \"Wikipedia\",\n",
            "  \"action_input\": \"Islam Makhachev\"\n",
            "}\n",
            "```\u001b[0m\u001b[33;1m\u001b[1;3mIslam Ramazanovich Makhachev (Russian: Ислам Рамазанович Махачев; born 27 October 1991) is a Russian professional mixed martial artist and former sambo competitor. He currently competes in the Lightweight division of the Ultimate Fighting Championship (UFC), where he is the current UFC Lightweight Champion.\u001b[0m\u001b[32;1m\u001b[1;3mI need to find the specific number of title defenses Islam Makhachev has as the UFC Lightweight Champion. \n",
            "Action:\n",
            "```\n",
            "{\n",
            "  \"action\": \"Wikipedia\",\n",
            "  \"action_input\": \"Islam Makhachev title defenses\"\n",
            "}\n",
            "```\u001b[0m\u001b[33;1m\u001b[1;3mIslam Ramazanovich Makhachev (Russian: Ислам Рамазанович Махачев; born 27 October 1991) is a Russian professional mixed martial artist and former sambo competitor. He currently competes in the Lightweight division of the Ultimate Fighting Championship (UFC), where he is the current UFC Lightweight Champion.\u001b[0m\u001b[32;1m\u001b[1;3mI will look for specific information about Islam Makhachev's title defenses in the UFC. \n",
            "\n",
            "Action:\n",
            "```\n",
            "{\n",
            "  \"action\": \"Wikipedia\",\n",
            "  \"action_input\": \"Islam Makhachev title defenses\"\n",
            "}\n",
            "```\u001b[0m\u001b[33;1m\u001b[1;3mIslam Ramazanovich Makhachev (Russian: Ислам Рамазанович Махачев; born 27 October 1991) is a Russian professional mixed martial artist and former sambo competitor. He currently competes in the Lightweight division of the Ultimate Fighting Championship (UFC), where he is the current UFC Lightweight Champion.\u001b[0m\u001b[32;1m\u001b[1;3mI need to find out the specific number of title defenses Islam Makhachev has in the UFC. \n",
            "\n",
            "Action:\n",
            "```\n",
            "{\n",
            "  \"action\": \"Wikipedia\",\n",
            "  \"action_input\": \"Islam Makhachev title defenses\"\n",
            "}\n",
            "```\u001b[0m\u001b[33;1m\u001b[1;3mIslam Ramazanovich Makhachev (Russian: Ислам Рамазанович Махачев; born 27 October 1991) is a Russian professional mixed martial artist and former sambo competitor. He currently competes in the Lightweight division of the Ultimate Fighting Championship (UFC), where he is the current UFC Lightweight Champion.\u001b[0m\u001b[32;1m\u001b[1;3mI need to find out the specific number of title defenses Islam Makhachev has in the UFC.\n",
            "\n",
            "Action:\n",
            "```\n",
            "{\n",
            "  \"action\": \"Wikipedia\",\n",
            "  \"action_input\": \"Islam Makhachev\"\n",
            "}\n",
            "```\u001b[0m\u001b[33;1m\u001b[1;3mIslam Ramazanovich Makhachev (Russian: Ислам Рамазанович Махачев; born 27 October 1991) is a Russian professional mixed martial artist and former sambo competitor. He currently competes in the Lightweight division of the Ultimate Fighting Championship (UFC), where he is the current UFC Lightweight Champion.\u001b[0m\u001b[32;1m\u001b[1;3m```\n",
            "{\n",
            "  \"action\": \"Wikipedia\",\n",
            "  \"action_input\": \"Islam Makhachev title defenses\"\n",
            "}\n",
            "```\u001b[0m\u001b[33;1m\u001b[1;3mIslam Ramazanovich Makhachev (Russian: Ислам Рамазанович Махачев; born 27 October 1991) is a Russian professional mixed martial artist and former sambo competitor. He currently competes in the Lightweight division of the Ultimate Fighting Championship (UFC), where he is the current UFC Lightweight Champion.\u001b[0m\u001b[32;1m\u001b[1;3m```\n",
            "{\n",
            "  \"action\": \"Wikipedia\",\n",
            "  \"action_input\": \"Islam Makhachev title defenses\"\n",
            "}\n",
            "```\u001b[0m\u001b[33;1m\u001b[1;3mIslam Ramazanovich Makhachev (Russian: Ислам Рамазанович Махачев; born 27 October 1991) is a Russian professional mixed martial artist and former sambo competitor. He currently competes in the Lightweight division of the Ultimate Fighting Championship (UFC), where he is the current UFC Lightweight Champion.\u001b[0m\u001b[32;1m\u001b[1;3mI will look for specific information about Islam Makhachev's title defenses in the UFC. \n",
            "\n",
            "Action:\n",
            "```\n",
            "{\n",
            "  \"action\": \"Wikipedia\",\n",
            "  \"action_input\": \"Islam Makhachev title defenses\"\n",
            "}\n",
            "```\u001b[0m\u001b[33;1m\u001b[1;3mIslam Ramazanovich Makhachev (Russian: Ислам Рамазанович Махачев; born 27 October 1991) is a Russian professional mixed martial artist and former sambo competitor. He currently competes in the Lightweight division of the Ultimate Fighting Championship (UFC), where he is the current UFC Lightweight Champion.\u001b[0m\u001b[32;1m\u001b[1;3mI need to find the specific number of title defenses Islam Makhachev has as the UFC Lightweight Champion.\n",
            "Action:\n",
            "```\n",
            "{\n",
            "  \"action\": \"Wikipedia\",\n",
            "  \"action_input\": \"Islam Makhachev title defenses\"\n",
            "}\n",
            "```\u001b[0m\u001b[33;1m\u001b[1;3mIslam Ramazanovich Makhachev (Russian: Ислам Рамазанович Махачев; born 27 October 1991) is a Russian professional mixed martial artist and former sambo competitor. He currently competes in the Lightweight division of the Ultimate Fighting Championship (UFC), where he is the current UFC Lightweight Champion.\u001b[0m\u001b[32;1m\u001b[1;3m```\n",
            "{\n",
            "  \"action\": \"Wikipedia\",\n",
            "  \"action_input\": \"Islam Makhachev title defenses\"\n",
            "}\n",
            "```\u001b[0m\u001b[33;1m\u001b[1;3mIslam Ramazanovich Makhachev (Russian: Ислам Рамазанович Махачев; born 27 October 1991) is a Russian professional mixed martial artist and former sambo competitor. He currently competes in the Lightweight division of the Ultimate Fighting Championship (UFC), where he is the current UFC Lightweight Champion.\u001b[0m\u001b[32;1m\u001b[1;3m\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Bot: Agent stopped due to iteration limit or time limit.\n",
            "User: exit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ReACT Agent with Docstore"
      ],
      "metadata": {
        "id": "YoQBQaPXssTU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "from langchain import hub\n",
        "from langchain.agents import AgentExecutor, create_react_agent\n",
        "from langchain.chains import create_history_aware_retriever, create_retrieval_chain\n",
        "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_core.messages import AIMessage, HumanMessage\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain_core.tools import Tool\n",
        "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
        "\n",
        "# Check if the Chroma vector store already exists\n",
        "if os.path.exists(persistent_directory):\n",
        "    print(\"Loading existing vector store...\")\n",
        "    db = Chroma(persist_directory=persistent_directory,\n",
        "                embedding_function=None)\n",
        "else:\n",
        "    raise FileNotFoundError(\n",
        "        f\"The directory {persistent_directory} does not exist. Please check the path.\"\n",
        "    )\n",
        "\n",
        "# Define the embedding model\n",
        "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\", api_key=api)\n",
        "\n",
        "# Load the existing vector store with the embedding function\n",
        "db = Chroma(persist_directory=persistent_directory,\n",
        "            embedding_function=embeddings)\n",
        "\n",
        "# Create a retriever for querying the vector store\n",
        "# `search_type` specifies the type of search (e.g., similarity)\n",
        "# `search_kwargs` contains additional arguments for the search (e.g., number of results to return)\n",
        "retriever = db.as_retriever(\n",
        "    search_type=\"similarity\",\n",
        "    search_kwargs={\"k\": 3},\n",
        ")\n",
        "\n",
        "# Create a ChatOpenAI model\n",
        "llm = ChatOpenAI(model=\"gpt-4o\", api_key=api)\n",
        "\n",
        "# Contextualize question prompt\n",
        "# This system prompt helps the AI understand that it should reformulate the question\n",
        "# based on the chat history to make it a standalone question\n",
        "contextualize_q_system_prompt = (\n",
        "    \"Given a chat history and the latest user question \"\n",
        "    \"which might reference context in the chat history, \"\n",
        "    \"formulate a standalone question which can be understood \"\n",
        "    \"without the chat history. Do NOT answer the question, just \"\n",
        "    \"reformulate it if needed and otherwise return it as is.\"\n",
        ")\n",
        "\n",
        "# Create a prompt template for contextualizing questions\n",
        "contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", contextualize_q_system_prompt),\n",
        "        MessagesPlaceholder(\"chat_history\"),\n",
        "        (\"human\", \"{input}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Create a history-aware retriever\n",
        "# This uses the LLM to help reformulate the question based on chat history\n",
        "history_aware_retriever = create_history_aware_retriever(\n",
        "    llm, retriever, contextualize_q_prompt\n",
        ")\n",
        "\n",
        "# Answer question prompt\n",
        "# This system prompt helps the AI understand that it should provide concise answers\n",
        "# based on the retrieved context and indicates what to do if the answer is unknown\n",
        "qa_system_prompt = (\n",
        "    \"You are an assistant for question-answering tasks. Use \"\n",
        "    \"the following pieces of retrieved context to answer the \"\n",
        "    \"question. If you don't know the answer, just say that you \"\n",
        "    \"don't know. Use three sentences maximum and keep the answer \"\n",
        "    \"concise.\"\n",
        "    \"\\n\\n\"\n",
        "    \"{context}\"\n",
        ")\n",
        "\n",
        "# Create a prompt template for answering questions\n",
        "qa_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", qa_system_prompt),\n",
        "        MessagesPlaceholder(\"chat_history\"),\n",
        "        (\"human\", \"{input}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Create a chain to combine documents for question answering\n",
        "# `create_stuff_documents_chain` feeds all retrieved context into the LLM\n",
        "question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)\n",
        "\n",
        "# Create a retrieval chain that combines the history-aware retriever and the question answering chain\n",
        "rag_chain = create_retrieval_chain(\n",
        "    history_aware_retriever, question_answer_chain)\n",
        "\n",
        "\n",
        "# Set Up ReAct Agent with Document Store Retriever\n",
        "# Load the ReAct Docstore Prompt\n",
        "react_docstore_prompt = hub.pull(\"hwchase17/react\")\n",
        "\n",
        "tools = [\n",
        "    Tool(\n",
        "        name=\"Answer Question\",\n",
        "        func=lambda input, **kwargs: rag_chain.invoke(\n",
        "            {\"input\": input, \"chat_history\": kwargs.get(\"chat_history\", [])}\n",
        "        ),\n",
        "        description=\"useful for when you need to answer questions about the context\",\n",
        "    )\n",
        "]\n",
        "\n",
        "# Create the ReAct Agent with document store retriever\n",
        "agent = create_react_agent(\n",
        "    llm=llm,\n",
        "    tools=tools,\n",
        "    prompt=react_docstore_prompt,\n",
        ")\n",
        "\n",
        "agent_executor = AgentExecutor.from_agent_and_tools(\n",
        "    agent=agent, tools=tools, handle_parsing_errors=True, verbose=True,\n",
        ")\n",
        "\n",
        "chat_history = []\n",
        "while True:\n",
        "    query = input(\"You: \")\n",
        "    if query.lower() == \"exit\":\n",
        "        break\n",
        "    response = agent_executor.invoke(\n",
        "        {\"input\": query, \"chat_history\": chat_history})\n",
        "    print(f\"AI: {response['output']}\")\n",
        "\n",
        "    # Update history\n",
        "    chat_history.append(HumanMessage(content=query))\n",
        "    chat_history.append(AIMessage(content=response[\"output\"]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "se-_wxxqsv2_",
        "outputId": "25aba459-82f0-4ba9-b3b1-fb39a3f9f37c"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading existing vector store...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/langsmith/client.py:272: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You: Hello! How are you?\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mThis is a simple greeting and doesn't require the use of any tools to answer. \n",
            "\n",
            "Final Answer: Hello! I'm just a virtual assistant, so I don't have feelings, but I'm here and ready to help you with any questions you have! How can I assist you today?\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "AI: Hello! I'm just a virtual assistant, so I don't have feelings, but I'm here and ready to help you with any questions you have! How can I assist you today?\n",
            "You: US bill of rights?\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mThe question is asking about the US Bill of Rights, which I can explain. \n",
            "\n",
            "Action: Answer Question\n",
            "Action Input: What is the US Bill of Rights and what does it contain?\u001b[0m\u001b[36;1m\u001b[1;3m{'input': 'What is the US Bill of Rights and what does it contain?', 'chat_history': [], 'context': [Document(metadata={'source': '/content/us_bill_of_rights.txt'}, page_content='1.F.'), Document(metadata={'source': '/content/us_bill_of_rights.txt'}, page_content='No person shall be held to answer for a capital, or otherwise infamous crime,\\nunless on a presentment or indictment of a Grand Jury, except in cases arising\\nin the land or naval forces, or in the Militia, when in actual service\\nin time of War or public danger; nor shall any person be subject for\\nthe same offense to be twice put in jeopardy of life or limb;\\nnor shall be compelled in any criminal case to be a witness against himself,\\nnor be deprived of life, liberty, or property, without due process of law;\\nnor shall private property be taken for public use without just compensation.'), Document(metadata={'source': '/content/us_bill_of_rights.txt'}, page_content='X\\n\\nThe powers not delegated to the United States by the Constitution,\\nnor prohibited by it to the States, are reserved to the States respectively,\\nor to the people.')], 'answer': 'The US Bill of Rights is the collective name for the first ten amendments to the United States Constitution. It contains provisions guaranteeing essential rights and freedoms, such as freedom of speech, religion, the right to bear arms, protection against unreasonable searches and seizures, and rights related to criminal prosecutions, including due process and protection against self-incrimination. It was ratified in 1791 to ensure the protection of individual liberties against government infringement.'}\u001b[0m\u001b[32;1m\u001b[1;3mI now know the final answer.\n",
            "\n",
            "Final Answer: The US Bill of Rights is the collective name for the first ten amendments to the United States Constitution. It was ratified in 1791 to ensure the protection of individual liberties against government infringement. The Bill of Rights contains provisions guaranteeing essential rights and freedoms, such as freedom of speech, religion, the right to bear arms, protection against unreasonable searches and seizures, and rights related to criminal prosecutions, including due process and protection against self-incrimination.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "AI: The US Bill of Rights is the collective name for the first ten amendments to the United States Constitution. It was ratified in 1791 to ensure the protection of individual liberties against government infringement. The Bill of Rights contains provisions guaranteeing essential rights and freedoms, such as freedom of speech, religion, the right to bear arms, protection against unreasonable searches and seizures, and rights related to criminal prosecutions, including due process and protection against self-incrimination.\n",
            "You: what is the temperature today?\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mTo determine the current temperature, I would typically access real-time weather data. However, I don't have the ability to fetch live data. I recommend checking a reliable weather website or app for the most accurate information on today's temperature in your location.\u001b[0mInvalid Format: Missing 'Action:' after 'Thought:'\u001b[32;1m\u001b[1;3mI cannot retrieve live data directly. I recommend checking a weather service or app for the most accurate and current temperature information for your location.\u001b[0mInvalid Format: Missing 'Action:' after 'Thought:'\u001b[32;1m\u001b[1;3mI cannot retrieve live temperature data directly. I recommend checking a weather service or app for the most accurate and current temperature information for your location.\u001b[0mInvalid Format: Missing 'Action:' after 'Thought:'\u001b[32;1m\u001b[1;3mAction: Answer Question\n",
            "Action Input: I cannot retrieve live temperature data directly. I recommend checking a weather service or app for the most accurate and current temperature information for your location.\u001b[0m\u001b[36;1m\u001b[1;3m{'input': 'I cannot retrieve live temperature data directly. I recommend checking a weather service or app for the most accurate and current temperature information for your location.', 'chat_history': [], 'context': [Document(metadata={'source': '/content/us_bill_of_rights.txt'}, page_content='Section 3. Information about the Project Gutenberg Literary Archive Foundation'), Document(metadata={'source': '/content/us_bill_of_rights.txt'}, page_content='Section 2. Information about the Mission of Project Gutenberg™'), Document(metadata={'source': '/content/us_bill_of_rights.txt'}, page_content='This website includes information about Project Gutenberg™,\\nincluding how to make donations to the Project Gutenberg Literary\\nArchive Foundation, how to help produce our new eBooks, and how to\\nsubscribe to our email newsletter to hear about new eBooks.')], 'answer': 'I understand you are looking for temperature data. As I cannot access live data, I recommend using a reliable weather service or app to get the most accurate and current temperature information for your location.'}\u001b[0m\u001b[32;1m\u001b[1;3mI understand you are looking for temperature data. As I cannot access live data, I recommend using a reliable weather service or app to get the most accurate and current temperature information for your location.\u001b[0mInvalid Format: Missing 'Action:' after 'Thought:'\u001b[32;1m\u001b[1;3mAction: Answer Question\n",
            "Action Input: I cannot retrieve live temperature data directly. I recommend checking a weather service or app for the most accurate and current temperature information for your location.\u001b[0m\u001b[36;1m\u001b[1;3m{'input': 'I cannot retrieve live temperature data directly. I recommend checking a weather service or app for the most accurate and current temperature information for your location.', 'chat_history': [], 'context': [Document(metadata={'source': '/content/us_bill_of_rights.txt'}, page_content='Section 3. Information about the Project Gutenberg Literary Archive Foundation'), Document(metadata={'source': '/content/us_bill_of_rights.txt'}, page_content='Section 2. Information about the Mission of Project Gutenberg™'), Document(metadata={'source': '/content/us_bill_of_rights.txt'}, page_content='This website includes information about Project Gutenberg™,\\nincluding how to make donations to the Project Gutenberg Literary\\nArchive Foundation, how to help produce our new eBooks, and how to\\nsubscribe to our email newsletter to hear about new eBooks.')], 'answer': \"I don't have access to live data, including current temperature information. For the most accurate and up-to-date weather details, please refer to a reliable weather service or app.\"}\u001b[0m\u001b[32;1m\u001b[1;3mI don't have access to live data, including current temperature information. For the most accurate and up-to-date weather details, please refer to a reliable weather service or app.\u001b[0mInvalid Format: Missing 'Action:' after 'Thought:'\u001b[32;1m\u001b[1;3mAction: Answer Question\n",
            "Action Input: I don't have access to live data, including current temperature information. For the most accurate and up-to-date weather details, please refer to a reliable weather service or app.\u001b[0m\u001b[36;1m\u001b[1;3m{'input': \"I don't have access to live data, including current temperature information. For the most accurate and up-to-date weather details, please refer to a reliable weather service or app.\", 'chat_history': [], 'context': [Document(metadata={'source': '/content/us_bill_of_rights.txt'}, page_content='Section 3. Information about the Project Gutenberg Literary Archive Foundation'), Document(metadata={'source': '/content/us_bill_of_rights.txt'}, page_content='The Foundation is committed to complying with the laws regulating\\ncharities and charitable donations in all 50 states of the United\\nStates. Compliance requirements are not uniform and it takes a\\nconsiderable effort, much paperwork and many fees to meet and keep up\\nwith these requirements. We do not solicit donations in locations\\nwhere we have not received written confirmation of compliance. To SEND\\nDONATIONS or determine the status of compliance for any particular state\\nvisit www.gutenberg.org/donate.'), Document(metadata={'source': '/content/us_bill_of_rights.txt'}, page_content='\\ufeffThe Project Gutenberg eBook of The United States Bill of Rights\\n    \\nThis ebook is for the use of anyone anywhere in the United States and\\nmost other parts of the world at no cost and with almost no restrictions\\nwhatsoever. You may copy it, give it away or re-use it under the terms\\nof the Project Gutenberg License included with this ebook or online\\nat www.gutenberg.org. If you are not located in the United States,\\nyou will have to check the laws of the country where you are located\\nbefore using this eBook.')], 'answer': \"I understand you don't have access to live data. For accurate current weather information, use trusted sources like weather websites, apps, or local news services. They provide real-time updates and forecasts.\"}\u001b[0m\u001b[32;1m\u001b[1;3mI don't have access to live data, including current temperature information. For the most accurate and up-to-date weather details, please refer to a reliable weather service or app.\u001b[0mInvalid Format: Missing 'Action:' after 'Thought:'\u001b[32;1m\u001b[1;3mFinal Answer: I do not have access to live data, including current temperature information. For the most accurate and up-to-date weather details, please refer to a reliable weather service or app.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "AI: I do not have access to live data, including current temperature information. For the most accurate and up-to-date weather details, please refer to a reliable weather service or app.\n",
            "You: exit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DAhwNPMBxN0G"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}