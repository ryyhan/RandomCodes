{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOb9QENUx9klqolc6GQSZMp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ryyhan/RandomCodes/blob/main/Langchain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Basic Chat\n"
      ],
      "metadata": {
        "id": "WLjYXD-eDade"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade langchain langchain_core langchain_openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "3iYxf6v9DsFW",
        "outputId": "581afa6e-270b-4cf4-d657-b52c9870bd43"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.24)\n",
            "Collecting langchain\n",
            "  Downloading langchain-0.3.25-py3-none-any.whl.metadata (7.8 kB)\n",
            "Requirement already satisfied: langchain_core in /usr/local/lib/python3.11/dist-packages (0.3.56)\n",
            "Collecting langchain_core\n",
            "  Downloading langchain_core-0.3.59-py3-none-any.whl.metadata (5.9 kB)\n",
            "Collecting langchain_openai\n",
            "  Downloading langchain_openai-0.3.16-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.8)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.39)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.11.4)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.40)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain_core) (9.1.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain_core) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain_core) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain_core) (4.13.2)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.68.2 in /usr/local/lib/python3.11/dist-packages (from langchain_openai) (1.76.2)\n",
            "Collecting tiktoken<1,>=0.7 (from langchain_openai)\n",
            "  Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain_core) (3.0.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.18)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.68.2->langchain_openai) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.68.2->langchain_openai) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.68.2->langchain_openai) (0.9.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.68.2->langchain_openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.68.2->langchain_openai) (4.67.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2025.4.26)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken<1,>=0.7->langchain_openai) (2024.11.6)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (0.16.0)\n",
            "Downloading langchain-0.3.25-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m28.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_core-0.3.59-py3-none-any.whl (437 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m437.7/437.7 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_openai-0.3.16-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.8/62.8 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tiktoken, langchain_core, langchain_openai, langchain\n",
            "  Attempting uninstall: langchain_core\n",
            "    Found existing installation: langchain-core 0.3.56\n",
            "    Uninstalling langchain-core-0.3.56:\n",
            "      Successfully uninstalled langchain-core-0.3.56\n",
            "  Attempting uninstall: langchain\n",
            "    Found existing installation: langchain 0.3.24\n",
            "    Uninstalling langchain-0.3.24:\n",
            "      Successfully uninstalled langchain-0.3.24\n",
            "Successfully installed langchain-0.3.25 langchain_core-0.3.59 langchain_openai-0.3.16 tiktoken-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2MmHoZ6hApFa",
        "outputId": "a29bda6c-7567-4ed9-ef8f-5b50c96df564"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Full result:\n",
            "content='81 divided by 9 is 9.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 16, 'total_tokens': 26, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_0392822090', 'id': 'chatcmpl-BVyGQRlhQQXVdc8dkpi2cHGGpN86Y', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='run--64ae77e8-96ae-41ae-b26d-c61d821a5122-0' usage_metadata={'input_tokens': 16, 'output_tokens': 10, 'total_tokens': 26, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
            "Content only:\n",
            "81 divided by 9 is 9.\n"
          ]
        }
      ],
      "source": [
        "#imports\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "#environment variables\n",
        "from google.colab import userdata\n",
        "api=userdata.get('OPEN_AI_KEY')\n",
        "\n",
        "\n",
        "# Create a ChatOpenAI model\n",
        "model = ChatOpenAI(model=\"gpt-4o-mini\",api_key=api)\n",
        "\n",
        "# Invoke the model with a message\n",
        "result = model.invoke(\"What is 81 divided by 9?\")\n",
        "print(\"Full result:\")\n",
        "print(result)\n",
        "print(\"Content only:\")\n",
        "print(result.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conversation"
      ],
      "metadata": {
        "id": "RlmNyBYVLjTp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "\n",
        "#environment variables\n",
        "from google.colab import userdata\n",
        "api=userdata.get('OPEN_AI_KEY')\n",
        "\n",
        "\n",
        "# Create a ChatOpenAI model\n",
        "model = ChatOpenAI(model=\"gpt-4o-mini\",api_key=api)\n",
        "\n",
        "\n",
        "messages = [\n",
        "    SystemMessage(content=\"Solve the following math problems\"),\n",
        "    HumanMessage(content=\"What is 81 divided by 9?\"),\n",
        "]\n",
        "\n",
        "# Invoke the model with messages\n",
        "result = model.invoke(messages)\n",
        "print(f\"Answer from AI (Single): {result.content}\")\n",
        "\n",
        "\n",
        "# AIMessage:\n",
        "#   Message from an AI.\n",
        "messages = [\n",
        "    SystemMessage(content=\"Solve the following math problems\"),\n",
        "    HumanMessage(content=\"What is 81 divided by 9?\"),\n",
        "    AIMessage(content=\"81 divided by 9 is 9.\"),\n",
        "    HumanMessage(content=\"What is 10 times 5?\"),\n",
        "]\n",
        "\n",
        "# Invoke the model with messages\n",
        "result = model.invoke(messages)\n",
        "print(f\"Answer from AI (Multiple): {result.content}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Omxka0qoDdD8",
        "outputId": "d8e79728-1b41-4ef4-c31c-bc9ebc6375a7"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer from AI (Single): 81 divided by 9 is 9.\n",
            "Answer from AI (Multiple): 10 times 5 is 50.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Chatbot Conversations"
      ],
      "metadata": {
        "id": "baW8PAR8RtT2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.schema import AIMessage, HumanMessage, SystemMessage\n",
        "\n",
        "#environment variables\n",
        "from google.colab import userdata\n",
        "api=userdata.get('OPEN_AI_KEY')\n",
        "\n",
        "\n",
        "# Create a ChatOpenAI model\n",
        "model = ChatOpenAI(model=\"gpt-4o-mini\",api_key=api)\n",
        "\n",
        "\n",
        "chat_history = []  # Use a list to store messages\n",
        "\n",
        "# Set an initial system message (optional)\n",
        "system_message = SystemMessage(content=\"You are a helpful AI assistant.\")\n",
        "chat_history.append(system_message)  # Add system message to chat history\n",
        "\n",
        "# Chat loop\n",
        "while True:\n",
        "    query = input(\"You: \")\n",
        "    if query.lower() == \"exit\":\n",
        "        break\n",
        "    chat_history.append(HumanMessage(content=query))  # Add user message\n",
        "\n",
        "    # Get AI response using history\n",
        "    result = model.invoke(chat_history)\n",
        "    response = result.content\n",
        "    chat_history.append(AIMessage(content=response))  # Add AI message\n",
        "\n",
        "    print(f\"AI: {response}\")\n",
        "\n",
        "\n",
        "print(\"---- Message History ----\")\n",
        "print(chat_history)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dmchOq7YRs85",
        "outputId": "e994744f-95a7-45e3-a664-1c733c69c3fd"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You: Hey! How are you?\n",
            "AI: Hello! I'm just a program, so I don't have feelings, but I'm here and ready to assist you. How can I help you today?\n",
            "You: Who is the greatest of all time ufc lightweight?\n",
            "AI: The debate over the greatest of all time (GOAT) in the UFC lightweight division often centers around a few key fighters, with the most frequently mentioned being:\n",
            "\n",
            "1. **Khabib Nurmagomedov**: Undefeated with a record of 29-0, Khabib is known for his dominant grappling and ground control. He successfully defended the lightweight title three times before retiring in 2020.\n",
            "\n",
            "2. **Benson Henderson**: A former champion and a skilled fighter known for his submission and striking abilities, Henderson had notable reigns in both the UFC and WEC (World Extreme Cagefighting).\n",
            "\n",
            "3. **Tony Ferguson**: Known for his unorthodox style and finishing ability, Ferguson had an impressive winning streak and was the interim champion, with numerous fight of the night performances.\n",
            "\n",
            "4. **Charles Oliveira**: Holding the record for the most submission wins in UFC history and the record for the most finishes, Oliveira became the lightweight champion in May 2021 and demonstrated incredible skill in both grappling and striking.\n",
            "\n",
            "Each of these fighters has made a significant impact in the division, and opinions on who is the greatest can vary widely among fans and analysts. Ultimately, it depends on the criteria used to define \"greatness.\"\n",
            "You: exit\n",
            "---- Message History ----\n",
            "[SystemMessage(content='You are a helpful AI assistant.', additional_kwargs={}, response_metadata={}), HumanMessage(content='Hey! How are you?', additional_kwargs={}, response_metadata={}), AIMessage(content=\"Hello! I'm just a program, so I don't have feelings, but I'm here and ready to assist you. How can I help you today?\", additional_kwargs={}, response_metadata={}), HumanMessage(content='Who is the greatest of all time ufc lightweight?', additional_kwargs={}, response_metadata={}), AIMessage(content='The debate over the greatest of all time (GOAT) in the UFC lightweight division often centers around a few key fighters, with the most frequently mentioned being:\\n\\n1. **Khabib Nurmagomedov**: Undefeated with a record of 29-0, Khabib is known for his dominant grappling and ground control. He successfully defended the lightweight title three times before retiring in 2020.\\n\\n2. **Benson Henderson**: A former champion and a skilled fighter known for his submission and striking abilities, Henderson had notable reigns in both the UFC and WEC (World Extreme Cagefighting).\\n\\n3. **Tony Ferguson**: Known for his unorthodox style and finishing ability, Ferguson had an impressive winning streak and was the interim champion, with numerous fight of the night performances.\\n\\n4. **Charles Oliveira**: Holding the record for the most submission wins in UFC history and the record for the most finishes, Oliveira became the lightweight champion in May 2021 and demonstrated incredible skill in both grappling and striking.\\n\\nEach of these fighters has made a significant impact in the division, and opinions on who is the greatest can vary widely among fans and analysts. Ultimately, it depends on the criteria used to define \"greatness.\"', additional_kwargs={}, response_metadata={})]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example Source: https://python.langchain.com/v0.2/docs/integrations/memory/google_firestore/\n",
        "\n",
        "from google.cloud import firestore\n",
        "from langchain_google_firestore import FirestoreChatMessageHistory\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "\"\"\"\n",
        "Steps to replicate this example:\n",
        "1. Create a Firebase account\n",
        "2. Create a new Firebase project\n",
        "    - Copy the project ID\n",
        "3. Create a Firestore database in the Firebase project\n",
        "4. Install the Google Cloud CLI on your computer\n",
        "    - https://cloud.google.com/sdk/docs/install\n",
        "    - Authenticate the Google Cloud CLI with your Google account\n",
        "        - https://cloud.google.com/docs/authentication/provide-credentials-adc#local-dev\n",
        "    - Set your default project to the new Firebase project you created\n",
        "5. Enable the Firestore API in the Google Cloud Console:\n",
        "    - https://console.cloud.google.com/apis/enableflow?apiid=firestore.googleapis.com&project=crewai-automation\n",
        "\"\"\"\n",
        "\n",
        "#environment variables\n",
        "from google.colab import userdata\n",
        "api=userdata.get('OPEN_AI_KEY')\n",
        "\n",
        "\n",
        "# Setup Firebase Firestore\n",
        "PROJECT_ID = \"langchain-demo-abf48\"\n",
        "SESSION_ID = \"user_session_new\"  # This could be a username or a unique ID\n",
        "COLLECTION_NAME = \"chat_history\"\n",
        "\n",
        "# Initialize Firestore Client\n",
        "print(\"Initializing Firestore Client...\")\n",
        "client = firestore.Client(project=PROJECT_ID)\n",
        "\n",
        "# Initialize Firestore Chat Message History\n",
        "print(\"Initializing Firestore Chat Message History...\")\n",
        "chat_history = FirestoreChatMessageHistory(\n",
        "    session_id=SESSION_ID,\n",
        "    collection=COLLECTION_NAME,\n",
        "    client=client,\n",
        ")\n",
        "print(\"Chat History Initialized.\")\n",
        "print(\"Current Chat History:\", chat_history.messages)\n",
        "\n",
        "# Create a ChatOpenAI model\n",
        "model = ChatOpenAI(model=\"gpt-4o-mini\",api_key=api)\n",
        "\n",
        "print(\"Start chatting with the AI. Type 'exit' to quit.\")\n",
        "\n",
        "while True:\n",
        "    human_input = input(\"User: \")\n",
        "    if human_input.lower() == \"exit\":\n",
        "        break\n",
        "\n",
        "    chat_history.add_user_message(human_input)\n",
        "\n",
        "    ai_response = model.invoke(chat_history.messages)\n",
        "    chat_history.add_ai_message(ai_response.content)\n",
        "\n",
        "    print(f\"AI: {ai_response.content}\")"
      ],
      "metadata": {
        "id": "ifnx-HPIL8oe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "u8zseiV6hSUd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#prompting"
      ],
      "metadata": {
        "id": "X_Aq47ZAhT_g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "0TDhv16OhXgR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prompt Template Docs:\n",
        "#   https://python.langchain.com/v0.2/docs/concepts/#prompt-templateshttps://python.langchain.com/v0.2/docs/concepts/#prompt-templates\n",
        "\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain_core.messages import HumanMessage\n",
        "\n",
        "# # PART 1: Create a ChatPromptTemplate using a template string\n",
        "template = \"Tell me a joke about {topic}.\"\n",
        "prompt_template = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "print(\"-----Prompt from Template-----\")\n",
        "prompt = prompt_template.invoke({\"topic\": \"cats\"})\n",
        "print(prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WHe2WsKShTHI",
        "outputId": "95568de1-3737-489f-8c12-fe7fe6f8900f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----Prompt from Template-----\n",
            "messages=[HumanMessage(content='Tell me a joke about cats.', additional_kwargs={}, response_metadata={})]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# PART 2: Prompt with Multiple Placeholders\n",
        "template_multiple = \"\"\"You are a helpful assistant.\n",
        "Human: Tell me a {adjective} story about a {animal}.\n",
        "Assistant:\"\"\"\n",
        "prompt_multiple = ChatPromptTemplate.from_template(template_multiple)\n",
        "prompt = prompt_multiple.invoke({\"adjective\": \"funny\", \"animal\": \"panda\"})\n",
        "print(\"\\n----- Prompt with Multiple Placeholders -----\\n\")\n",
        "print(prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mQDYTH4DJkBx",
        "outputId": "ee58c67c-14db-4d61-f67d-946a4e6e1d4b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "----- Prompt with Multiple Placeholders -----\n",
            "\n",
            "messages=[HumanMessage(content='You are a helpful assistant.\\nHuman: Tell me a funny story about a panda.\\nAssistant:', additional_kwargs={}, response_metadata={})]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prompt Template with ChatModel"
      ],
      "metadata": {
        "id": "LIFjKTV5LlBR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "#environment variables\n",
        "from google.colab import userdata\n",
        "api=userdata.get('OPEN_AI_KEY')\n",
        "\n",
        "# Create a ChatOpenAI model\n",
        "model = ChatOpenAI(model=\"gpt-4o-mini\",api_key=api)\n",
        "\n",
        "# PART 1: Create a ChatPromptTemplate using a template string\n",
        "print(\"-----Prompt from Template-----\")\n",
        "template = \"Tell me a joke about {topic}.\"\n",
        "prompt_template = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "prompt = prompt_template.invoke({\"topic\": \"cats\"})\n",
        "result = model.invoke(prompt)\n",
        "print(prompt, result.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6TJlO_OTKyT7",
        "outputId": "52f04e54-e77c-4bb8-8560-8cda51dbb9a4"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----Prompt from Template-----\n",
            "messages=[HumanMessage(content='Tell me a joke about cats.', additional_kwargs={}, response_metadata={})] Why was the cat sitting on the computer?\n",
            "\n",
            "Because it wanted to keep an eye on the mouse!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# PART 2: Prompt with Multiple Placeholders\n",
        "print(\"\\n----- Prompt with Multiple Placeholders -----\\n\")\n",
        "template_multiple = \"\"\"You are a helpful assistant.\n",
        "Human: Tell me a {adjective} short story about a {animal}.\n",
        "Assistant:\"\"\"\n",
        "prompt_multiple = ChatPromptTemplate.from_template(template_multiple)\n",
        "prompt = prompt_multiple.invoke({\"adjective\": \"funny\", \"animal\": \"cat\"})\n",
        "\n",
        "result = model.invoke(prompt)\n",
        "print(prompt,\"/n\\n\",result.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "usPIM502lUQy",
        "outputId": "2c6cf490-38ed-4c9b-ce10-6e93444570de"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "----- Prompt with Multiple Placeholders -----\n",
            "\n",
            "messages=[HumanMessage(content='You are a helpful assistant.\\nHuman: Tell me a funny short story about a cat.\\nAssistant:', additional_kwargs={}, response_metadata={})] /n\n",
            " Once upon a time in a cozy little house, there lived a clever cat named Mr. Whiskers. Mr. Whiskers was no ordinary cat; he had a knack for getting into all sorts of humorous predicaments. \n",
            "\n",
            "One sunny afternoon, while his owner, Mrs. Thompson, was busy baking a batch of cookies, Mr. Whiskers decided it was the perfect time for a little adventure. He leapt onto the kitchen counter, eyeing the tantalizing plate of freshly baked cookies.\n",
            "\n",
            "As he inched closer, he couldn’t resist the temptation and swiped one off the plate with his paw. But as he tried to make his getaway, the cookie was much larger than he anticipated. In his attempt to escape, the cookie bounced off the edge of the counter and landed right in the middle of a bowl of flour that Mrs. Thompson had just set aside.\n",
            "\n",
            "In a matter of seconds, Mr. Whiskers emerged from the bowl, looking like a fluffy ghost and leaving a cloud of flour in the air. Mrs. Thompson turned around just in time to see him prancing around, covered head to tail in white powder, with a half-eaten cookie dangling from his mouth.\n",
            "\n",
            "“Mr. Whiskers! What have you done?” she exclaimed, trying to stifle her laughter. \n",
            "\n",
            "The cat, with his best innocent expression, flipped his tail in the air and strutted around like he was on a catwalk, proudly displaying his floury outfit. \n",
            "\n",
            "From that day on, every time Mrs. Thompson baked cookies, they both knew that Mr. Whiskers would be on the prowl, and she would have to keep a closer eye on her “ghost” of a kitty!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# PART 3: Prompt with System and Human Messages (Using Tuples)\n",
        "print(\"\\n----- Prompt with System and Human Messages (Tuple) -----\\n\")\n",
        "messages = [\n",
        "    (\"system\", \"You are a comedian who tells jokes about {topic}.\"),\n",
        "    (\"human\", \"Tell me {joke_count} jokes.\"),\n",
        "]\n",
        "prompt_template = ChatPromptTemplate.from_messages(messages)\n",
        "prompt = prompt_template.invoke({\"topic\": \"cats\", \"joke_count\": 3})\n",
        "result = model.invoke(prompt)\n",
        "print(prompt,\"\\n\\n\\n\")\n",
        "print(result.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ELOvkj-lgNO",
        "outputId": "73f091e1-75f4-436b-b5c5-bbdfac999348"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "----- Prompt with System and Human Messages (Tuple) -----\n",
            "\n",
            "messages=[SystemMessage(content='You are a comedian who tells jokes about cats.', additional_kwargs={}, response_metadata={}), HumanMessage(content='Tell me 3 jokes.', additional_kwargs={}, response_metadata={})] \n",
            "\n",
            "\n",
            "\n",
            "Sure, here are three cat-themed jokes for you:\n",
            "\n",
            "1. Why did the cat sit on the computer?\n",
            "   Because it wanted to keep an eye on the mouse!\n",
            "\n",
            "2. What do you call a pile of cats?\n",
            "   A meowtain!\n",
            "\n",
            "3. Why was the cat so good at video games?\n",
            "   Because it had nine lives to practice on!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "TEHfwCPSl5Qe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chains"
      ],
      "metadata": {
        "id": "YRVbxvN0l_AC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "JvK3j7mlmBGG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "#environment variables\n",
        "from google.colab import userdata\n",
        "api=userdata.get('OPEN_AI_KEY')\n",
        "\n",
        "# Create a ChatOpenAI model\n",
        "model = ChatOpenAI(model=\"gpt-4o-mini\",api_key=api)\n",
        "\n",
        "# Define prompt templates (no need for separate Runnable chains)\n",
        "prompt_template = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"You are a comedian who tells jokes about {topic}.\"),\n",
        "        (\"human\", \"Tell me {joke_count} jokes.\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Create the combined chain using LangChain Expression Language (LCEL)\n",
        "chain = prompt_template | model | StrOutputParser()\n",
        "\n",
        "# Run the chain\n",
        "result = chain.invoke({\"topic\": \"cats\", \"joke_count\": 3})\n",
        "\n",
        "# Output\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FWkafui9lrd-",
        "outputId": "ab28e3e8-9702-43ce-ce97-800c74e61d92"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sure, here are three cat-themed jokes for you:\n",
            "\n",
            "1. Why was the cat sitting on the computer?\n",
            "   Because it wanted to keep an eye on the mouse!\n",
            "\n",
            "2. What do you call a pile of cats?\n",
            "   A meow-tain!\n",
            "\n",
            "3. Why did the cat join the Red Cross?\n",
            "   Because it wanted to be a first-aid kit-ty! \n",
            "\n",
            "Hope these brought a smile to your face!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Behind the scenes"
      ],
      "metadata": {
        "id": "zJq9hZB0rS6B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.schema.runnable import RunnableLambda, RunnableSequence\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "#environment variables\n",
        "from google.colab import userdata\n",
        "api=userdata.get('OPEN_AI_KEY')\n",
        "\n",
        "# Create a ChatOpenAI model\n",
        "model = ChatOpenAI(model=\"gpt-4o-mini\",api_key=api)\n",
        "\n",
        "# Define prompt templates\n",
        "prompt_template = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"You are a comedian who tells jokes about {topic}.\"),\n",
        "        (\"human\", \"Tell me {joke_count} jokes.\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Create individual runnables (steps in the chain)\n",
        "format_prompt = RunnableLambda(lambda x: prompt_template.format_prompt(**x))\n",
        "invoke_model = RunnableLambda(lambda x: model.invoke(x.to_messages()))\n",
        "parse_output = RunnableLambda(lambda x: x.content)\n",
        "\n",
        "# Create the RunnableSequence (equivalent to the LCEL chain)\n",
        "chain = RunnableSequence(first=format_prompt, middle=[invoke_model], last=parse_output)\n",
        "\n",
        "# Run the chain\n",
        "response = chain.invoke({\"topic\": \"cats\", \"joke_count\": 3})\n",
        "\n",
        "# Output\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K9a2ZF8CrSrO",
        "outputId": "c24bff00-5dc3-4ce6-96f6-5eacc33b270e"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sure, here are three cat-themed jokes for you!\n",
            "\n",
            "1. Why did the cat sit on the computer?\n",
            "   Because it wanted to keep an eye on the mouse!\n",
            "\n",
            "2. What do you call a pile of cats?\n",
            "   A meowtain!\n",
            "\n",
            "3. Why was the cat so good at video games?\n",
            "   Because it had nine lives — one for each level!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Custom runnnable/Chain\n"
      ],
      "metadata": {
        "id": "YXQedniysApn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.schema.output_parser import StrOutputParser\n",
        "from langchain.schema.runnable import RunnableLambda\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "#environment variables\n",
        "from google.colab import userdata\n",
        "api=userdata.get('OPEN_AI_KEY')\n",
        "\n",
        "# Create a ChatOpenAI model\n",
        "model = ChatOpenAI(model=\"gpt-4o-mini\",api_key=api)\n",
        "\n",
        "# Define prompt templates\n",
        "prompt_template = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"You are a comedian who tells jokes about {topic}.\"),\n",
        "        (\"human\", \"Tell me {joke_count} jokes.\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Define additional processing steps using RunnableLambda\n",
        "uppercase_output = RunnableLambda(lambda x: x.upper())\n",
        "count_words = RunnableLambda(lambda x: f\"Word count: {len(x.split())}\\n{x}\")\n",
        "\n",
        "# Create the combined chain using LangChain Expression Language (LCEL)\n",
        "chain = prompt_template | model | StrOutputParser() | uppercase_output | count_words\n",
        "\n",
        "# Run the chain\n",
        "result = chain.invoke({\"topic\": \"cats\", \"joke_count\": 3})\n",
        "\n",
        "# Output\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RXISKPg4o_RC",
        "outputId": "9fcd7b97-c54b-4b30-abe9-c2916b616fcf"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word count: 63\n",
            "SURE! HERE ARE THREE CAT-THEMED JOKES FOR YOU:\n",
            "\n",
            "1. WHY DID THE CAT SIT ON THE COMPUTER?\n",
            "   BECAUSE IT WANTED TO KEEP AN EYE ON THE MOUSE!\n",
            "\n",
            "2. WHAT DID THE CAT SAY WHEN IT LOST ALL ITS MONEY?\n",
            "   \"I'M PAW!\"\n",
            "\n",
            "3. WHY WAS THE CAT SO GOOD AT VIDEO GAMES?\n",
            "   BECAUSE IT HAD NINE LIVES TO PRACTICE! \n",
            "\n",
            "HOPE THOSE MADE YOU SMILE!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Parallel Chain"
      ],
      "metadata": {
        "id": "sDyYqRgssKCZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.schema.output_parser import StrOutputParser\n",
        "from langchain.schema.runnable import RunnableParallel, RunnableLambda\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "#environment variables\n",
        "from google.colab import userdata\n",
        "api=userdata.get('OPEN_AI_KEY')\n",
        "\n",
        "# Create a ChatOpenAI model\n",
        "model = ChatOpenAI(model=\"gpt-4o-mini\",api_key=api)\n",
        "\n",
        "# Define prompt template\n",
        "prompt_template = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"You are an expert product reviewer.\"),\n",
        "        (\"human\", \"List the main features of the product {product_name}.\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "\n",
        "# Define pros analysis step\n",
        "def analyze_pros(features):\n",
        "    pros_template = ChatPromptTemplate.from_messages(\n",
        "        [\n",
        "            (\"system\", \"You are an expert product reviewer.\"),\n",
        "            (\n",
        "                \"human\",\n",
        "                \"Given these features: {features}, list the pros of these features.\",\n",
        "            ),\n",
        "        ]\n",
        "    )\n",
        "    return pros_template.format_prompt(features=features)\n",
        "\n",
        "\n",
        "# Define cons analysis step\n",
        "def analyze_cons(features):\n",
        "    cons_template = ChatPromptTemplate.from_messages(\n",
        "        [\n",
        "            (\"system\", \"You are an expert product reviewer.\"),\n",
        "            (\n",
        "                \"human\",\n",
        "                \"Given these features: {features}, list the cons of these features.\",\n",
        "            ),\n",
        "        ]\n",
        "    )\n",
        "    return cons_template.format_prompt(features=features)\n",
        "\n",
        "\n",
        "# Combine pros and cons into a final review\n",
        "def combine_pros_cons(pros, cons):\n",
        "    return f\"Pros:\\n{pros}\\n\\nCons:\\n{cons}\"\n",
        "\n",
        "\n",
        "# Simplify branches with LCEL\n",
        "pros_branch_chain = (\n",
        "    RunnableLambda(lambda x: analyze_pros(x)) | model | StrOutputParser()\n",
        ")\n",
        "\n",
        "cons_branch_chain = (\n",
        "    RunnableLambda(lambda x: analyze_cons(x)) | model | StrOutputParser()\n",
        ")\n",
        "\n",
        "# Create the combined chain using LangChain Expression Language (LCEL)\n",
        "chain = (\n",
        "    prompt_template\n",
        "    | model\n",
        "    | StrOutputParser()\n",
        "    | RunnableParallel(branches={\"pros\": pros_branch_chain, \"cons\": cons_branch_chain})\n",
        "    | RunnableLambda(lambda x: combine_pros_cons(x[\"branches\"][\"pros\"], x[\"branches\"][\"cons\"]))\n",
        ")\n",
        "\n",
        "# Run the chain\n",
        "result = chain.invoke({\"product_name\": \"MacBook Pro\"})\n",
        "\n",
        "# Output\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GoTXss2bsGS3",
        "outputId": "6d8dc065-2c07-480b-acc3-913b5805ff01"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pros:\n",
            "The MacBook Pro includes a range of features that come with various advantages, making it a popular choice among professionals, creatives, and everyday users. Here are the pros associated with each feature:\n",
            "\n",
            "1. **Display**:\n",
            "   - **Pros**: The high-resolution Retina display with True Tone technology provides exceptional color accuracy and sharpness, enhancing visual experiences for tasks like photo editing, graphic design, and video consumption. The ability to choose between 13-inch and 16-inch models allows users to select a size that best fits their needs.\n",
            "\n",
            "2. **Processor Options**:\n",
            "   - **Pros**: The latest Apple silicon (M1, M1 Pro, M1 Max, and M2) delivers remarkable performance improvements, enabling faster processing for demanding applications. With optimized energy efficiency, these chips allow for better battery life and quieter operation, as they generate less heat compared to Intel-based models.\n",
            "\n",
            "3. **Memory (RAM)**:\n",
            "   - **Pros**: The availability of unified memory configurations (up to 64GB) supports multitasking and enhances performance in memory-intensive applications, making it ideal for professionals working with large files, such as 3D modeling or video editing.\n",
            "\n",
            "4. **Storage**:\n",
            "   - **Pros**: SSD storage options ranging from 256GB to 8TB offer exceptional speed for data access, significantly reducing load times for applications and files. The flexibility in storage size accommodates a variety of user needs, from casual users to data-heavy professionals.\n",
            "\n",
            "5. **Battery Life**:\n",
            "   - **Pros**: Impressive battery life of up to 20 hours allows users to work on the go without the constant need for charging, making it suitable for long work sessions, travel, or remote workplaces.\n",
            "\n",
            "6. **Graphics Performance**:\n",
            "   - **Pros**: Integrated graphics in the M1 Pro and M1 Max chips provide superb graphics performance, making the MacBook Pro highly capable for creative tasks such as video editing, 3D rendering, and even gaming, without the need for additional graphics hardware.\n",
            "\n",
            "7. **Ports and Connectivity**:\n",
            "   - **Pros**: Multiple Thunderbolt 4/USB-C ports, along with HDMI and SDXC card slots, enhance connectivity options for external displays, peripherals, and other devices. Support for Wi-Fi 6 ensures faster internet speeds and better performance in crowded networks.\n",
            "\n",
            "8. **Keyboard and Trackpad**:\n",
            "   - **Pros**: The Magic Keyboard with scissor mechanism offers a comfortable and responsive typing experience, while the large Force Touch trackpad provides precise control and gestures, enhancing productivity and usability.\n",
            "\n",
            "9. **Audio and Video**:\n",
            "   - **Pros**: The high-fidelity six-speaker sound system and studio-quality microphones deliver excellent audio quality for music, movies, and calls. The 1080p FaceTime HD camera improves video conferencing experiences, which is crucial for remote work and collaboration.\n",
            "\n",
            "10. **Operating System**:\n",
            "    - **Pros**: macOS is user-friendly and seamlessly integrates with other Apple products and services, enhancing user experience and providing a cohesive ecosystem for Apple users. Regular updates ensure security and access to new features.\n",
            "\n",
            "11. **Build Quality**:\n",
            "    - **Pros**: The sleek, lightweight aluminum design not only provides aesthetic appeal but also ensures durability and portability, allowing users to carry the laptop easily without compromising on build integrity.\n",
            "\n",
            "Overall, these pros make the MacBook Pro a versatile and powerful laptop well-suited for a wide range of users, from professionals to creatives and general consumers.\n",
            "\n",
            "Cons:\n",
            "While the MacBook Pro has a lot of impressive features, it also has its downsides. Here’s a look at the cons associated with the features mentioned:\n",
            "\n",
            "1. **Display**: \n",
            "   - High glare under direct sunlight can make it difficult to view.\n",
            "   - Lack of touch screen may disappoint users accustomed to that feature on other laptops.\n",
            "\n",
            "2. **Processor Options**: \n",
            "   - Being dependent on Apple's proprietary chips can limit options for users who prefer Intel, especially for specific software compatibility.\n",
            "   - The higher-end chips can be quite expensive, making it inaccessible for some buyers.\n",
            "\n",
            "3. **Memory (RAM)**: \n",
            "   - Non-upgradable RAM means users must choose their memory configuration wisely at purchase, as it cannot be expanded later.\n",
            "   - Maximum configurations (64GB) can be expensive and may not be necessary for average users.\n",
            "\n",
            "4. **Storage**: \n",
            "   - SSD options can be priced steeply, particularly for higher-capacity drives (8TB).\n",
            "   - As with RAM, the non-upgradable storage means users must pay for the amount they think they'll need upfront.\n",
            "\n",
            "5. **Battery Life**: \n",
            "   - While impressive on paper, real-world usage may yield lower battery life, particularly under heavy workloads or gaming.\n",
            "   - Battery longevity can diminish over time, requiring expensive replacement options.\n",
            "\n",
            "6. **Graphics Performance**: \n",
            "   - Integrated graphics, while powerful, may not match the performance level of high-end dedicated GPUs for hardcore gaming or professional-grade graphic work.\n",
            "   - Applications that rely on specific graphics card drivers may perform poorly.\n",
            "\n",
            "7. **Ports and Connectivity**: \n",
            "   - Having only USB-C/Thunderbolt ports may require users to rely on multiple adapters for peripherals, which can be cumbersome.\n",
            "   - HDMI support is limited to certain resolutions or capabilities compared to dedicated graphics solutions.\n",
            "\n",
            "8. **Keyboard and Trackpad**: \n",
            "   - Some users may find the Magic Keyboard to lack tactile feedback compared to traditional keyboards.\n",
            "   - The large trackpad may inadvertently receive input causing slight frustration for some; \n",
            "\n",
            "9. **Audio and Video**: \n",
            "   - The high-fidelity audio system, while good, might not be adequate for audiophiles who prefer high-end audio equipment.\n",
            "   - The 1080p FaceTime HD camera, while decent, is still outclassed by some contemporary laptops that offer higher resolution webcams.\n",
            "\n",
            "10. **Operating System**: \n",
            "    - macOS may not be suitable for users who are more comfortable with Windows or other operating systems.\n",
            "    - Certain software and games may not be available or work optimally on macOS, potentially limiting user capabilities.\n",
            "\n",
            "11. **Build Quality**: \n",
            "    - While durable, the aluminum body can be prone to dents and scratches if not handled carefully.\n",
            "    - Lightness can lead to a perception of less ruggedness compared to some more heavy-duty laptops.\n",
            "\n",
            "In summary, while the MacBook Pro excels in many areas, it does have its shortcomings that potential buyers should consider based on their individual needs and use cases.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Branched Chains"
      ],
      "metadata": {
        "id": "0AEfkQue3CSV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.schema.output_parser import StrOutputParser\n",
        "from langchain.schema.runnable import RunnableBranch\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "#environment variables\n",
        "from google.colab import userdata\n",
        "api=userdata.get('OPEN_AI_KEY')\n",
        "\n",
        "# Create a ChatOpenAI model\n",
        "model = ChatOpenAI(model=\"gpt-4o-mini\",api_key=api)\n",
        "\n",
        "\n",
        "# Define prompt templates for different feedback types\n",
        "positive_feedback_template = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"You are a helpful assistant.\"),\n",
        "        (\"human\",\n",
        "         \"Generate a thank you note for this positive feedback: {feedback}.\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "negative_feedback_template = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"You are a helpful assistant.\"),\n",
        "        (\"human\",\n",
        "         \"Generate a response addressing this negative feedback: {feedback}.\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "neutral_feedback_template = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"You are a helpful assistant.\"),\n",
        "        (\n",
        "            \"human\",\n",
        "            \"Generate a request for more details for this neutral feedback: {feedback}.\",\n",
        "        ),\n",
        "    ]\n",
        ")\n",
        "\n",
        "escalate_feedback_template = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"You are a helpful assistant.\"),\n",
        "        (\n",
        "            \"human\",\n",
        "            \"Generate a message to escalate this feedback to a human agent: {feedback}.\",\n",
        "        ),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Define the feedback classification template\n",
        "classification_template = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"You are a helpful assistant.\"),\n",
        "        (\"human\",\n",
        "         \"Classify the sentiment of this feedback as positive, negative, neutral, or escalate: {feedback}.\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Define the runnable branches for handling feedback\n",
        "branches = RunnableBranch(\n",
        "    (\n",
        "        lambda x: \"positive\" in x,\n",
        "        positive_feedback_template | model | StrOutputParser()  # Positive feedback chain\n",
        "    ),\n",
        "    (\n",
        "        lambda x: \"negative\" in x,\n",
        "        negative_feedback_template | model | StrOutputParser()  # Negative feedback chain\n",
        "    ),\n",
        "    (\n",
        "        lambda x: \"neutral\" in x,\n",
        "        neutral_feedback_template | model | StrOutputParser()  # Neutral feedback chain\n",
        "    ),\n",
        "    escalate_feedback_template | model | StrOutputParser()\n",
        ")\n",
        "\n",
        "# Create the classification chain\n",
        "classification_chain = classification_template | model | StrOutputParser()\n",
        "\n",
        "# Combine classification and response generation into one chain\n",
        "chain = classification_chain | branches\n",
        "\n",
        "# Run the chain with an example review\n",
        "# Good review - \"The product is excellent. I really enjoyed using it and found it very helpful.\"\n",
        "# Bad review - \"The product is terrible. It broke after just one use and the quality is very poor.\"\n",
        "# Neutral review - \"The product is okay. It works as expected but nothing exceptional.\"\n",
        "# Default - \"I'm not sure about the product yet. Can you tell me more about its features and benefits?\"\n",
        "\n",
        "review = \"The product is terrible. It broke after just one use and the quality is very poor.\"\n",
        "result = chain.invoke({\"feedback\": review})\n",
        "\n",
        "# Output the result\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qtafj7wAtR0N",
        "outputId": "5423bc28-e34b-4159-9479-66ca22034d54"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thank you for sharing your feedback with us. I’m sorry to hear that your experience did not meet your expectations. We value your insights and would like to understand your concerns better. Please let us know more about what specifically did not work for you so we can address it and improve. Your satisfaction is important to us, and we are committed to making things right.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QyuoKqbO6B_S"
      },
      "execution_count": 15,
      "outputs": []
    }
  ]
}