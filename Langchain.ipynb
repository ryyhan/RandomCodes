{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNEyZV/xEj82HYpLCYGCHBA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ryyhan/RandomCodes/blob/main/Langchain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Basic Chat\n"
      ],
      "metadata": {
        "id": "WLjYXD-eDade"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade langchain langchain_core langchain_openai langchain_community chromadb openai tiktoken"
      ],
      "metadata": {
        "collapsed": true,
        "id": "3iYxf6v9DsFW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ae20903-c82b-4e15-90a9-97eed94426a1"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.25)\n",
            "Requirement already satisfied: langchain_core in /usr/local/lib/python3.11/dist-packages (0.3.59)\n",
            "Requirement already satisfied: langchain_openai in /usr/local/lib/python3.11/dist-packages (0.3.16)\n",
            "Requirement already satisfied: langchain_community in /usr/local/lib/python3.11/dist-packages (0.3.24)\n",
            "Requirement already satisfied: chromadb in /usr/local/lib/python3.11/dist-packages (1.0.8)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (1.78.1)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.11/dist-packages (0.9.0)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.8)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.39)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.11.4)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.40)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain_core) (9.1.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain_core) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain_core) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain_core) (4.13.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (3.11.15)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.9.1)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.4.0)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.0.2)\n",
            "Requirement already satisfied: build>=1.0.3 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.2.2.post1)\n",
            "Requirement already satisfied: fastapi==0.115.9 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.115.9)\n",
            "Requirement already satisfied: uvicorn>=0.18.3 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.34.2)\n",
            "Requirement already satisfied: posthog>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (4.0.1)\n",
            "Requirement already satisfied: onnxruntime>=1.14.1 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.22.0)\n",
            "Requirement already satisfied: opentelemetry-api>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.33.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.33.0)\n",
            "Requirement already satisfied: opentelemetry-instrumentation-fastapi>=0.41b0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.54b0)\n",
            "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.33.0)\n",
            "Requirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.21.1)\n",
            "Requirement already satisfied: pypika>=0.48.9 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.48.9)\n",
            "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (4.67.1)\n",
            "Requirement already satisfied: overrides>=7.3.1 in /usr/local/lib/python3.11/dist-packages (from chromadb) (7.7.0)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.11/dist-packages (from chromadb) (6.5.2)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.71.0)\n",
            "Requirement already satisfied: bcrypt>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from chromadb) (4.3.0)\n",
            "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.15.3)\n",
            "Requirement already satisfied: kubernetes>=28.1.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (32.0.1)\n",
            "Requirement already satisfied: mmh3>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from chromadb) (5.1.0)\n",
            "Requirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.11/dist-packages (from chromadb) (3.10.18)\n",
            "Requirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.28.1)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (13.9.4)\n",
            "Requirement already satisfied: jsonschema>=4.19.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (4.23.0)\n",
            "Requirement already satisfied: starlette<0.46.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from fastapi==0.115.9->chromadb) (0.45.3)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.9.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2024.11.6)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.20.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.11/dist-packages (from build>=1.0.3->chromadb) (1.2.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (0.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain_core) (3.0.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (0.24.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.9.0.post0)\n",
            "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.38.0)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\n",
            "Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (3.2.2)\n",
            "Requirement already satisfied: urllib3>=1.24.2 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.4.0)\n",
            "Requirement already satisfied: durationpy>=0.7 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (0.9)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (25.2.10)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (5.29.4)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.13.1)\n",
            "Requirement already satisfied: deprecated>=1.2.6 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (1.2.18)\n",
            "Requirement already satisfied: importlib-metadata<8.7.0,>=6.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (8.6.1)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.70.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.33.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.33.0)\n",
            "Requirement already satisfied: opentelemetry-proto==1.33.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.33.0)\n",
            "Requirement already satisfied: opentelemetry-instrumentation-asgi==0.54b0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.54b0)\n",
            "Requirement already satisfied: opentelemetry-instrumentation==0.54b0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.54b0)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions==0.54b0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.54b0)\n",
            "Requirement already satisfied: opentelemetry-util-http==0.54b0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.54b0)\n",
            "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-instrumentation==0.54b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (1.17.2)\n",
            "Requirement already satisfied: asgiref~=3.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-instrumentation-asgi==0.54b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (3.8.1)\n",
            "Requirement already satisfied: backoff>=1.10.0 in /usr/local/lib/python3.11/dist-packages (from posthog>=2.4.0->chromadb) (2.2.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.0)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain_community) (1.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->chromadb) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->chromadb) (2.19.1)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.11/dist-packages (from tokenizers>=0.13.2->chromadb) (0.30.2)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.9.0->chromadb) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.9.0->chromadb) (1.5.4)\n",
            "Requirement already satisfied: httptools>=0.6.3 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.6.4)\n",
            "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.21.0)\n",
            "Requirement already satisfied: watchfiles>=0.13 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.0.5)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (15.0.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2025.3.2)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata<8.7.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.21.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community) (1.1.0)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.11/dist-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2MmHoZ6hApFa",
        "outputId": "edd819a7-2791-400b-86c3-702aacbcb84c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Full result:\n",
            "content='81 divided by 9 equals 9.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 16, 'total_tokens': 26, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_0392822090', 'id': 'chatcmpl-BWOWKGESmbpwCKaMmqRz9oOgd8g4q', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='run--7098e3c8-5034-47fb-a51b-fd4247e51321-0' usage_metadata={'input_tokens': 16, 'output_tokens': 10, 'total_tokens': 26, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
            "Content only:\n",
            "81 divided by 9 equals 9.\n"
          ]
        }
      ],
      "source": [
        "#imports\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "#environment variables\n",
        "from google.colab import userdata\n",
        "api=userdata.get('OPEN_AI_KEY')\n",
        "\n",
        "\n",
        "# Create a ChatOpenAI model\n",
        "model = ChatOpenAI(model=\"gpt-4o-mini\",api_key=api)\n",
        "\n",
        "# Invoke the model with a message\n",
        "result = model.invoke(\"What is 81 divided by 9?\")\n",
        "print(\"Full result:\")\n",
        "print(result)\n",
        "print(\"Content only:\")\n",
        "print(result.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conversation"
      ],
      "metadata": {
        "id": "RlmNyBYVLjTp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "\n",
        "#environment variables\n",
        "from google.colab import userdata\n",
        "api=userdata.get('OPEN_AI_KEY')\n",
        "\n",
        "\n",
        "# Create a ChatOpenAI model\n",
        "model = ChatOpenAI(model=\"gpt-4o-mini\",api_key=api)\n",
        "\n",
        "\n",
        "messages = [\n",
        "    SystemMessage(content=\"Solve the following math problems\"),\n",
        "    HumanMessage(content=\"What is 81 divided by 9?\"),\n",
        "]\n",
        "\n",
        "# Invoke the model with messages\n",
        "result = model.invoke(messages)\n",
        "print(f\"Answer from AI (Single): {result.content}\")\n",
        "\n",
        "\n",
        "# AIMessage:\n",
        "#   Message from an AI.\n",
        "messages = [\n",
        "    SystemMessage(content=\"Solve the following math problems\"),\n",
        "    HumanMessage(content=\"What is 81 divided by 9?\"),\n",
        "    AIMessage(content=\"81 divided by 9 is 9.\"),\n",
        "    HumanMessage(content=\"What is 10 times 5?\"),\n",
        "]\n",
        "\n",
        "# Invoke the model with messages\n",
        "result = model.invoke(messages)\n",
        "print(f\"Answer from AI (Multiple): {result.content}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Omxka0qoDdD8",
        "outputId": "78a280af-76b9-4634-ae3c-d2d6e741255c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer from AI (Single): 81 divided by 9 equals 9.\n",
            "Answer from AI (Multiple): 10 times 5 is 50.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Chatbot Conversations"
      ],
      "metadata": {
        "id": "baW8PAR8RtT2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.schema import AIMessage, HumanMessage, SystemMessage\n",
        "\n",
        "#environment variables\n",
        "from google.colab import userdata\n",
        "api=userdata.get('OPEN_AI_KEY')\n",
        "\n",
        "\n",
        "# Create a ChatOpenAI model\n",
        "model = ChatOpenAI(model=\"gpt-4o-mini\",api_key=api)\n",
        "\n",
        "\n",
        "chat_history = []  # Use a list to store messages\n",
        "\n",
        "# Set an initial system message (optional)\n",
        "system_message = SystemMessage(content=\"You are a helpful AI assistant.\")\n",
        "chat_history.append(system_message)  # Add system message to chat history\n",
        "\n",
        "# Chat loop\n",
        "while True:\n",
        "    query = input(\"You: \")\n",
        "    if query.lower() == \"exit\":\n",
        "        break\n",
        "    chat_history.append(HumanMessage(content=query))  # Add user message\n",
        "\n",
        "    # Get AI response using history\n",
        "    result = model.invoke(chat_history)\n",
        "    response = result.content\n",
        "    chat_history.append(AIMessage(content=response))  # Add AI message\n",
        "\n",
        "    print(f\"AI: {response}\")\n",
        "\n",
        "\n",
        "print(\"---- Message History ----\")\n",
        "print(chat_history)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dmchOq7YRs85",
        "outputId": "cb2fb1a8-2809-4d3c-b1b4-7624b6f9cd52"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You: \n",
            "AI: Hello! How can I assist you today?\n",
            "You: \n",
            "AI: It seems you might not have entered a message. Is there something specific you'd like to ask or talk about? I'm here to help!\n",
            "You: exit\n",
            "---- Message History ----\n",
            "[SystemMessage(content='You are a helpful AI assistant.', additional_kwargs={}, response_metadata={}), HumanMessage(content='', additional_kwargs={}, response_metadata={}), AIMessage(content='Hello! How can I assist you today?', additional_kwargs={}, response_metadata={}), HumanMessage(content='', additional_kwargs={}, response_metadata={}), AIMessage(content=\"It seems you might not have entered a message. Is there something specific you'd like to ask or talk about? I'm here to help!\", additional_kwargs={}, response_metadata={})]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example Source: https://python.langchain.com/v0.2/docs/integrations/memory/google_firestore/\n",
        "\n",
        "from google.cloud import firestore\n",
        "from langchain_google_firestore import FirestoreChatMessageHistory\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "\"\"\"\n",
        "Steps to replicate this example:\n",
        "1. Create a Firebase account\n",
        "2. Create a new Firebase project\n",
        "    - Copy the project ID\n",
        "3. Create a Firestore database in the Firebase project\n",
        "4. Install the Google Cloud CLI on your computer\n",
        "    - https://cloud.google.com/sdk/docs/install\n",
        "    - Authenticate the Google Cloud CLI with your Google account\n",
        "        - https://cloud.google.com/docs/authentication/provide-credentials-adc#local-dev\n",
        "    - Set your default project to the new Firebase project you created\n",
        "5. Enable the Firestore API in the Google Cloud Console:\n",
        "    - https://console.cloud.google.com/apis/enableflow?apiid=firestore.googleapis.com&project=crewai-automation\n",
        "\"\"\"\n",
        "\n",
        "#environment variables\n",
        "from google.colab import userdata\n",
        "api=userdata.get('OPEN_AI_KEY')\n",
        "\n",
        "\n",
        "# Setup Firebase Firestore\n",
        "PROJECT_ID = \"langchain-demo-abf48\"\n",
        "SESSION_ID = \"user_session_new\"  # This could be a username or a unique ID\n",
        "COLLECTION_NAME = \"chat_history\"\n",
        "\n",
        "# Initialize Firestore Client\n",
        "print(\"Initializing Firestore Client...\")\n",
        "client = firestore.Client(project=PROJECT_ID)\n",
        "\n",
        "# Initialize Firestore Chat Message History\n",
        "print(\"Initializing Firestore Chat Message History...\")\n",
        "chat_history = FirestoreChatMessageHistory(\n",
        "    session_id=SESSION_ID,\n",
        "    collection=COLLECTION_NAME,\n",
        "    client=client,\n",
        ")\n",
        "print(\"Chat History Initialized.\")\n",
        "print(\"Current Chat History:\", chat_history.messages)\n",
        "\n",
        "# Create a ChatOpenAI model\n",
        "model = ChatOpenAI(model=\"gpt-4o-mini\",api_key=api)\n",
        "\n",
        "print(\"Start chatting with the AI. Type 'exit' to quit.\")\n",
        "\n",
        "while True:\n",
        "    human_input = input(\"User: \")\n",
        "    if human_input.lower() == \"exit\":\n",
        "        break\n",
        "\n",
        "    chat_history.add_user_message(human_input)\n",
        "\n",
        "    ai_response = model.invoke(chat_history.messages)\n",
        "    chat_history.add_ai_message(ai_response.content)\n",
        "\n",
        "    print(f\"AI: {ai_response.content}\")"
      ],
      "metadata": {
        "id": "ifnx-HPIL8oe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "u8zseiV6hSUd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#prompting"
      ],
      "metadata": {
        "id": "X_Aq47ZAhT_g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "0TDhv16OhXgR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prompt Template Docs:\n",
        "#   https://python.langchain.com/v0.2/docs/concepts/#prompt-templateshttps://python.langchain.com/v0.2/docs/concepts/#prompt-templates\n",
        "\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain_core.messages import HumanMessage\n",
        "\n",
        "# # PART 1: Create a ChatPromptTemplate using a template string\n",
        "template = \"Tell me a joke about {topic}.\"\n",
        "prompt_template = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "print(\"-----Prompt from Template-----\")\n",
        "prompt = prompt_template.invoke({\"topic\": \"cats\"})\n",
        "print(prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WHe2WsKShTHI",
        "outputId": "b8891e45-85f4-48ac-b735-834af71432c4"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----Prompt from Template-----\n",
            "messages=[HumanMessage(content='Tell me a joke about cats.', additional_kwargs={}, response_metadata={})]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# PART 2: Prompt with Multiple Placeholders\n",
        "template_multiple = \"\"\"You are a helpful assistant.\n",
        "Human: Tell me a {adjective} story about a {animal}.\n",
        "Assistant:\"\"\"\n",
        "prompt_multiple = ChatPromptTemplate.from_template(template_multiple)\n",
        "prompt = prompt_multiple.invoke({\"adjective\": \"funny\", \"animal\": \"panda\"})\n",
        "print(\"\\n----- Prompt with Multiple Placeholders -----\\n\")\n",
        "print(prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mQDYTH4DJkBx",
        "outputId": "218a45ec-de3e-447a-ce8e-294eec18136d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "----- Prompt with Multiple Placeholders -----\n",
            "\n",
            "messages=[HumanMessage(content='You are a helpful assistant.\\nHuman: Tell me a funny story about a panda.\\nAssistant:', additional_kwargs={}, response_metadata={})]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prompt Template with ChatModel"
      ],
      "metadata": {
        "id": "LIFjKTV5LlBR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "#environment variables\n",
        "from google.colab import userdata\n",
        "api=userdata.get('OPEN_AI_KEY')\n",
        "\n",
        "# Create a ChatOpenAI model\n",
        "model = ChatOpenAI(model=\"gpt-4o-mini\",api_key=api)\n",
        "\n",
        "# PART 1: Create a ChatPromptTemplate using a template string\n",
        "print(\"-----Prompt from Template-----\")\n",
        "template = \"Tell me a joke about {topic}.\"\n",
        "prompt_template = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "prompt = prompt_template.invoke({\"topic\": \"cats\"})\n",
        "result = model.invoke(prompt)\n",
        "print(prompt, result.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6TJlO_OTKyT7",
        "outputId": "b02520fb-3558-4893-f208-3c0d8f66cf0d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----Prompt from Template-----\n",
            "messages=[HumanMessage(content='Tell me a joke about cats.', additional_kwargs={}, response_metadata={})] Why was the cat sitting on the computer?\n",
            "\n",
            "Because it wanted to keep an eye on the mouse!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# PART 2: Prompt with Multiple Placeholders\n",
        "print(\"\\n----- Prompt with Multiple Placeholders -----\\n\")\n",
        "template_multiple = \"\"\"You are a helpful assistant.\n",
        "Human: Tell me a {adjective} short story about a {animal}.\n",
        "Assistant:\"\"\"\n",
        "prompt_multiple = ChatPromptTemplate.from_template(template_multiple)\n",
        "prompt = prompt_multiple.invoke({\"adjective\": \"funny\", \"animal\": \"cat\"})\n",
        "\n",
        "result = model.invoke(prompt)\n",
        "print(prompt,\"/n\\n\",result.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "usPIM502lUQy",
        "outputId": "1f14a652-df1c-434f-e4d8-492b9cda4c07"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "----- Prompt with Multiple Placeholders -----\n",
            "\n",
            "messages=[HumanMessage(content='You are a helpful assistant.\\nHuman: Tell me a funny short story about a cat.\\nAssistant:', additional_kwargs={}, response_metadata={})] /n\n",
            " Once upon a time, there was a mischievous cat named Whiskers who lived with a kind old lady named Mrs. Jenkins. Whiskers had a peculiar hobby: he loved to steal socks. It didn't matter if they were colorful, striped, or polka-dotted; he was an equal opportunity sock thief.\n",
            "\n",
            "One day, Mrs. Jenkins decided to host a tea party for her friends. Per usual, she put on her favorite floral dress and set the table with her best china. As she bustled around the house preparing, little did she know that Whiskers was up to no good in her bedroom.\n",
            "\n",
            "With a swift leap, he dove into Mrs. Jenkins’ sock drawer, sending a cascade of socks flying into the air. In a matter of minutes, he had gathered a small mountain of them and was proudly strutting around the house looking like a furry, sock-clad lion.\n",
            "\n",
            "Just as the doorbell rang, announcing the arrival of her guests, Mrs. Jenkins noticed an unusual sight: Whiskers parading through the living room, draped in a rainbow of socks like a fashion model on a catwalk. Her friends gasped and then burst into laughter.\n",
            "\n",
            "“Whiskers, you’re ready for the party!” Mrs. Jenkins chuckled, as she picked up a pair of bright pink socks that had somehow ended up on the chandelier.\n",
            "\n",
            "From that day on, Whiskers became the star of every tea party, captivating guests with his sock fashion shows. He never did stop stealing socks, but at least he made everyone smile. And Mrs. Jenkins embraced her new life as the proud owner of the most fashionable cat in town!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# PART 3: Prompt with System and Human Messages (Using Tuples)\n",
        "print(\"\\n----- Prompt with System and Human Messages (Tuple) -----\\n\")\n",
        "messages = [\n",
        "    (\"system\", \"You are a comedian who tells jokes about {topic}.\"),\n",
        "    (\"human\", \"Tell me {joke_count} jokes.\"),\n",
        "]\n",
        "prompt_template = ChatPromptTemplate.from_messages(messages)\n",
        "prompt = prompt_template.invoke({\"topic\": \"cats\", \"joke_count\": 3})\n",
        "result = model.invoke(prompt)\n",
        "print(prompt,\"\\n\\n\\n\")\n",
        "print(result.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ELOvkj-lgNO",
        "outputId": "343c07eb-5585-4fd2-89cf-d765c2c724e6"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "----- Prompt with System and Human Messages (Tuple) -----\n",
            "\n",
            "messages=[SystemMessage(content='You are a comedian who tells jokes about cats.', additional_kwargs={}, response_metadata={}), HumanMessage(content='Tell me 3 jokes.', additional_kwargs={}, response_metadata={})] \n",
            "\n",
            "\n",
            "\n",
            "Sure! Here are three cat-themed jokes for you:\n",
            "\n",
            "1. Why was the cat sitting on the computer?\n",
            "   Because it wanted to keep an eye on the mouse!\n",
            "\n",
            "2. What do you call a pile of cats?\n",
            "   A meowtain!\n",
            "\n",
            "3. Why did the cat join Instagram?\n",
            "   Because it wanted to be a \"purrfessional\" influencer! \n",
            "\n",
            "Hope these gave you a good laugh!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "TEHfwCPSl5Qe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chains"
      ],
      "metadata": {
        "id": "YRVbxvN0l_AC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "JvK3j7mlmBGG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "#environment variables\n",
        "from google.colab import userdata\n",
        "api=userdata.get('OPEN_AI_KEY')\n",
        "\n",
        "# Create a ChatOpenAI model\n",
        "model = ChatOpenAI(model=\"gpt-4o-mini\",api_key=api)\n",
        "\n",
        "# Define prompt templates (no need for separate Runnable chains)\n",
        "prompt_template = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"You are a comedian who tells jokes about {topic}.\"),\n",
        "        (\"human\", \"Tell me {joke_count} jokes.\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Create the combined chain using LangChain Expression Language (LCEL)\n",
        "chain = prompt_template | model | StrOutputParser()\n",
        "\n",
        "# Run the chain\n",
        "result = chain.invoke({\"topic\": \"cats\", \"joke_count\": 3})\n",
        "\n",
        "# Output\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FWkafui9lrd-",
        "outputId": "c581efc7-ae5b-44ef-a69f-212a757b8485"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sure! Here are three cat-themed jokes for you:\n",
            "\n",
            "1. Why was the cat sitting on the computer?\n",
            "   Because it wanted to keep an eye on the mouse!\n",
            "\n",
            "2. What do cats like to eat for breakfast?\n",
            "   Mice Krispies!\n",
            "\n",
            "3. Why did the cat join Instagram?\n",
            "   Because it wanted to be a purr-fessional influencer! \n",
            "\n",
            "Hope these made you smile!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Behind the scenes"
      ],
      "metadata": {
        "id": "zJq9hZB0rS6B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.schema.runnable import RunnableLambda, RunnableSequence\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "#environment variables\n",
        "from google.colab import userdata\n",
        "api=userdata.get('OPEN_AI_KEY')\n",
        "\n",
        "# Create a ChatOpenAI model\n",
        "model = ChatOpenAI(model=\"gpt-4o-mini\",api_key=api)\n",
        "\n",
        "# Define prompt templates\n",
        "prompt_template = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"You are a comedian who tells jokes about {topic}.\"),\n",
        "        (\"human\", \"Tell me {joke_count} jokes.\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Create individual runnables (steps in the chain)\n",
        "format_prompt = RunnableLambda(lambda x: prompt_template.format_prompt(**x))\n",
        "invoke_model = RunnableLambda(lambda x: model.invoke(x.to_messages()))\n",
        "parse_output = RunnableLambda(lambda x: x.content)\n",
        "\n",
        "# Create the RunnableSequence (equivalent to the LCEL chain)\n",
        "chain = RunnableSequence(first=format_prompt, middle=[invoke_model], last=parse_output)\n",
        "\n",
        "# Run the chain\n",
        "response = chain.invoke({\"topic\": \"cats\", \"joke_count\": 3})\n",
        "\n",
        "# Output\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K9a2ZF8CrSrO",
        "outputId": "c819d670-46be-4926-d36d-bd6a8516e8a0"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sure, here are three cat-themed jokes for you!\n",
            "\n",
            "1. Why did the cat sit on the computer?  \n",
            "   Because it wanted to keep an eye on the mouse!\n",
            "\n",
            "2. What do you call a pile of cats?  \n",
            "   A meow-tain!\n",
            "\n",
            "3. Why was the cat so good at video games?  \n",
            "   Because it had nine lives to master every level! \n",
            "\n",
            "Hope these made you smile!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Custom runnnable/Chain\n"
      ],
      "metadata": {
        "id": "YXQedniysApn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.schema.output_parser import StrOutputParser\n",
        "from langchain.schema.runnable import RunnableLambda\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "#environment variables\n",
        "from google.colab import userdata\n",
        "api=userdata.get('OPEN_AI_KEY')\n",
        "\n",
        "# Create a ChatOpenAI model\n",
        "model = ChatOpenAI(model=\"gpt-4o-mini\",api_key=api)\n",
        "\n",
        "# Define prompt templates\n",
        "prompt_template = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"You are a comedian who tells jokes about {topic}.\"),\n",
        "        (\"human\", \"Tell me {joke_count} jokes.\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Define additional processing steps using RunnableLambda\n",
        "uppercase_output = RunnableLambda(lambda x: x.upper())\n",
        "count_words = RunnableLambda(lambda x: f\"Word count: {len(x.split())}\\n{x}\")\n",
        "\n",
        "# Create the combined chain using LangChain Expression Language (LCEL)\n",
        "chain = prompt_template | model | StrOutputParser() | uppercase_output | count_words\n",
        "\n",
        "# Run the chain\n",
        "result = chain.invoke({\"topic\": \"cats\", \"joke_count\": 3})\n",
        "\n",
        "# Output\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RXISKPg4o_RC",
        "outputId": "8caca074-37b8-4f83-cc9e-3ea0906f7fb5"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word count: 59\n",
            "SURE! HERE ARE THREE CAT-THEMED JOKES FOR YOU:\n",
            "\n",
            "1. WHY DID THE CAT SIT ON THE COMPUTER?\n",
            "   BECAUSE IT WANTED TO KEEP AN EYE ON THE MOUSE!\n",
            "\n",
            "2. WHAT DO YOU CALL A PILE OF CATS?\n",
            "   A MEOWTAIN!\n",
            "\n",
            "3. WHY WAS THE CAT SO GOOD AT VIDEO GAMES?\n",
            "   BECAUSE IT HAD NINE LIVES TO SPARE! \n",
            "\n",
            "HOPE YOU ENJOYED THEM!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Parallel Chain"
      ],
      "metadata": {
        "id": "sDyYqRgssKCZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.schema.output_parser import StrOutputParser\n",
        "from langchain.schema.runnable import RunnableParallel, RunnableLambda\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "#environment variables\n",
        "from google.colab import userdata\n",
        "api=userdata.get('OPEN_AI_KEY')\n",
        "\n",
        "# Create a ChatOpenAI model\n",
        "model = ChatOpenAI(model=\"gpt-4o-mini\",api_key=api)\n",
        "\n",
        "# Define prompt template\n",
        "prompt_template = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"You are an expert product reviewer.\"),\n",
        "        (\"human\", \"List the main features of the product {product_name}.\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "\n",
        "# Define pros analysis step\n",
        "def analyze_pros(features):\n",
        "    pros_template = ChatPromptTemplate.from_messages(\n",
        "        [\n",
        "            (\"system\", \"You are an expert product reviewer.\"),\n",
        "            (\n",
        "                \"human\",\n",
        "                \"Given these features: {features}, list the pros of these features.\",\n",
        "            ),\n",
        "        ]\n",
        "    )\n",
        "    return pros_template.format_prompt(features=features)\n",
        "\n",
        "\n",
        "# Define cons analysis step\n",
        "def analyze_cons(features):\n",
        "    cons_template = ChatPromptTemplate.from_messages(\n",
        "        [\n",
        "            (\"system\", \"You are an expert product reviewer.\"),\n",
        "            (\n",
        "                \"human\",\n",
        "                \"Given these features: {features}, list the cons of these features.\",\n",
        "            ),\n",
        "        ]\n",
        "    )\n",
        "    return cons_template.format_prompt(features=features)\n",
        "\n",
        "\n",
        "# Combine pros and cons into a final review\n",
        "def combine_pros_cons(pros, cons):\n",
        "    return f\"Pros:\\n{pros}\\n\\nCons:\\n{cons}\"\n",
        "\n",
        "\n",
        "# Simplify branches with LCEL\n",
        "pros_branch_chain = (\n",
        "    RunnableLambda(lambda x: analyze_pros(x)) | model | StrOutputParser()\n",
        ")\n",
        "\n",
        "cons_branch_chain = (\n",
        "    RunnableLambda(lambda x: analyze_cons(x)) | model | StrOutputParser()\n",
        ")\n",
        "\n",
        "# Create the combined chain using LangChain Expression Language (LCEL)\n",
        "chain = (\n",
        "    prompt_template\n",
        "    | model\n",
        "    | StrOutputParser()\n",
        "    | RunnableParallel(branches={\"pros\": pros_branch_chain, \"cons\": cons_branch_chain})\n",
        "    | RunnableLambda(lambda x: combine_pros_cons(x[\"branches\"][\"pros\"], x[\"branches\"][\"cons\"]))\n",
        ")\n",
        "\n",
        "# Run the chain\n",
        "result = chain.invoke({\"product_name\": \"MacBook Pro\"})\n",
        "\n",
        "# Output\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GoTXss2bsGS3",
        "outputId": "a83d7cb1-04aa-46d2-9560-7c228b50e521"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pros:\n",
            "Based on the impressive features listed for the MacBook Pro models as of October 2023, here are the pros associated with them:\n",
            "\n",
            "1. **Display**:\n",
            "   - **Retina display with True Tone**: Provides exceptional color accuracy and clarity, making it ideal for graphic design, photo editing, and video production.\n",
            "   - **High brightness (up to 1600 nits)**: Enhances usability in bright environments, ensuring visibility without straining the eyes.\n",
            "   - **P3 wide color gamut**: Delivers vivid, lifelike colors that are perfect for creative professionals.\n",
            "   - **ProMotion technology (120Hz refresh rate)**: Provides smoother visual experiences, especially beneficial for tasks like video editing and gaming.\n",
            "\n",
            "2. **Processor**:\n",
            "   - **Apple M1, M1 Pro, M1 Max, or M2 chips**: Offers industry-leading performance and energy efficiency, enabling faster workflows and demanding tasks.\n",
            "   - **Unified memory architecture**: Reduces latency and increases data transfer speeds, enhancing overall performance.\n",
            "\n",
            "3. **Graphics**:\n",
            "   - **Powerful integrated graphics**: Capable of handling graphically intensive applications and tasks without the need for a dedicated GPU on most models.\n",
            "   - **Support for external GPUs**: Offers flexibility for users needing enhanced graphical processing for specialized tasks.\n",
            "\n",
            "4. **Battery Life**:\n",
            "   - **Up to 20 hours of usage**: Exceptional battery performance allows users to work longer without the need for frequent charging.\n",
            "   - **Fast charging capability**: Reduces downtime, enabling a quick recharge when needed.\n",
            "\n",
            "5. **Storage**:\n",
            "   - **SSD options up to 8TB**: Provides ample space for large files and applications, catering to professionals who need extensive storage.\n",
            "   - **Fast read/write speeds**: Ensures swift data access and enhances system responsiveness.\n",
            "\n",
            "6. **Memory**:\n",
            "   - **RAM options from 8GB to 64GB**: Offers scalability based on user needs, from casual users to professionals requiring extensive multitasking.\n",
            "   - **Unified memory architecture**: Enhances efficiency in handling processes, allowing for smoother operation when multitasking or running demanding software.\n",
            "\n",
            "7. **Design**:\n",
            "   - **Sleek, lightweight aluminum chassis**: Ensures portability without compromising on durability and aesthetic appeal.\n",
            "   - **Thin bezels**: Maximizes screen real estate, providing a more immersive viewing experience.\n",
            "\n",
            "8. **Keyboard and Trackpad**:\n",
            "   - **Magic Keyboard with refined scissor mechanism**: Enhances comfort and typing accuracy, reducing fatigue during long typing sessions.\n",
            "   - **Large Force Touch trackpad**: Offers intuitive control and gesture support, enhancing the overall user experience.\n",
            "\n",
            "9. **Connectivity**:\n",
            "   - **Multiple Thunderbolt 4/USB-C ports**: Provides versatility in connectivity with high-speed transfer rates and support for various devices.\n",
            "   - **HDMI and SD card slot**: Eliminates the need for additional dongles, streamlining the connection of cameras and external displays.\n",
            "   - **MagSafe charging port**: Offers ease of use and safety, easily detaching in case of accidental pulls, preventing device damage.\n",
            "\n",
            "10. **Audio and Video**:\n",
            "    - **High-fidelity six-speaker sound system**: Delivers immersive sound quality, making it ideal for media consumption and content creation.\n",
            "    - **Studio-quality microphone array**: Ensures clear audio during calls and recordings, suited for professionals conducting video conferences or podcasts.\n",
            "    - **1080p FaceTime HD camera**: Provides sharp video quality for improved video calls, essential for remote work and virtual meetings.\n",
            "\n",
            "11. **Operating System**:\n",
            "    - **macOS with features like Universal Control and AirPlay to Mac**: Promotes seamless integration within the Apple ecosystem, enhancing productivity and user experience.\n",
            "\n",
            "Overall, these features establish the MacBook Pro as a robust and versatile tool for professionals, creatives, and everyday users alike, combining performance, innovation, and user-centric design.\n",
            "\n",
            "Cons:\n",
            "While the MacBook Pro offers numerous features and advantages, there are also some potential downsides to consider:\n",
            "\n",
            "1. **Display**:\n",
            "   - **Brightness vs. Power Consumption**: While the high brightness (up to 1600 nits) is impressive, it can lead to increased battery consumption when operating at maximum brightness.\n",
            "   - **Price**: The premium display technology may contribute to a higher overall price tag compared to other laptops with standard displays.\n",
            "\n",
            "2. **Processor**:\n",
            "   - **Software Compatibility**: The transition to Apple silicon (M1, M1 Pro, M1 Max, M2 chips) can lead to compatibility issues with older software that hasn’t been updated for ARM architecture.\n",
            "   - **Cooling**: Some users have reported that under heavy workloads, the fans can become loud, particularly in the higher-end models.\n",
            "\n",
            "3. **Graphics**:\n",
            "   - **Integrated GPU Limitations**: While the integrated graphics on the M1 Pro and M1 Max are powerful, they may not match the performance of high-end dedicated GPUs for gaming or intensive graphics tasks.\n",
            "\n",
            "4. **Battery Life**:\n",
            "   - **Variable Performance**: Real-world battery life often varies based on usage patterns, and power-intensive applications may lead to shorter battery life than advertised.\n",
            "   - **No Removable Battery**: Users cannot replace the battery themselves if it degrades over time, necessitating a trip to an Apple service center.\n",
            "\n",
            "5. **Storage**:\n",
            "   - **Fixed Storage Options**: Storage configurations are set at the time of purchase and cannot be upgraded later, which may be a limitation for users needing more space down the line.\n",
            "   - **Cost of Upgrades**: Upgrading to larger SSD options can significantly increase the overall cost of the machine.\n",
            "\n",
            "6. **Memory**:\n",
            "   - **Unified Memory Limitation**: While unified memory architecture offers advantages, it means that users can't upgrade RAM after purchase, locking them into their initial configuration.\n",
            "   - **Higher Costs for Upgrades**: Purchasing devices with higher RAM configurations can lead to a steep increase in price.\n",
            "\n",
            "7. **Design**:\n",
            "   - **Portability vs. Performance**: While the sleek design is visually appealing and lightweight, it may not be as rugged as some competing models that are built to withstand harsher conditions.\n",
            "   - **Limited Color Options**: The color choices tend to be more conservative, which may not appeal to those seeking more vibrant designs.\n",
            "\n",
            "8. **Keyboard and Trackpad**:\n",
            "   - **Familiarity**: Users accustomed to other keyboard layouts may find it takes time to adjust to Apple's Magic Keyboard.\n",
            "   - **Trackpad Size**: While the large trackpad is great for gestures, it may take up space that some users might prefer to have clear.\n",
            "\n",
            "9. **Connectivity**:\n",
            "   - **Limited Ports**: Though the addition of HDMI and MagSafe is appreciated, some users may still find the number of ports insufficient, leading to reliance on adapters and dongles.\n",
            "   - **Dependence on Thunderbolt Ecosystem**: Users without Thunderbolt devices may feel limited in their connectivity options.\n",
            "\n",
            "10. **Audio and Video**:\n",
            "    - **Quality Variations**: While the audio and microphone quality is generally excellent, there may be variability depending on environmental conditions (like background noise).\n",
            "    - **Camera Limitations**: While a 1080p camera is good for most users, it may still lag behind dedicated external webcams in terms of overall image quality, particularly in low-light situations.\n",
            "\n",
            "11. **Operating System**:\n",
            "    - **Learning Curve**: Users transitioning from Windows may face a learning curve when adapting to macOS, which can be challenging for some.\n",
            "    - **Software Availability**: Certain software and games may not be available or fully optimized for macOS, which could limit the device’s versatility for some users.\n",
            "\n",
            "Overall, while the MacBook Pro is a powerhouse with compelling features suitable for various tasks, these potential drawbacks should be carefully considered by prospective buyers based on their specific needs and usage scenarios.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Branched Chains"
      ],
      "metadata": {
        "id": "0AEfkQue3CSV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.schema.output_parser import StrOutputParser\n",
        "from langchain.schema.runnable import RunnableBranch\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "#environment variables\n",
        "from google.colab import userdata\n",
        "api=userdata.get('OPEN_AI_KEY')\n",
        "\n",
        "# Create a ChatOpenAI model\n",
        "model = ChatOpenAI(model=\"gpt-4o-mini\",api_key=api)\n",
        "\n",
        "\n",
        "# Define prompt templates for different feedback types\n",
        "positive_feedback_template = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"You are a helpful assistant.\"),\n",
        "        (\"human\",\n",
        "         \"Generate a thank you note for this positive feedback: {feedback}.\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "negative_feedback_template = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"You are a helpful assistant.\"),\n",
        "        (\"human\",\n",
        "         \"Generate a response addressing this negative feedback: {feedback}.\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "neutral_feedback_template = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"You are a helpful assistant.\"),\n",
        "        (\n",
        "            \"human\",\n",
        "            \"Generate a request for more details for this neutral feedback: {feedback}.\",\n",
        "        ),\n",
        "    ]\n",
        ")\n",
        "\n",
        "escalate_feedback_template = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"You are a helpful assistant.\"),\n",
        "        (\n",
        "            \"human\",\n",
        "            \"Generate a message to escalate this feedback to a human agent: {feedback}.\",\n",
        "        ),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Define the feedback classification template\n",
        "classification_template = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"You are a helpful assistant.\"),\n",
        "        (\"human\",\n",
        "         \"Classify the sentiment of this feedback as positive, negative, neutral, or escalate: {feedback}.\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Define the runnable branches for handling feedback\n",
        "branches = RunnableBranch(\n",
        "    (\n",
        "        lambda x: \"positive\" in x,\n",
        "        positive_feedback_template | model | StrOutputParser()  # Positive feedback chain\n",
        "    ),\n",
        "    (\n",
        "        lambda x: \"negative\" in x,\n",
        "        negative_feedback_template | model | StrOutputParser()  # Negative feedback chain\n",
        "    ),\n",
        "    (\n",
        "        lambda x: \"neutral\" in x,\n",
        "        neutral_feedback_template | model | StrOutputParser()  # Neutral feedback chain\n",
        "    ),\n",
        "    escalate_feedback_template | model | StrOutputParser()\n",
        ")\n",
        "\n",
        "# Create the classification chain\n",
        "classification_chain = classification_template | model | StrOutputParser()\n",
        "\n",
        "# Combine classification and response generation into one chain\n",
        "chain = classification_chain | branches\n",
        "\n",
        "# Run the chain with an example review\n",
        "# Good review - \"The product is excellent. I really enjoyed using it and found it very helpful.\"\n",
        "# Bad review - \"The product is terrible. It broke after just one use and the quality is very poor.\"\n",
        "# Neutral review - \"The product is okay. It works as expected but nothing exceptional.\"\n",
        "# Default - \"I'm not sure about the product yet. Can you tell me more about its features and benefits?\"\n",
        "\n",
        "review = \"The product is terrible. It broke after just one use and the quality is very poor.\"\n",
        "result = chain.invoke({\"feedback\": review})\n",
        "\n",
        "# Output the result\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qtafj7wAtR0N",
        "outputId": "ca05e9f8-a015-49cb-c3d0-b0382713c6b8"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Absolutely! However, to craft a more tailored response, could you please provide me with the specific details of the negative feedback you received? This will help me generate an appropriate and effective response.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "xxzmDedfB67k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RAG - Ingestion"
      ],
      "metadata": {
        "id": "o5gOHpHKB-hB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "NQ05tvOmCCF8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pypdf\n",
        "import os\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.document_loaders import PyPDFLoader # Changed to PyPDFLoader\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "\n",
        "# Define file path (Assuming it's in the current directory)\n",
        "file_path = '/content/us_bill_of_rights.txt'\n",
        "persistent_directory = '/content/db/chroma_db'\n",
        "\n",
        "# Check if the Chroma vector store already exists\n",
        "if not os.path.exists(persistent_directory):\n",
        "    print(\"Persistent directory does not exist. Initializing vector store...\")\n",
        "\n",
        "    # Ensure the text file exists\n",
        "    if not os.path.exists(file_path):\n",
        "        raise FileNotFoundError(\n",
        "            f\"The file {file_path} does not exist. Please check the path.\"\n",
        "        )\n",
        "\n",
        "    # Read the text content from the file\n",
        "    loader = PyPDFLoader(file_path) # Changed to PyPDFLoader\n",
        "    documents = loader.load()\n",
        "\n",
        "    # Split the document into chunks\n",
        "    text_splitter = CharacterTextSplitter(chunk_size=200, chunk_overlap=5)\n",
        "    docs = text_splitter.split_documents(documents)\n",
        "\n",
        "    # Display information about the split documents\n",
        "    print(\"\\n--- Document Chunks Information ---\")\n",
        "    print(f\"Number of document chunks: {len(docs)}\")\n",
        "    print(f\"Sample chunk:\\n{docs[0].page_content}\\n\")\n",
        "\n",
        "    # Create embeddings\n",
        "    print(\"\\n--- Creating embeddings ---\")\n",
        "    embeddings = OpenAIEmbeddings(\n",
        "        model=\"text-embedding-ada-002\", openai_api_key=api\n",
        "    )\n",
        "    print(\"\\n--- Finished creating embeddings ---\")\n",
        "\n",
        "    # Create the vector store and persist it automatically\n",
        "    print(\"\\n--- Creating vector store ---\")\n",
        "    db = Chroma.from_documents(\n",
        "        docs, embeddings, persist_directory=persistent_directory)\n",
        "    print(\"\\n--- Finished creating vector store ---\")\n",
        "\n",
        "else:\n",
        "    print(\"Vector store already exists. No need to initialize.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QyuoKqbO6B_S",
        "outputId": "2ba8044a-7277-40d2-80ad-132b9666741b"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pypdf in /usr/local/lib/python3.11/dist-packages (5.5.0)\n",
            "Persistent directory does not exist. Initializing vector store...\n",
            "\n",
            "--- Document Chunks Information ---\n",
            "Number of document chunks: 2\n",
            "Sample chunk:\n",
            "Rehan Khan\n",
            "dayel.rehan@gmail.com\n",
            " \n",
            "+918176987106\n",
            " \n",
            "rehan-khan25\n",
            " \n",
            "ryyhan\n",
            " \n",
            "ORCID\n",
            " \n",
            "Professional Experience\n",
            "Troopr Labs\n",
            "AI Engineer\n",
            "Jul 2024 – Dec 2024 | Texas, Remote\n",
            "•Built a Helpdesk Assessment Feature, generating detailed CRM reports (Jira, Salesforce) by \n",
            "categorizing 5M+ tickets into topic-based groups for streamlined queries.\n",
            "•Automated knowledge article creation from cases, training AI Agents to cut LLM token costs and \n",
            "boost query resolution efficiency by 30%.\n",
            "•Reduced customer query times using AI-trained knowledge articles, improving response accuracy \n",
            "by 20% and lowering costs.\n",
            "•Designed Multi-AI Agent workflows with LangGraph, enhancing collaboration and workflow \n",
            "efficiency by 25%\n",
            "Opsight AI\n",
            "AI/ML Intern\n",
            "Jan 2024 – Jun 2024 | New Delhi, India\n",
            "•Built a Retrieval-Augmented Generation (RAG) system, slashing info retrieval time from 10-15 mins \n",
            "to <1 min, boosting efficiency by 90%.\n",
            "•Led Elite Robots’ EC series arm project, collecting 100GB+ data and integrating InfluxDB with 50+ \n",
            "tags for real-time monitoring and Python-based parameter control.\n",
            "•Fine-tuned LLMs for domain-specific datasets, enhancing model performance.\n",
            "National Institute of Technology Trichy (NIT Trichy)\n",
            "Summer Intern\n",
            "May 2023 – Jul 2023 | Trichy, Remote\n",
            "•Conducted 6+ months of network lateral movement research, identifying and mitigating threats, \n",
            "and publishing actionable insights.\n",
            "•Explored advanced cybersecurity techniques (cyber deception, anomaly detection) using machine \n",
            "learning, reducing false positives by 15% and improving incident response efficiency.\n",
            "Education\n",
            "Oriental Institute of Science and Technology 2020 – 2024 | Bhopal\n",
            "Bachelor of Technology (B.Tech) - Computer Science and Engineering - (Data Science) GPA-8.65\n",
            "Projects\n",
            "Algometrix [Available on PYPI, the official Python Package Index]\n",
            "Python, scikit-learn, pandas, numpy, PyPI\n",
            "•Created a Python library for data scientists, speeding up ML model selection and evaluation by \n",
            "25%.\n",
            "•Published on PyPI, achieving 1,000+ downloads with seamless pip install integration.\n",
            "AI Assess\n",
            "Python, Streamlit, FastAPI, OpenAI, LangChain, PyPDF2\n",
            "•Developed an AI tool for educators, generating 10+ customizable MCQs/subjective questions from \n",
            "PDFs or keywords with adjustable difficulty.\n",
            "•Built a FastAPI backend and Streamlit frontend, automating answer evaluation for 30% faster query \n",
            "resolution and real-time scoring.\n",
            "•Improved evaluation accuracy by 20% with robust normalization for inconsistent inputs.\n",
            "Hybrid Fine-Tuned Agentic RAG\n",
            "Python, Langchain, PyTorch, Transformers, FastAPI, FineTuned LLM\n",
            "•Engineered a Retrieval-Augmented Generation (RAG) system with a fine-tuned LLM, integrating \n",
            "hybrid search (vector embeddings + BM25 keyword search) to boost query relevance by 35% and \n",
            "cut retrieval time by 50%.\n",
            "•Optimized retrieval with semantic chunking, multi-query expansion, and cross-encoder reranking, \n",
            "improving context precision by 40% for complex queries.\n",
            "\n",
            "\n",
            "--- Creating embeddings ---\n",
            "\n",
            "--- Finished creating embeddings ---\n",
            "\n",
            "--- Creating vector store ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-17-6d83d0338717>:37: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import OpenAIEmbeddings``.\n",
            "  embeddings = OpenAIEmbeddings(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Finished creating vector store ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Retrieval"
      ],
      "metadata": {
        "id": "9CTWnAA_yaku"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "# Define the persistent directory\n",
        "persistent_directory = '/content/db/chroma_db'\n",
        "\n",
        "# Define the embedding model\n",
        "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\",openai_api_key=api)\n",
        "\n",
        "# Load the existing vector store with the embedding function\n",
        "db = Chroma(persist_directory=persistent_directory,\n",
        "            embedding_function=embeddings)\n",
        "\n",
        "# Define the user's question\n",
        "query = \"Tell me about professional experience in RAG??\"\n",
        "\n",
        "# Retrieve relevant documents based on the query\n",
        "retriever = db.as_retriever(\n",
        "    search_type=\"similarity_score_threshold\",\n",
        "    search_kwargs={\"k\": 3, \"score_threshold\": 0.1},\n",
        ")\n",
        "relevant_docs = retriever.invoke(query)\n",
        "\n",
        "# Display the relevant results with metadata\n",
        "print(\"\\n--- Relevant Documents ---\")\n",
        "for i, doc in enumerate(relevant_docs, 1):\n",
        "    print(f\"Document {i}:\\n{doc.page_content}\\n\")\n",
        "    if doc.metadata:\n",
        "        print(f\"Source: {doc.metadata.get('source', 'Unknown')}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iZ78bHc2_lJw",
        "outputId": "70bf24d4-9e70-4d58-8ca3-601d86559efc"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-18-88df20135dd4>:13: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
            "  db = Chroma(persist_directory=persistent_directory,\n",
            "/usr/local/lib/python3.11/dist-packages/langchain_core/vectorstores/base.py:1082: UserWarning: Relevance scores must be between 0 and 1, got [(Document(metadata={'creator': 'FlowCV - https://flowcv.com', 'page_label': '1', 'producer': 'Skia/PDF m131', 'total_pages': 2, 'moddate': '2025-03-06T12:30:23+00:00', 'creationdate': '2025-03-06T12:30:23+00:00', 'page': 0, 'source': '/content/RehanKhanResume.pdf', 'keywords': 'Free Online Resume Builder, FlowCV - https://flowcv.com'}, page_content='Rehan Khan\\ndayel.rehan@gmail.com\\n \\n+918176987106\\n \\nrehan-khan25\\n \\nryyhan\\n \\nORCID\\n \\nProfessional Experience\\nTroopr Labs\\nAI Engineer\\nJul\\xa02024 – Dec\\xa02024 | Texas, Remote\\n•Built a Helpdesk Assessment Feature, generating detailed CRM reports (Jira, Salesforce) by \\ncategorizing 5M+ tickets into topic-based groups for streamlined queries.\\n•Automated knowledge article creation from cases, training AI Agents to cut LLM token costs and \\nboost query resolution efficiency by 30%.\\n•Reduced customer query times using AI-trained knowledge articles, improving response accuracy \\nby 20% and lowering costs.\\n•Designed Multi-AI Agent workflows with LangGraph, enhancing collaboration and workflow \\nefficiency by 25%\\nOpsight AI\\nAI/ML Intern\\nJan\\xa02024 – Jun\\xa02024 | New Delhi, India\\n•Built a Retrieval-Augmented Generation (RAG) system, slashing info retrieval time from 10-15 mins \\nto <1 min, boosting efficiency by 90%.\\n•Led Elite Robots’ EC series arm project, collecting 100GB+ data and integrating InfluxDB with 50+ \\ntags for real-time monitoring and Python-based parameter control.\\n•Fine-tuned LLMs for domain-specific datasets, enhancing model performance.\\nNational Institute of Technology Trichy (NIT Trichy)\\nSummer Intern\\nMay\\xa02023 – Jul\\xa02023 | Trichy, Remote\\n•Conducted 6+ months of network lateral movement research, identifying and mitigating threats, \\nand publishing actionable insights.\\n•Explored advanced cybersecurity techniques (cyber deception, anomaly detection) using machine \\nlearning, reducing false positives by 15% and improving incident response efficiency.\\nEducation\\nOriental Institute of Science and Technology 2020 – 2024 | Bhopal\\nBachelor of Technology (B.Tech) - Computer Science and Engineering - (Data Science) GPA-8.65\\nProjects\\nAlgometrix [Available on PYPI, the official Python Package Index]\\nPython, scikit-learn, pandas, numpy, PyPI\\n•Created a Python library for data scientists, speeding up ML model selection and evaluation by \\n25%.\\n•Published on PyPI, achieving 1,000+ downloads with seamless pip install integration.\\nAI Assess\\nPython, Streamlit, FastAPI, OpenAI, LangChain, PyPDF2\\n•Developed an AI tool for educators, generating 10+ customizable MCQs/subjective questions from \\nPDFs or keywords with adjustable difficulty.\\n•Built a FastAPI backend and Streamlit frontend, automating answer evaluation for 30% faster query \\nresolution and real-time scoring.\\n•Improved evaluation accuracy by 20% with robust normalization for inconsistent inputs.\\nHybrid Fine-Tuned Agentic RAG\\nPython, Langchain, PyTorch, Transformers, FastAPI, FineTuned LLM\\n•Engineered a Retrieval-Augmented Generation (RAG) system with a fine-tuned LLM, integrating \\nhybrid search (vector embeddings + BM25 keyword search) to boost query relevance by 35% and \\ncut retrieval time by 50%.\\n•Optimized retrieval with semantic chunking, multi-query expansion, and cross-encoder reranking, \\nimproving context precision by 40% for complex queries.'), -0.3642309403744495), (Document(metadata={'page_label': '2', 'source': '/content/RehanKhanResume.pdf', 'moddate': '2025-03-06T12:30:23+00:00', 'total_pages': 2, 'creator': 'FlowCV - https://flowcv.com', 'producer': 'Skia/PDF m131', 'page': 1, 'keywords': 'Free Online Resume Builder, FlowCV - https://flowcv.com', 'creationdate': '2025-03-06T12:30:23+00:00'}, page_content=\"•Deployed a FastAPI REST interface with dynamic caching and load balancing, enhancing cross-\\nplatform accessibility by 80% and ensuring scalability.\\nPublications (50+ Citations)\\nConversational AI : Dialoguing Most Humanly With Non-Humans- Wiley Publications\\nThe Promises and Perils of Artificial Intelligence: An Ethical and Social Analysis, IGI Global\\nSkills\\nProgramming:\\xa0Python, Java, SQL\\nAI/ML:\\xa0LangChain, LangGraph, PyTorch, Agentic AI, HuggingFace, scikit-learn, NLP, Generative AI,\\nLLMs, Llama Index, Open AI,\\nTools:\\xa0Git/GitHub, Docker, Linux, SQLite, FastAPI, Streamlit, Vector Databases, ChromaDB, Pinecone\\nLibraries:\\xa0Streamlit, Pandas, NumPy, NLTK, PyPDF2, PyMuPDF,\\nCertificates\\nMachine Learning by Andrew Ng , Harvard's CS50 - Python , AWS Machine Learning Foundations\\n, Philosophy of Science , AI, Empathy & Ethics , Philosophy and Cognitive Sciences , \\nGenerative AI with LLMs , Introduction to LangGraph\"), -0.3815025503068199)]\n",
            "  self.vectorstore.similarity_search_with_relevance_scores(\n",
            "WARNING:langchain_core.vectorstores.base:No relevant docs were retrieved using the relevance score threshold 0.1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Relevant Documents ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## retrieval methods types"
      ],
      "metadata": {
        "id": "FHEGPGkGxYGK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "from dotenv import load_dotenv\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "# Define the persistent directory\n",
        "current_dir = os.path.dirname(os.path.abspath(__file__))\n",
        "db_dir = os.path.join(current_dir, \"db\")\n",
        "persistent_directory = os.path.join(db_dir, \"chroma_db_with_metadata\")\n",
        "\n",
        "# Define the embedding model\n",
        "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
        "\n",
        "# Load the existing vector store with the embedding function\n",
        "db = Chroma(persist_directory=persistent_directory,\n",
        "            embedding_function=embeddings)\n",
        "\n",
        "\n",
        "# Function to query a vector store with different search types and parameters\n",
        "def query_vector_store(\n",
        "    store_name, query, embedding_function, search_type, search_kwargs\n",
        "):\n",
        "    if os.path.exists(persistent_directory):\n",
        "        print(f\"\\n--- Querying the Vector Store {store_name} ---\")\n",
        "        db = Chroma(\n",
        "            persist_directory=persistent_directory,\n",
        "            embedding_function=embedding_function,\n",
        "        )\n",
        "        retriever = db.as_retriever(\n",
        "            search_type=search_type,\n",
        "            search_kwargs=search_kwargs,\n",
        "        )\n",
        "        relevant_docs = retriever.invoke(query)\n",
        "        # Display the relevant results with metadata\n",
        "        print(f\"\\n--- Relevant Documents for {store_name} ---\")\n",
        "        for i, doc in enumerate(relevant_docs, 1):\n",
        "            print(f\"Document {i}:\\n{doc.page_content}\\n\")\n",
        "            if doc.metadata:\n",
        "                print(f\"Source: {doc.metadata.get('source', 'Unknown')}\\n\")\n",
        "    else:\n",
        "        print(f\"Vector store {store_name} does not exist.\")\n",
        "\n",
        "\n",
        "# Define the user's question\n",
        "query = \"How did Juliet die?\"\n",
        "\n",
        "# Showcase different retrieval methods\n",
        "\n",
        "# 1. Similarity Search\n",
        "# This method retrieves documents based on vector similarity.\n",
        "# It finds the most similar documents to the query vector based on cosine similarity.\n",
        "# Use this when you want to retrieve the top k most similar documents.\n",
        "print(\"\\n--- Using Similarity Search ---\")\n",
        "query_vector_store(\"chroma_db_with_metadata\", query,\n",
        "                   embeddings, \"similarity\", {\"k\": 3})\n",
        "\n",
        "# 2. Max Marginal Relevance (MMR)\n",
        "# This method balances between selecting documents that are relevant to the query and diverse among themselves.\n",
        "# 'fetch_k' specifies the number of documents to initially fetch based on similarity.\n",
        "# 'lambda_mult' controls the diversity of the results: 1 for minimum diversity, 0 for maximum.\n",
        "# Use this when you want to avoid redundancy and retrieve diverse yet relevant documents.\n",
        "# Note: Relevance measures how closely documents match the query.\n",
        "# Note: Diversity ensures that the retrieved documents are not too similar to each other,\n",
        "#       providing a broader range of information.\n",
        "print(\"\\n--- Using Max Marginal Relevance (MMR) ---\")\n",
        "query_vector_store(\n",
        "    \"chroma_db_with_metadata\",\n",
        "    query,\n",
        "    embeddings,\n",
        "    \"mmr\",\n",
        "    {\"k\": 3, \"fetch_k\": 20, \"lambda_mult\": 0.5},\n",
        ")\n",
        "\n",
        "# 3. Similarity Score Threshold\n",
        "# This method retrieves documents that exceed a certain similarity score threshold.\n",
        "# 'score_threshold' sets the minimum similarity score a document must have to be considered relevant.\n",
        "# Use this when you want to ensure that only highly relevant documents are retrieved, filtering out less relevant ones.\n",
        "print(\"\\n--- Using Similarity Score Threshold ---\")\n",
        "query_vector_store(\n",
        "    \"chroma_db_with_metadata\",\n",
        "    query,\n",
        "    embeddings,\n",
        "    \"similarity_score_threshold\",\n",
        "    {\"k\": 3, \"score_threshold\": 0.1},\n",
        ")\n",
        "\n",
        "print(\"Querying demonstrations with different search types completed.\")"
      ],
      "metadata": {
        "id": "hFbqIEAdJOXL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Naive RAG"
      ],
      "metadata": {
        "id": "iEAJAvuyxf6y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "from dotenv import load_dotenv\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_core.messages import HumanMessage, SystemMessage\n",
        "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
        "\n",
        "# Load environment variables from .env\n",
        "load_dotenv()\n",
        "\n",
        "# Define the persistent directory\n",
        "current_dir = os.path.dirname(os.path.abspath(__file__))\n",
        "persistent_directory = os.path.join(\n",
        "    current_dir, \"db\", \"chroma_db_with_metadata\")\n",
        "\n",
        "# Define the embedding model\n",
        "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
        "\n",
        "# Load the existing vector store with the embedding function\n",
        "db = Chroma(persist_directory=persistent_directory,\n",
        "            embedding_function=embeddings)\n",
        "\n",
        "# Define the user's question\n",
        "query = \"How can I learn more about LangChain?\"\n",
        "\n",
        "# Retrieve relevant documents based on the query\n",
        "retriever = db.as_retriever(\n",
        "    search_type=\"similarity\",\n",
        "    search_kwargs={\"k\": 1},\n",
        ")\n",
        "relevant_docs = retriever.invoke(query)\n",
        "\n",
        "# Display the relevant results with metadata\n",
        "print(\"\\n--- Relevant Documents ---\")\n",
        "for i, doc in enumerate(relevant_docs, 1):\n",
        "    print(f\"Document {i}:\\n{doc.page_content}\\n\")\n",
        "\n",
        "# Combine the query and the relevant document contents\n",
        "combined_input = (\n",
        "    \"Here are some documents that might help answer the question: \"\n",
        "    + query\n",
        "    + \"\\n\\nRelevant Documents:\\n\"\n",
        "    + \"\\n\\n\".join([doc.page_content for doc in relevant_docs])\n",
        "    + \"\\n\\nPlease provide an answer based only on the provided documents. If the answer is not found in the documents, respond with 'I'm not sure'.\"\n",
        ")\n",
        "\n",
        "# Create a ChatOpenAI model\n",
        "model = ChatOpenAI(model=\"gpt-4o\")\n",
        "\n",
        "# Define the messages for the model\n",
        "messages = [\n",
        "    SystemMessage(content=\"You are a helpful assistant.\"),\n",
        "    HumanMessage(content=combined_input),\n",
        "]\n",
        "\n",
        "# Invoke the model with the combined input\n",
        "result = model.invoke(messages)\n",
        "\n",
        "# Display the full result and content only\n",
        "print(\"\\n--- Generated Response ---\")\n",
        "# print(\"Full result:\")\n",
        "# print(result)\n",
        "print(\"Content only:\")\n",
        "print(result.content)"
      ],
      "metadata": {
        "id": "kDx8dXO_xfpG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conversational History RAG"
      ],
      "metadata": {
        "id": "x0ai04ImswTl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "from dotenv import load_dotenv\n",
        "from langchain.chains import create_history_aware_retriever, create_retrieval_chain\n",
        "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_core.messages import HumanMessage, SystemMessage\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
        "\n",
        "# Load environment variables from .env\n",
        "load_dotenv()\n",
        "\n",
        "# Define the persistent directory\n",
        "current_dir = os.path.dirname(os.path.abspath(__file__))\n",
        "persistent_directory = os.path.join(current_dir, \"db\", \"chroma_db_with_metadata\")\n",
        "\n",
        "# Define the embedding model\n",
        "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
        "\n",
        "# Load the existing vector store with the embedding function\n",
        "db = Chroma(persist_directory=persistent_directory, embedding_function=embeddings)\n",
        "\n",
        "# Create a retriever for querying the vector store\n",
        "# `search_type` specifies the type of search (e.g., similarity)\n",
        "# `search_kwargs` contains additional arguments for the search (e.g., number of results to return)\n",
        "retriever = db.as_retriever(\n",
        "    search_type=\"similarity\",\n",
        "    search_kwargs={\"k\": 3},\n",
        ")\n",
        "\n",
        "# Create a ChatOpenAI model\n",
        "llm = ChatOpenAI(model=\"gpt-4o\")\n",
        "\n",
        "# Contextualize question prompt\n",
        "# This system prompt helps the AI understand that it should reformulate the question\n",
        "# based on the chat history to make it a standalone question\n",
        "contextualize_q_system_prompt = (\n",
        "    \"Given a chat history and the latest user question \"\n",
        "    \"which might reference context in the chat history, \"\n",
        "    \"formulate a standalone question which can be understood \"\n",
        "    \"without the chat history. Do NOT answer the question, just \"\n",
        "    \"reformulate it if needed and otherwise return it as is.\"\n",
        ")\n",
        "\n",
        "# Create a prompt template for contextualizing questions\n",
        "contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", contextualize_q_system_prompt),\n",
        "        MessagesPlaceholder(\"chat_history\"),\n",
        "        (\"human\", \"{input}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Create a history-aware retriever\n",
        "# This uses the LLM to help reformulate the question based on chat history\n",
        "history_aware_retriever = create_history_aware_retriever(\n",
        "    llm, retriever, contextualize_q_prompt\n",
        ")\n",
        "\n",
        "# Answer question prompt\n",
        "# This system prompt helps the AI understand that it should provide concise answers\n",
        "# based on the retrieved context and indicates what to do if the answer is unknown\n",
        "qa_system_prompt = (\n",
        "    \"You are an assistant for question-answering tasks. Use \"\n",
        "    \"the following pieces of retrieved context to answer the \"\n",
        "    \"question. If you don't know the answer, just say that you \"\n",
        "    \"don't know. Use three sentences maximum and keep the answer \"\n",
        "    \"concise.\"\n",
        "    \"\\n\\n\"\n",
        "    \"{context}\"\n",
        ")\n",
        "\n",
        "# Create a prompt template for answering questions\n",
        "qa_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", qa_system_prompt),\n",
        "        MessagesPlaceholder(\"chat_history\"),\n",
        "        (\"human\", \"{input}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Create a chain to combine documents for question answering\n",
        "# `create_stuff_documents_chain` feeds all retrieved context into the LLM\n",
        "question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)\n",
        "\n",
        "# Create a retrieval chain that combines the history-aware retriever and the question answering chain\n",
        "rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)\n",
        "\n",
        "\n",
        "# Function to simulate a continual chat\n",
        "def continual_chat():\n",
        "    print(\"Start chatting with the AI! Type 'exit' to end the conversation.\")\n",
        "    chat_history = []  # Collect chat history here (a sequence of messages)\n",
        "    while True:\n",
        "        query = input(\"You: \")\n",
        "        if query.lower() == \"exit\":\n",
        "            break\n",
        "        # Process the user's query through the retrieval chain\n",
        "        result = rag_chain.invoke({\"input\": query, \"chat_history\": chat_history})\n",
        "        # Display the AI's response\n",
        "        print(f\"AI: {result['answer']}\")\n",
        "        # Update the chat history\n",
        "        chat_history.append(HumanMessage(content=query))\n",
        "        chat_history.append(SystemMessage(content=result[\"answer\"]))"
      ],
      "metadata": {
        "id": "qlWnpaz5smNd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}