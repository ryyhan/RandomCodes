{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPbYDZV6DZ6zAPUGxI49SEd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ryyhan/RandomCodes/blob/main/Langchain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Basic Chat\n"
      ],
      "metadata": {
        "id": "WLjYXD-eDade"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade langchain langchain_core langchain_openai langchain_community chromadb openai tiktoken"
      ],
      "metadata": {
        "collapsed": true,
        "id": "3iYxf6v9DsFW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2MmHoZ6hApFa",
        "outputId": "b59f9635-33c2-4f5e-8046-96d4ddae1806"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Full result:\n",
            "content='81 divided by 9 equals 9.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 16, 'total_tokens': 26, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_0392822090', 'id': 'chatcmpl-BVymTmYKTllhfedxKMU1FspoNSm14', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='run--294546fb-62b0-490a-bb36-efc84fdaa578-0' usage_metadata={'input_tokens': 16, 'output_tokens': 10, 'total_tokens': 26, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
            "Content only:\n",
            "81 divided by 9 equals 9.\n"
          ]
        }
      ],
      "source": [
        "#imports\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "#environment variables\n",
        "from google.colab import userdata\n",
        "api=userdata.get('OPEN_AI_KEY')\n",
        "\n",
        "\n",
        "# Create a ChatOpenAI model\n",
        "model = ChatOpenAI(model=\"gpt-4o-mini\",api_key=api)\n",
        "\n",
        "# Invoke the model with a message\n",
        "result = model.invoke(\"What is 81 divided by 9?\")\n",
        "print(\"Full result:\")\n",
        "print(result)\n",
        "print(\"Content only:\")\n",
        "print(result.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conversation"
      ],
      "metadata": {
        "id": "RlmNyBYVLjTp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "\n",
        "#environment variables\n",
        "from google.colab import userdata\n",
        "api=userdata.get('OPEN_AI_KEY')\n",
        "\n",
        "\n",
        "# Create a ChatOpenAI model\n",
        "model = ChatOpenAI(model=\"gpt-4o-mini\",api_key=api)\n",
        "\n",
        "\n",
        "messages = [\n",
        "    SystemMessage(content=\"Solve the following math problems\"),\n",
        "    HumanMessage(content=\"What is 81 divided by 9?\"),\n",
        "]\n",
        "\n",
        "# Invoke the model with messages\n",
        "result = model.invoke(messages)\n",
        "print(f\"Answer from AI (Single): {result.content}\")\n",
        "\n",
        "\n",
        "# AIMessage:\n",
        "#   Message from an AI.\n",
        "messages = [\n",
        "    SystemMessage(content=\"Solve the following math problems\"),\n",
        "    HumanMessage(content=\"What is 81 divided by 9?\"),\n",
        "    AIMessage(content=\"81 divided by 9 is 9.\"),\n",
        "    HumanMessage(content=\"What is 10 times 5?\"),\n",
        "]\n",
        "\n",
        "# Invoke the model with messages\n",
        "result = model.invoke(messages)\n",
        "print(f\"Answer from AI (Multiple): {result.content}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Omxka0qoDdD8",
        "outputId": "d07920a2-2396-466c-e0f4-eb7e0776086c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer from AI (Single): 81 divided by 9 is 9.\n",
            "Answer from AI (Multiple): 10 times 5 is 50.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Chatbot Conversations"
      ],
      "metadata": {
        "id": "baW8PAR8RtT2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.schema import AIMessage, HumanMessage, SystemMessage\n",
        "\n",
        "#environment variables\n",
        "from google.colab import userdata\n",
        "api=userdata.get('OPEN_AI_KEY')\n",
        "\n",
        "\n",
        "# Create a ChatOpenAI model\n",
        "model = ChatOpenAI(model=\"gpt-4o-mini\",api_key=api)\n",
        "\n",
        "\n",
        "chat_history = []  # Use a list to store messages\n",
        "\n",
        "# Set an initial system message (optional)\n",
        "system_message = SystemMessage(content=\"You are a helpful AI assistant.\")\n",
        "chat_history.append(system_message)  # Add system message to chat history\n",
        "\n",
        "# Chat loop\n",
        "while True:\n",
        "    query = input(\"You: \")\n",
        "    if query.lower() == \"exit\":\n",
        "        break\n",
        "    chat_history.append(HumanMessage(content=query))  # Add user message\n",
        "\n",
        "    # Get AI response using history\n",
        "    result = model.invoke(chat_history)\n",
        "    response = result.content\n",
        "    chat_history.append(AIMessage(content=response))  # Add AI message\n",
        "\n",
        "    print(f\"AI: {response}\")\n",
        "\n",
        "\n",
        "print(\"---- Message History ----\")\n",
        "print(chat_history)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dmchOq7YRs85",
        "outputId": "acfefd04-37f8-4651-f14f-7c4764b16a32"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You: Who is the featherweight UFC GOAT?\n",
            "AI: The title of the greatest of all time (GOAT) in the featherweight division of the UFC is often debated, but two fighters who are frequently mentioned in this discussion are José Aldo and Max Holloway.\n",
            "\n",
            "**José Aldo** is celebrated for his long reign as the UFC Featherweight Champion, where he defended his title seven times from 2010 to 2015. His striking, particularly his leg kicks, and his overall skill set made him a dominant force in the division for years.\n",
            "\n",
            "**Max Holloway**, on the other hand, has made a strong case for himself with his high-volume striking, exceptional boxing skills, and being a two-time champion. Holloway's victories include notable wins over former champions and top contenders, and he holds the record for the most significant strikes landed in UFC history.\n",
            "\n",
            "Both fighters have made significant impacts in the featherweight division, and opinions on who the GOAT is can vary based on personal preferences and metrics considered.\n",
            "You: what about lightweight?\n",
            "AI: In the UFC lightweight division, the debate for the greatest of all time (GOAT) typically centers around two fighters: **Khabib Nurmagomedov** and **Benson Henderson**.\n",
            "\n",
            "1. **Khabib Nurmagomedov**: Khabib is widely regarded as one of the greatest lightweights, if not the greatest, due to his undefeated record of 29-0 and his dominance in fights. He held the UFC Lightweight Championship and successfully defended it three times before retiring in 2020. His grappling, ground control, and ability to impose his game plan on opponents set him apart in the sport.\n",
            "\n",
            "2. **Benson Henderson**: Henderson was the UFC Lightweight Champion and defended his title three times. He is known for his well-rounded skill set, including wrestling, striking, and submissions. Henderson has notable victories over other top fighters and has fought some of the best in the division during his time.\n",
            "\n",
            "Other fighters like **Conor McGregor**, who became the first fighter in UFC history to hold two titles simultaneously, and **Tony Ferguson**, known for his unorthodox style and finishing ability, are also often part of the conversation.\n",
            "\n",
            "Overall, Khabib Nurmagomedov is frequently cited as the lightweight GOAT due to his perfect record, dominant performances, and the legacy he left before stepping away from the sport.\n",
            "You: exit\n",
            "---- Message History ----\n",
            "[SystemMessage(content='You are a helpful AI assistant.', additional_kwargs={}, response_metadata={}), HumanMessage(content='Who is the featherweight UFC GOAT?', additional_kwargs={}, response_metadata={}), AIMessage(content=\"The title of the greatest of all time (GOAT) in the featherweight division of the UFC is often debated, but two fighters who are frequently mentioned in this discussion are José Aldo and Max Holloway.\\n\\n**José Aldo** is celebrated for his long reign as the UFC Featherweight Champion, where he defended his title seven times from 2010 to 2015. His striking, particularly his leg kicks, and his overall skill set made him a dominant force in the division for years.\\n\\n**Max Holloway**, on the other hand, has made a strong case for himself with his high-volume striking, exceptional boxing skills, and being a two-time champion. Holloway's victories include notable wins over former champions and top contenders, and he holds the record for the most significant strikes landed in UFC history.\\n\\nBoth fighters have made significant impacts in the featherweight division, and opinions on who the GOAT is can vary based on personal preferences and metrics considered.\", additional_kwargs={}, response_metadata={}), HumanMessage(content='what about lightweight?', additional_kwargs={}, response_metadata={}), AIMessage(content='In the UFC lightweight division, the debate for the greatest of all time (GOAT) typically centers around two fighters: **Khabib Nurmagomedov** and **Benson Henderson**.\\n\\n1. **Khabib Nurmagomedov**: Khabib is widely regarded as one of the greatest lightweights, if not the greatest, due to his undefeated record of 29-0 and his dominance in fights. He held the UFC Lightweight Championship and successfully defended it three times before retiring in 2020. His grappling, ground control, and ability to impose his game plan on opponents set him apart in the sport.\\n\\n2. **Benson Henderson**: Henderson was the UFC Lightweight Champion and defended his title three times. He is known for his well-rounded skill set, including wrestling, striking, and submissions. Henderson has notable victories over other top fighters and has fought some of the best in the division during his time.\\n\\nOther fighters like **Conor McGregor**, who became the first fighter in UFC history to hold two titles simultaneously, and **Tony Ferguson**, known for his unorthodox style and finishing ability, are also often part of the conversation.\\n\\nOverall, Khabib Nurmagomedov is frequently cited as the lightweight GOAT due to his perfect record, dominant performances, and the legacy he left before stepping away from the sport.', additional_kwargs={}, response_metadata={})]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example Source: https://python.langchain.com/v0.2/docs/integrations/memory/google_firestore/\n",
        "\n",
        "from google.cloud import firestore\n",
        "from langchain_google_firestore import FirestoreChatMessageHistory\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "\"\"\"\n",
        "Steps to replicate this example:\n",
        "1. Create a Firebase account\n",
        "2. Create a new Firebase project\n",
        "    - Copy the project ID\n",
        "3. Create a Firestore database in the Firebase project\n",
        "4. Install the Google Cloud CLI on your computer\n",
        "    - https://cloud.google.com/sdk/docs/install\n",
        "    - Authenticate the Google Cloud CLI with your Google account\n",
        "        - https://cloud.google.com/docs/authentication/provide-credentials-adc#local-dev\n",
        "    - Set your default project to the new Firebase project you created\n",
        "5. Enable the Firestore API in the Google Cloud Console:\n",
        "    - https://console.cloud.google.com/apis/enableflow?apiid=firestore.googleapis.com&project=crewai-automation\n",
        "\"\"\"\n",
        "\n",
        "#environment variables\n",
        "from google.colab import userdata\n",
        "api=userdata.get('OPEN_AI_KEY')\n",
        "\n",
        "\n",
        "# Setup Firebase Firestore\n",
        "PROJECT_ID = \"langchain-demo-abf48\"\n",
        "SESSION_ID = \"user_session_new\"  # This could be a username or a unique ID\n",
        "COLLECTION_NAME = \"chat_history\"\n",
        "\n",
        "# Initialize Firestore Client\n",
        "print(\"Initializing Firestore Client...\")\n",
        "client = firestore.Client(project=PROJECT_ID)\n",
        "\n",
        "# Initialize Firestore Chat Message History\n",
        "print(\"Initializing Firestore Chat Message History...\")\n",
        "chat_history = FirestoreChatMessageHistory(\n",
        "    session_id=SESSION_ID,\n",
        "    collection=COLLECTION_NAME,\n",
        "    client=client,\n",
        ")\n",
        "print(\"Chat History Initialized.\")\n",
        "print(\"Current Chat History:\", chat_history.messages)\n",
        "\n",
        "# Create a ChatOpenAI model\n",
        "model = ChatOpenAI(model=\"gpt-4o-mini\",api_key=api)\n",
        "\n",
        "print(\"Start chatting with the AI. Type 'exit' to quit.\")\n",
        "\n",
        "while True:\n",
        "    human_input = input(\"User: \")\n",
        "    if human_input.lower() == \"exit\":\n",
        "        break\n",
        "\n",
        "    chat_history.add_user_message(human_input)\n",
        "\n",
        "    ai_response = model.invoke(chat_history.messages)\n",
        "    chat_history.add_ai_message(ai_response.content)\n",
        "\n",
        "    print(f\"AI: {ai_response.content}\")"
      ],
      "metadata": {
        "id": "ifnx-HPIL8oe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "u8zseiV6hSUd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#prompting"
      ],
      "metadata": {
        "id": "X_Aq47ZAhT_g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "0TDhv16OhXgR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prompt Template Docs:\n",
        "#   https://python.langchain.com/v0.2/docs/concepts/#prompt-templateshttps://python.langchain.com/v0.2/docs/concepts/#prompt-templates\n",
        "\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain_core.messages import HumanMessage\n",
        "\n",
        "# # PART 1: Create a ChatPromptTemplate using a template string\n",
        "template = \"Tell me a joke about {topic}.\"\n",
        "prompt_template = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "print(\"-----Prompt from Template-----\")\n",
        "prompt = prompt_template.invoke({\"topic\": \"cats\"})\n",
        "print(prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WHe2WsKShTHI",
        "outputId": "8985829d-4190-4dd5-d43a-df52084ff73c"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----Prompt from Template-----\n",
            "messages=[HumanMessage(content='Tell me a joke about cats.', additional_kwargs={}, response_metadata={})]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# PART 2: Prompt with Multiple Placeholders\n",
        "template_multiple = \"\"\"You are a helpful assistant.\n",
        "Human: Tell me a {adjective} story about a {animal}.\n",
        "Assistant:\"\"\"\n",
        "prompt_multiple = ChatPromptTemplate.from_template(template_multiple)\n",
        "prompt = prompt_multiple.invoke({\"adjective\": \"funny\", \"animal\": \"panda\"})\n",
        "print(\"\\n----- Prompt with Multiple Placeholders -----\\n\")\n",
        "print(prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mQDYTH4DJkBx",
        "outputId": "47baa2f9-aa0f-410b-8970-182fe4889b9b"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "----- Prompt with Multiple Placeholders -----\n",
            "\n",
            "messages=[HumanMessage(content='You are a helpful assistant.\\nHuman: Tell me a funny story about a panda.\\nAssistant:', additional_kwargs={}, response_metadata={})]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prompt Template with ChatModel"
      ],
      "metadata": {
        "id": "LIFjKTV5LlBR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "#environment variables\n",
        "from google.colab import userdata\n",
        "api=userdata.get('OPEN_AI_KEY')\n",
        "\n",
        "# Create a ChatOpenAI model\n",
        "model = ChatOpenAI(model=\"gpt-4o-mini\",api_key=api)\n",
        "\n",
        "# PART 1: Create a ChatPromptTemplate using a template string\n",
        "print(\"-----Prompt from Template-----\")\n",
        "template = \"Tell me a joke about {topic}.\"\n",
        "prompt_template = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "prompt = prompt_template.invoke({\"topic\": \"cats\"})\n",
        "result = model.invoke(prompt)\n",
        "print(prompt, result.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6TJlO_OTKyT7",
        "outputId": "b5e42a5d-088d-41ce-d25d-f1e400667a09"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----Prompt from Template-----\n",
            "messages=[HumanMessage(content='Tell me a joke about cats.', additional_kwargs={}, response_metadata={})] Why did the cat sit on the computer?\n",
            "\n",
            "Because it wanted to keep an eye on the mouse!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# PART 2: Prompt with Multiple Placeholders\n",
        "print(\"\\n----- Prompt with Multiple Placeholders -----\\n\")\n",
        "template_multiple = \"\"\"You are a helpful assistant.\n",
        "Human: Tell me a {adjective} short story about a {animal}.\n",
        "Assistant:\"\"\"\n",
        "prompt_multiple = ChatPromptTemplate.from_template(template_multiple)\n",
        "prompt = prompt_multiple.invoke({\"adjective\": \"funny\", \"animal\": \"cat\"})\n",
        "\n",
        "result = model.invoke(prompt)\n",
        "print(prompt,\"/n\\n\",result.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "usPIM502lUQy",
        "outputId": "05b2289e-0639-4aee-b5e6-967b03cae283"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "----- Prompt with Multiple Placeholders -----\n",
            "\n",
            "messages=[HumanMessage(content='You are a helpful assistant.\\nHuman: Tell me a funny short story about a cat.\\nAssistant:', additional_kwargs={}, response_metadata={})] /n\n",
            " Once, there was a sassy cat named Whiskers who had a knack for turning everyday situations into hilarious adventures. One sunny afternoon, Whiskers decided he was the king of the house and that all humans should bow down to him. He strutted around with his tail held high, surveying his kingdom from the couch throne.\n",
            "\n",
            "His human, Sarah, was in the kitchen baking cookies. The smell wafted through the air, attracting Whiskers like a moth to a flame. He leaped down from his regal perch and tiptoed toward the kitchen, plotting his next move to score some delicious treats. \n",
            "\n",
            "As Sarah opened the oven to check on the cookies, Whiskers thought this was the perfect moment for his royal entrance. He sprinted in, but his overconfidence got the better of him. In his haste, he hopped right onto the oven door! \n",
            "\n",
            "There was a loud clang, and before Whiskers could figure out what was happening, the oven door swung open, and he did an exaggerated somersault right into a pile of flour Sarah had just set down, creating a cloud of white dust everywhere. \n",
            "\n",
            "When Sarah turned around, she was greeted by a comically covered Whiskers—flour all over his fur, looking like a fluffy snowball. Disoriented but proud, he shook his head, sending flour flying everywhere, including onto Sarah’s freshly baked cookies. \n",
            "\n",
            "Sarah burst into laughter, and Whiskers, sensing he might have miscalculated his royal entrance, decided that perhaps he should retreat to the safety of his couch throne. But not before giving Sarah a look that clearly said, “I meant to do that!” From that day on, he was known not just as King Whiskers, but also as the Clown Prince of the Kitchen.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# PART 3: Prompt with System and Human Messages (Using Tuples)\n",
        "print(\"\\n----- Prompt with System and Human Messages (Tuple) -----\\n\")\n",
        "messages = [\n",
        "    (\"system\", \"You are a comedian who tells jokes about {topic}.\"),\n",
        "    (\"human\", \"Tell me {joke_count} jokes.\"),\n",
        "]\n",
        "prompt_template = ChatPromptTemplate.from_messages(messages)\n",
        "prompt = prompt_template.invoke({\"topic\": \"cats\", \"joke_count\": 3})\n",
        "result = model.invoke(prompt)\n",
        "print(prompt,\"\\n\\n\\n\")\n",
        "print(result.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ELOvkj-lgNO",
        "outputId": "78d181ec-8042-4594-908c-d5ea8fa71dc3"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "----- Prompt with System and Human Messages (Tuple) -----\n",
            "\n",
            "messages=[SystemMessage(content='You are a comedian who tells jokes about cats.', additional_kwargs={}, response_metadata={}), HumanMessage(content='Tell me 3 jokes.', additional_kwargs={}, response_metadata={})] \n",
            "\n",
            "\n",
            "\n",
            "Sure! Here are three cat-inspired jokes for you:\n",
            "\n",
            "1. Why did the cat sit on the computer?\n",
            "   Because it wanted to keep an eye on the mouse!\n",
            "\n",
            "2. What do you call a pile of cats?\n",
            "   A meowtain!\n",
            "\n",
            "3. Why was the cat so good at video games?\n",
            "   Because it had nine lives to practice with!\n",
            "\n",
            "Hope those gave you a laugh!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "TEHfwCPSl5Qe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chains"
      ],
      "metadata": {
        "id": "YRVbxvN0l_AC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "JvK3j7mlmBGG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "#environment variables\n",
        "from google.colab import userdata\n",
        "api=userdata.get('OPEN_AI_KEY')\n",
        "\n",
        "# Create a ChatOpenAI model\n",
        "model = ChatOpenAI(model=\"gpt-4o-mini\",api_key=api)\n",
        "\n",
        "# Define prompt templates (no need for separate Runnable chains)\n",
        "prompt_template = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"You are a comedian who tells jokes about {topic}.\"),\n",
        "        (\"human\", \"Tell me {joke_count} jokes.\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Create the combined chain using LangChain Expression Language (LCEL)\n",
        "chain = prompt_template | model | StrOutputParser()\n",
        "\n",
        "# Run the chain\n",
        "result = chain.invoke({\"topic\": \"cats\", \"joke_count\": 3})\n",
        "\n",
        "# Output\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FWkafui9lrd-",
        "outputId": "924911ba-971d-4127-e654-8ff13a9ea434"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sure! Here are three cat-themed jokes for you:\n",
            "\n",
            "1. Why did the cat sit on the computer?\n",
            "   Because it wanted to keep an eye on the mouse!\n",
            "\n",
            "2. What do you call a pile of cats?\n",
            "   A meowtain!\n",
            "\n",
            "3. Why was the cat so good at video games?\n",
            "   Because it had nine lives to practice!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Behind the scenes"
      ],
      "metadata": {
        "id": "zJq9hZB0rS6B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.schema.runnable import RunnableLambda, RunnableSequence\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "#environment variables\n",
        "from google.colab import userdata\n",
        "api=userdata.get('OPEN_AI_KEY')\n",
        "\n",
        "# Create a ChatOpenAI model\n",
        "model = ChatOpenAI(model=\"gpt-4o-mini\",api_key=api)\n",
        "\n",
        "# Define prompt templates\n",
        "prompt_template = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"You are a comedian who tells jokes about {topic}.\"),\n",
        "        (\"human\", \"Tell me {joke_count} jokes.\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Create individual runnables (steps in the chain)\n",
        "format_prompt = RunnableLambda(lambda x: prompt_template.format_prompt(**x))\n",
        "invoke_model = RunnableLambda(lambda x: model.invoke(x.to_messages()))\n",
        "parse_output = RunnableLambda(lambda x: x.content)\n",
        "\n",
        "# Create the RunnableSequence (equivalent to the LCEL chain)\n",
        "chain = RunnableSequence(first=format_prompt, middle=[invoke_model], last=parse_output)\n",
        "\n",
        "# Run the chain\n",
        "response = chain.invoke({\"topic\": \"cats\", \"joke_count\": 3})\n",
        "\n",
        "# Output\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K9a2ZF8CrSrO",
        "outputId": "62c94710-f276-4876-872c-a1332a92a17a"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sure! Here are three cat-themed jokes for you:\n",
            "\n",
            "1. Why was the cat sitting on the computer?  \n",
            "   Because it wanted to keep an eye on the mouse!\n",
            "\n",
            "2. What do you call a pile of cats?  \n",
            "   A meowtain!\n",
            "\n",
            "3. Why did the cat join Instagram?  \n",
            "   Because it wanted to be a purr-fect influencer! \n",
            "\n",
            "Hope you enjoyed those!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Custom runnnable/Chain\n"
      ],
      "metadata": {
        "id": "YXQedniysApn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.schema.output_parser import StrOutputParser\n",
        "from langchain.schema.runnable import RunnableLambda\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "#environment variables\n",
        "from google.colab import userdata\n",
        "api=userdata.get('OPEN_AI_KEY')\n",
        "\n",
        "# Create a ChatOpenAI model\n",
        "model = ChatOpenAI(model=\"gpt-4o-mini\",api_key=api)\n",
        "\n",
        "# Define prompt templates\n",
        "prompt_template = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"You are a comedian who tells jokes about {topic}.\"),\n",
        "        (\"human\", \"Tell me {joke_count} jokes.\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Define additional processing steps using RunnableLambda\n",
        "uppercase_output = RunnableLambda(lambda x: x.upper())\n",
        "count_words = RunnableLambda(lambda x: f\"Word count: {len(x.split())}\\n{x}\")\n",
        "\n",
        "# Create the combined chain using LangChain Expression Language (LCEL)\n",
        "chain = prompt_template | model | StrOutputParser() | uppercase_output | count_words\n",
        "\n",
        "# Run the chain\n",
        "result = chain.invoke({\"topic\": \"cats\", \"joke_count\": 3})\n",
        "\n",
        "# Output\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RXISKPg4o_RC",
        "outputId": "73783d25-9fe7-4709-de3a-125e5b1c7ef7"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word count: 61\n",
            "SURE, HERE ARE THREE CAT-THEMED JOKES FOR YOU:\n",
            "\n",
            "1. WHY DID THE CAT SIT ON THE COMPUTER?\n",
            "   BECAUSE IT WANTED TO KEEP AN EYE ON THE MOUSE!\n",
            "\n",
            "2. WHAT DO CATS LIKE TO EAT FOR BREAKFAST?\n",
            "   MICE KRISPIES!\n",
            "\n",
            "3. WHY WAS THE CAT SO GOOD AT VIDEO GAMES?\n",
            "   BECAUSE IT HAD NINE LIVES AND ENDLESS RETRIES! \n",
            "\n",
            "HOPE THESE MADE YOU LAUGH!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Parallel Chain"
      ],
      "metadata": {
        "id": "sDyYqRgssKCZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.schema.output_parser import StrOutputParser\n",
        "from langchain.schema.runnable import RunnableParallel, RunnableLambda\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "#environment variables\n",
        "from google.colab import userdata\n",
        "api=userdata.get('OPEN_AI_KEY')\n",
        "\n",
        "# Create a ChatOpenAI model\n",
        "model = ChatOpenAI(model=\"gpt-4o-mini\",api_key=api)\n",
        "\n",
        "# Define prompt template\n",
        "prompt_template = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"You are an expert product reviewer.\"),\n",
        "        (\"human\", \"List the main features of the product {product_name}.\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "\n",
        "# Define pros analysis step\n",
        "def analyze_pros(features):\n",
        "    pros_template = ChatPromptTemplate.from_messages(\n",
        "        [\n",
        "            (\"system\", \"You are an expert product reviewer.\"),\n",
        "            (\n",
        "                \"human\",\n",
        "                \"Given these features: {features}, list the pros of these features.\",\n",
        "            ),\n",
        "        ]\n",
        "    )\n",
        "    return pros_template.format_prompt(features=features)\n",
        "\n",
        "\n",
        "# Define cons analysis step\n",
        "def analyze_cons(features):\n",
        "    cons_template = ChatPromptTemplate.from_messages(\n",
        "        [\n",
        "            (\"system\", \"You are an expert product reviewer.\"),\n",
        "            (\n",
        "                \"human\",\n",
        "                \"Given these features: {features}, list the cons of these features.\",\n",
        "            ),\n",
        "        ]\n",
        "    )\n",
        "    return cons_template.format_prompt(features=features)\n",
        "\n",
        "\n",
        "# Combine pros and cons into a final review\n",
        "def combine_pros_cons(pros, cons):\n",
        "    return f\"Pros:\\n{pros}\\n\\nCons:\\n{cons}\"\n",
        "\n",
        "\n",
        "# Simplify branches with LCEL\n",
        "pros_branch_chain = (\n",
        "    RunnableLambda(lambda x: analyze_pros(x)) | model | StrOutputParser()\n",
        ")\n",
        "\n",
        "cons_branch_chain = (\n",
        "    RunnableLambda(lambda x: analyze_cons(x)) | model | StrOutputParser()\n",
        ")\n",
        "\n",
        "# Create the combined chain using LangChain Expression Language (LCEL)\n",
        "chain = (\n",
        "    prompt_template\n",
        "    | model\n",
        "    | StrOutputParser()\n",
        "    | RunnableParallel(branches={\"pros\": pros_branch_chain, \"cons\": cons_branch_chain})\n",
        "    | RunnableLambda(lambda x: combine_pros_cons(x[\"branches\"][\"pros\"], x[\"branches\"][\"cons\"]))\n",
        ")\n",
        "\n",
        "# Run the chain\n",
        "result = chain.invoke({\"product_name\": \"MacBook Pro\"})\n",
        "\n",
        "# Output\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GoTXss2bsGS3",
        "outputId": "c91052a6-bcae-444c-d7fe-740b4bd33315"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pros:\n",
            "Certainly! Here’s a breakdown of the pros associated with each feature of the MacBook Pro:\n",
            "\n",
            "1. **Display**:\n",
            "   - **High-Resolution Retina Display**: Delivers crisp visuals and sharp text, making it ideal for graphic design, video editing, and other visual tasks.\n",
            "   - **True Tone Technology**: Ensures that colors appear consistent under different lighting conditions, enhancing the viewing experience.\n",
            "   - **P3 Wide Color Gamut**: Facilitates vibrant, true-to-life colors, which is crucial for professionals in photography and videography.\n",
            "   - **ProMotion Technology**: Offers smooth scrolling and responsive visuals, improving the overall user experience, especially during gaming or high-paced content interaction.\n",
            "\n",
            "2. **Processor**:\n",
            "   - **M-Series Chips**: Provide leading performance and energy efficiency, enabling demanding tasks to be executed with ease and extending battery life.\n",
            "   - **Multiple Core Options**: Tailor the device according to specific professional needs, allowing for everything from simple tasks to complex computations.\n",
            "\n",
            "3. **Graphics**:\n",
            "   - **Integrated Graphics Performance**: Enhances performance in graphics-intensive apps, making it suitable for gaming, 3D rendering, and video editing.\n",
            "   - **Higher Core Count on M1 Pro and M1 Max**: Allows for more efficient multitasking and processing in graphic-heavy workloads.\n",
            "\n",
            "4. **Memory (RAM)**:\n",
            "   - **Unified Memory Options**: Facilitates impressive multitasking capabilities and quick data access speeds, essential for professionals who run heavy applications simultaneously.\n",
            "\n",
            "5. **Storage**:\n",
            "   - **Fast SSD Options**: Results in quick load times for applications and files, significantly enhancing productivity and efficiency in workflows.\n",
            "\n",
            "6. **Connectivity**:\n",
            "   - **Thunderbolt / USB 4 Ports**: Enable high-speed data transfers and the ability to connect multiple monitors, enhancing versatility for workspaces.\n",
            "   - **MagSafe 3**: Provides easy and secure charging, reducing the risk of accidental disconnections.\n",
            "   - **HDMI and SD Card Slot**: Added convenience for professionals working with cameras and external displays without needing additional adapters.\n",
            "\n",
            "7. **Keyboard**:\n",
            "   - **Magic Keyboard**: Offers a comfortable typing experience, which is crucial for long typing sessions or coding.\n",
            "   - **Return to Function Keys**: Provides a more traditional experience for users accustomed to physical function keys.\n",
            "\n",
            "8. **Audio**:\n",
            "   - **High-Fidelity Sound System**: Delivers rich audio quality for media consumption, making it great for music, movies, and video calls.\n",
            "   - **Studio-Quality Microphone Array**: Ensures clear sound during calls and recordings, beneficial for remote work and podcasting.\n",
            "\n",
            "9. **Battery Life**:\n",
            "   - **Exceptional Longevity**: Allows users to work effectively for an extended period without needing a recharge, which is particularly beneficial for mobile professionals.\n",
            "\n",
            "10. **Operating System**:\n",
            "    - **macOS Integration**: Ensures a seamless user experience across Apple products, enhancing productivity with continuity features.\n",
            "\n",
            "11. **Build Quality**:\n",
            "    - **Durable Chassis**: The premium aluminum build is both aesthetically pleasing and robust, contributing to longevity and reliability.\n",
            "    - **Lightweight and Portable**: Easy to carry, making it a suitable choice for professionals who need to travel.\n",
            "\n",
            "12. **Security Features**:\n",
            "    - **Touch ID**: Offers quick and secure authentication, ensuring user privacy and data security.\n",
            "    - **Enhanced Security in M-Series Chips**: Provides robust protections against malware and unauthorized access, crucial for professional environments.\n",
            "\n",
            "13. **Color Options**:\n",
            "    - **Variety of Finishes**: Allows users to choose a device that matches their personal style, adding a touch of individuality to a professional tool.\n",
            "\n",
            "Overall, the MacBook Pro's features are tailored to meet the demands of professionals and power users, ensuring exceptional performance, reliability, and user comfort across everyday tasks and more demanding applications.\n",
            "\n",
            "Cons:\n",
            "While the MacBook Pro models offer a myriad of impressive features for professionals and power users, there are also several drawbacks that potential buyers should consider:\n",
            "\n",
            "1. **Display**:\n",
            "   - **Brightness & Reflection**: Although the Retina Display is high-quality, under bright lighting conditions, glare can be an issue, impacting visibility.\n",
            "   - **Limited Touch Interaction**: Unlike some competitors, the MacBook Pro lacks a touchscreen, which may frustrate users who desire direct interaction with their displays.\n",
            "\n",
            "2. **Processor**:\n",
            "   - **Compatibility**: Older software or applications not optimized for Apple’s M-series chips may experience compatibility issues or performance lags.\n",
            "   - **No Upgrades**: The types of processors are built-in and cannot be upgraded post-purchase, limiting future-proofing.\n",
            "\n",
            "3. **Graphics**:\n",
            "   - **Integrated**: While the integrated graphics in M1 Pro and M1 Max models are powerful, dedicated graphics cards may still outperform them for certain high-end gaming or graphical applications.\n",
            "\n",
            "4. **Memory (RAM)**:\n",
            "   - **Cost of Upgrades**: Upgrading RAM can be quite expensive during initial purchase, adding significant costs to high-end configurations.\n",
            "   - **No User Upgradeable Options**: Like the processor, memory is soldered in and cannot be upgraded later.\n",
            "\n",
            "5. **Storage**:\n",
            "   - **Price Premium**: The pricing for higher storage capacities (1TB and above) can be steep, making it cost-prohibitive for some users.\n",
            "   - **No User Expandability**: Similar to RAM, the SSD is not user-replaceable or expandable after purchase.\n",
            "\n",
            "6. **Connectivity**:\n",
            "   - **Limited Ports**: While Thunderbolt/USB 4 ports offer speed, having fewer ports can be a limitation without the need for adapters or docks.\n",
            "   - **No USB-A**: The absence of traditional USB-A ports may inconvenience users who have legacy devices or peripherals.\n",
            "\n",
            "7. **Keyboard**:\n",
            "   - **Adaptation Period**: Users transitioning from models with a Touch Bar may need time to adapt to the standard function keys.\n",
            "   - **Potential Issues**: While the Magic Keyboard has addressed past reliability issues, any keyboard can encounter wear and tear over time.\n",
            "\n",
            "8. **Audio**:\n",
            "   - **No 3.5mm jack**: While newer models retain the headphone jack which was previously removed, the quality of audio output through the jack is subjective and might not meet audiophile standards.\n",
            "\n",
            "9. **Battery Life**:\n",
            "   - **Real-world Usage**: While advertised battery life is impressive, intensive tasks or high-brightness settings can significantly reduce battery longevity.\n",
            "   - **Charging Time**: Rapid charging can be an advantage, but users may find waiting for the laptop to fully charge inconvenient for those on the go.\n",
            "\n",
            "10. **Operating System**:\n",
            "    - **macOS Limitations**: Some users may prefer Windows for specific applications, and switching operating systems can pose a learning curve.\n",
            "    - **Software Availability**: Certain software may not be available or optimized for macOS, limiting options for professional use.\n",
            "\n",
            "11. **Build Quality**:\n",
            "    - **Durability Risks**: The sleek, lightweight design, while premium, may be more susceptible to scratches and dings compared to bulkier laptops.\n",
            "    - **Heat Management**: Under heavy workloads, the laptop can generate noticeable heat, which may affect performance or comfort over time.\n",
            "\n",
            "12. **Security Features**:\n",
            "    - **Touch ID Limitations**: The reliance on Touch ID might not be enough for users requiring more advanced biometric security options.\n",
            "    - **Privacy Concerns**: As with all devices, there could be ongoing concerns regarding data privacy, especially with built-in microphones and cameras.\n",
            "\n",
            "13. **Color Options**:\n",
            "    - **Limited Choices**: While the standard colors are elegant, the lack of diverse color options may not appeal to users wanting personalization.\n",
            "\n",
            "Overall, while the MacBook Pro is a powerful and attractive option for many professionals, its high price point, potential limitations in connectivity, upgradeability, and compatibility can be valid considerations for prospective buyers. Always assess personal needs and usage scenarios before investing in a high-end device.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Branched Chains"
      ],
      "metadata": {
        "id": "0AEfkQue3CSV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.schema.output_parser import StrOutputParser\n",
        "from langchain.schema.runnable import RunnableBranch\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "#environment variables\n",
        "from google.colab import userdata\n",
        "api=userdata.get('OPEN_AI_KEY')\n",
        "\n",
        "# Create a ChatOpenAI model\n",
        "model = ChatOpenAI(model=\"gpt-4o-mini\",api_key=api)\n",
        "\n",
        "\n",
        "# Define prompt templates for different feedback types\n",
        "positive_feedback_template = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"You are a helpful assistant.\"),\n",
        "        (\"human\",\n",
        "         \"Generate a thank you note for this positive feedback: {feedback}.\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "negative_feedback_template = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"You are a helpful assistant.\"),\n",
        "        (\"human\",\n",
        "         \"Generate a response addressing this negative feedback: {feedback}.\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "neutral_feedback_template = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"You are a helpful assistant.\"),\n",
        "        (\n",
        "            \"human\",\n",
        "            \"Generate a request for more details for this neutral feedback: {feedback}.\",\n",
        "        ),\n",
        "    ]\n",
        ")\n",
        "\n",
        "escalate_feedback_template = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"You are a helpful assistant.\"),\n",
        "        (\n",
        "            \"human\",\n",
        "            \"Generate a message to escalate this feedback to a human agent: {feedback}.\",\n",
        "        ),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Define the feedback classification template\n",
        "classification_template = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"You are a helpful assistant.\"),\n",
        "        (\"human\",\n",
        "         \"Classify the sentiment of this feedback as positive, negative, neutral, or escalate: {feedback}.\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Define the runnable branches for handling feedback\n",
        "branches = RunnableBranch(\n",
        "    (\n",
        "        lambda x: \"positive\" in x,\n",
        "        positive_feedback_template | model | StrOutputParser()  # Positive feedback chain\n",
        "    ),\n",
        "    (\n",
        "        lambda x: \"negative\" in x,\n",
        "        negative_feedback_template | model | StrOutputParser()  # Negative feedback chain\n",
        "    ),\n",
        "    (\n",
        "        lambda x: \"neutral\" in x,\n",
        "        neutral_feedback_template | model | StrOutputParser()  # Neutral feedback chain\n",
        "    ),\n",
        "    escalate_feedback_template | model | StrOutputParser()\n",
        ")\n",
        "\n",
        "# Create the classification chain\n",
        "classification_chain = classification_template | model | StrOutputParser()\n",
        "\n",
        "# Combine classification and response generation into one chain\n",
        "chain = classification_chain | branches\n",
        "\n",
        "# Run the chain with an example review\n",
        "# Good review - \"The product is excellent. I really enjoyed using it and found it very helpful.\"\n",
        "# Bad review - \"The product is terrible. It broke after just one use and the quality is very poor.\"\n",
        "# Neutral review - \"The product is okay. It works as expected but nothing exceptional.\"\n",
        "# Default - \"I'm not sure about the product yet. Can you tell me more about its features and benefits?\"\n",
        "\n",
        "review = \"The product is terrible. It broke after just one use and the quality is very poor.\"\n",
        "result = chain.invoke({\"feedback\": review})\n",
        "\n",
        "# Output the result\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qtafj7wAtR0N",
        "outputId": "dfb2550f-e074-4d40-ca6a-04de3bc9914d"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thank you for taking the time to share your feedback. I genuinely appreciate your input and I’m sorry to hear that your experience did not meet your expectations. Your concerns are important to us, and we are committed to improving. \n",
            "\n",
            "Could you please provide more specific details about what went wrong? This information will help us understand the situation better and work towards a solution. We value your opinion and want to ensure we make things right. Thank you again for your feedback, and I hope to hear from you soon.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "xxzmDedfB67k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RAG - Ingestion"
      ],
      "metadata": {
        "id": "o5gOHpHKB-hB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "NQ05tvOmCCF8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pypdf\n",
        "import os\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.document_loaders import PyPDFLoader # Changed to PyPDFLoader\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "\n",
        "# Define file path (Assuming it's in the current directory)\n",
        "file_path = '/content/RehanKhanResume.pdf'\n",
        "persistent_directory = '/content/db/chroma_db'\n",
        "\n",
        "# Check if the Chroma vector store already exists\n",
        "if not os.path.exists(persistent_directory):\n",
        "    print(\"Persistent directory does not exist. Initializing vector store...\")\n",
        "\n",
        "    # Ensure the text file exists\n",
        "    if not os.path.exists(file_path):\n",
        "        raise FileNotFoundError(\n",
        "            f\"The file {file_path} does not exist. Please check the path.\"\n",
        "        )\n",
        "\n",
        "    # Read the text content from the file\n",
        "    loader = PyPDFLoader(file_path) # Changed to PyPDFLoader\n",
        "    documents = loader.load()\n",
        "\n",
        "    # Split the document into chunks\n",
        "    text_splitter = CharacterTextSplitter(chunk_size=200, chunk_overlap=5)\n",
        "    docs = text_splitter.split_documents(documents)\n",
        "\n",
        "    # Display information about the split documents\n",
        "    print(\"\\n--- Document Chunks Information ---\")\n",
        "    print(f\"Number of document chunks: {len(docs)}\")\n",
        "    print(f\"Sample chunk:\\n{docs[0].page_content}\\n\")\n",
        "\n",
        "    # Create embeddings\n",
        "    print(\"\\n--- Creating embeddings ---\")\n",
        "    embeddings = OpenAIEmbeddings(\n",
        "        model=\"text-embedding-ada-002\", openai_api_key=api\n",
        "    )\n",
        "    print(\"\\n--- Finished creating embeddings ---\")\n",
        "\n",
        "    # Create the vector store and persist it automatically\n",
        "    print(\"\\n--- Creating vector store ---\")\n",
        "    db = Chroma.from_documents(\n",
        "        docs, embeddings, persist_directory=persistent_directory)\n",
        "    print(\"\\n--- Finished creating vector store ---\")\n",
        "\n",
        "else:\n",
        "    print(\"Vector store already exists. No need to initialize.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QyuoKqbO6B_S",
        "outputId": "a0596553-4f76-4e7f-92de-e0b9f0248755"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pypdf in /usr/local/lib/python3.11/dist-packages (5.4.0)\n",
            "Persistent directory does not exist. Initializing vector store...\n",
            "\n",
            "--- Document Chunks Information ---\n",
            "Number of document chunks: 2\n",
            "Sample chunk:\n",
            "Rehan Khan\n",
            "dayel.rehan@gmail.com\n",
            " \n",
            "+918176987106\n",
            " \n",
            "rehan-khan25\n",
            " \n",
            "ryyhan\n",
            " \n",
            "ORCID\n",
            " \n",
            "Professional Experience\n",
            "Troopr Labs\n",
            "AI Engineer\n",
            "Jul 2024 – Dec 2024 | Texas, Remote\n",
            "•Built a Helpdesk Assessment Feature, generating detailed CRM reports (Jira, Salesforce) by \n",
            "categorizing 5M+ tickets into topic-based groups for streamlined queries.\n",
            "•Automated knowledge article creation from cases, training AI Agents to cut LLM token costs and \n",
            "boost query resolution efficiency by 30%.\n",
            "•Reduced customer query times using AI-trained knowledge articles, improving response accuracy \n",
            "by 20% and lowering costs.\n",
            "•Designed Multi-AI Agent workflows with LangGraph, enhancing collaboration and workflow \n",
            "efficiency by 25%\n",
            "Opsight AI\n",
            "AI/ML Intern\n",
            "Jan 2024 – Jun 2024 | New Delhi, India\n",
            "•Built a Retrieval-Augmented Generation (RAG) system, slashing info retrieval time from 10-15 mins \n",
            "to <1 min, boosting efficiency by 90%.\n",
            "•Led Elite Robots’ EC series arm project, collecting 100GB+ data and integrating InfluxDB with 50+ \n",
            "tags for real-time monitoring and Python-based parameter control.\n",
            "•Fine-tuned LLMs for domain-specific datasets, enhancing model performance.\n",
            "National Institute of Technology Trichy (NIT Trichy)\n",
            "Summer Intern\n",
            "May 2023 – Jul 2023 | Trichy, Remote\n",
            "•Conducted 6+ months of network lateral movement research, identifying and mitigating threats, \n",
            "and publishing actionable insights.\n",
            "•Explored advanced cybersecurity techniques (cyber deception, anomaly detection) using machine \n",
            "learning, reducing false positives by 15% and improving incident response efficiency.\n",
            "Education\n",
            "Oriental Institute of Science and Technology 2020 – 2024 | Bhopal\n",
            "Bachelor of Technology (B.Tech) - Computer Science and Engineering - (Data Science) GPA-8.65\n",
            "Projects\n",
            "Algometrix [Available on PYPI, the official Python Package Index]\n",
            "Python, scikit-learn, pandas, numpy, PyPI\n",
            "•Created a Python library for data scientists, speeding up ML model selection and evaluation by \n",
            "25%.\n",
            "•Published on PyPI, achieving 1,000+ downloads with seamless pip install integration.\n",
            "AI Assess\n",
            "Python, Streamlit, FastAPI, OpenAI, LangChain, PyPDF2\n",
            "•Developed an AI tool for educators, generating 10+ customizable MCQs/subjective questions from \n",
            "PDFs or keywords with adjustable difficulty.\n",
            "•Built a FastAPI backend and Streamlit frontend, automating answer evaluation for 30% faster query \n",
            "resolution and real-time scoring.\n",
            "•Improved evaluation accuracy by 20% with robust normalization for inconsistent inputs.\n",
            "Hybrid Fine-Tuned Agentic RAG\n",
            "Python, Langchain, PyTorch, Transformers, FastAPI, FineTuned LLM\n",
            "•Engineered a Retrieval-Augmented Generation (RAG) system with a fine-tuned LLM, integrating \n",
            "hybrid search (vector embeddings + BM25 keyword search) to boost query relevance by 35% and \n",
            "cut retrieval time by 50%.\n",
            "•Optimized retrieval with semantic chunking, multi-query expansion, and cross-encoder reranking, \n",
            "improving context precision by 40% for complex queries.\n",
            "\n",
            "\n",
            "--- Creating embeddings ---\n",
            "\n",
            "--- Finished creating embeddings ---\n",
            "\n",
            "--- Creating vector store ---\n",
            "\n",
            "--- Finished creating vector store ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "# Define the persistent directory\n",
        "persistent_directory = '/content/db/chroma_db'\n",
        "\n",
        "# Define the embedding model\n",
        "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\",openai_api_key=api)\n",
        "\n",
        "# Load the existing vector store with the embedding function\n",
        "db = Chroma(persist_directory=persistent_directory,\n",
        "            embedding_function=embeddings)\n",
        "\n",
        "# Define the user's question\n",
        "query = \"Tell me about professional experience in RAG??\"\n",
        "\n",
        "# Retrieve relevant documents based on the query\n",
        "retriever = db.as_retriever(\n",
        "    search_type=\"similarity_score_threshold\",\n",
        "    search_kwargs={\"k\": 3, \"score_threshold\": 0.1},\n",
        ")\n",
        "relevant_docs = retriever.invoke(query)\n",
        "\n",
        "# Display the relevant results with metadata\n",
        "print(\"\\n--- Relevant Documents ---\")\n",
        "for i, doc in enumerate(relevant_docs, 1):\n",
        "    print(f\"Document {i}:\\n{doc.page_content}\\n\")\n",
        "    if doc.metadata:\n",
        "        print(f\"Source: {doc.metadata.get('source', 'Unknown')}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iZ78bHc2_lJw",
        "outputId": "ee3b7e43-7cd6-4166-9be1-45cbfa363511"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/langchain_core/vectorstores/base.py:1082: UserWarning: Relevance scores must be between 0 and 1, got [(Document(metadata={'creator': 'FlowCV - https://flowcv.com', 'moddate': '2025-03-06T12:30:23+00:00', 'creationdate': '2025-03-06T12:30:23+00:00', 'page_label': '1', 'source': '/content/RehanKhanResume.pdf', 'producer': 'Skia/PDF m131', 'page': 0, 'keywords': 'Free Online Resume Builder, FlowCV - https://flowcv.com', 'total_pages': 2}, page_content='Rehan Khan\\ndayel.rehan@gmail.com\\n \\n+918176987106\\n \\nrehan-khan25\\n \\nryyhan\\n \\nORCID\\n \\nProfessional Experience\\nTroopr Labs\\nAI Engineer\\nJul\\xa02024 – Dec\\xa02024 | Texas, Remote\\n•Built a Helpdesk Assessment Feature, generating detailed CRM reports (Jira, Salesforce) by \\ncategorizing 5M+ tickets into topic-based groups for streamlined queries.\\n•Automated knowledge article creation from cases, training AI Agents to cut LLM token costs and \\nboost query resolution efficiency by 30%.\\n•Reduced customer query times using AI-trained knowledge articles, improving response accuracy \\nby 20% and lowering costs.\\n•Designed Multi-AI Agent workflows with LangGraph, enhancing collaboration and workflow \\nefficiency by 25%\\nOpsight AI\\nAI/ML Intern\\nJan\\xa02024 – Jun\\xa02024 | New Delhi, India\\n•Built a Retrieval-Augmented Generation (RAG) system, slashing info retrieval time from 10-15 mins \\nto <1 min, boosting efficiency by 90%.\\n•Led Elite Robots’ EC series arm project, collecting 100GB+ data and integrating InfluxDB with 50+ \\ntags for real-time monitoring and Python-based parameter control.\\n•Fine-tuned LLMs for domain-specific datasets, enhancing model performance.\\nNational Institute of Technology Trichy (NIT Trichy)\\nSummer Intern\\nMay\\xa02023 – Jul\\xa02023 | Trichy, Remote\\n•Conducted 6+ months of network lateral movement research, identifying and mitigating threats, \\nand publishing actionable insights.\\n•Explored advanced cybersecurity techniques (cyber deception, anomaly detection) using machine \\nlearning, reducing false positives by 15% and improving incident response efficiency.\\nEducation\\nOriental Institute of Science and Technology 2020 – 2024 | Bhopal\\nBachelor of Technology (B.Tech) - Computer Science and Engineering - (Data Science) GPA-8.65\\nProjects\\nAlgometrix [Available on PYPI, the official Python Package Index]\\nPython, scikit-learn, pandas, numpy, PyPI\\n•Created a Python library for data scientists, speeding up ML model selection and evaluation by \\n25%.\\n•Published on PyPI, achieving 1,000+ downloads with seamless pip install integration.\\nAI Assess\\nPython, Streamlit, FastAPI, OpenAI, LangChain, PyPDF2\\n•Developed an AI tool for educators, generating 10+ customizable MCQs/subjective questions from \\nPDFs or keywords with adjustable difficulty.\\n•Built a FastAPI backend and Streamlit frontend, automating answer evaluation for 30% faster query \\nresolution and real-time scoring.\\n•Improved evaluation accuracy by 20% with robust normalization for inconsistent inputs.\\nHybrid Fine-Tuned Agentic RAG\\nPython, Langchain, PyTorch, Transformers, FastAPI, FineTuned LLM\\n•Engineered a Retrieval-Augmented Generation (RAG) system with a fine-tuned LLM, integrating \\nhybrid search (vector embeddings + BM25 keyword search) to boost query relevance by 35% and \\ncut retrieval time by 50%.\\n•Optimized retrieval with semantic chunking, multi-query expansion, and cross-encoder reranking, \\nimproving context precision by 40% for complex queries.'), -0.3642309403744495), (Document(metadata={'creationdate': '2025-03-06T12:30:23+00:00', 'producer': 'Skia/PDF m131', 'source': '/content/RehanKhanResume.pdf', 'page': 1, 'keywords': 'Free Online Resume Builder, FlowCV - https://flowcv.com', 'total_pages': 2, 'page_label': '2', 'creator': 'FlowCV - https://flowcv.com', 'moddate': '2025-03-06T12:30:23+00:00'}, page_content=\"•Deployed a FastAPI REST interface with dynamic caching and load balancing, enhancing cross-\\nplatform accessibility by 80% and ensuring scalability.\\nPublications (50+ Citations)\\nConversational AI : Dialoguing Most Humanly With Non-Humans- Wiley Publications\\nThe Promises and Perils of Artificial Intelligence: An Ethical and Social Analysis, IGI Global\\nSkills\\nProgramming:\\xa0Python, Java, SQL\\nAI/ML:\\xa0LangChain, LangGraph, PyTorch, Agentic AI, HuggingFace, scikit-learn, NLP, Generative AI,\\nLLMs, Llama Index, Open AI,\\nTools:\\xa0Git/GitHub, Docker, Linux, SQLite, FastAPI, Streamlit, Vector Databases, ChromaDB, Pinecone\\nLibraries:\\xa0Streamlit, Pandas, NumPy, NLTK, PyPDF2, PyMuPDF,\\nCertificates\\nMachine Learning by Andrew Ng , Harvard's CS50 - Python , AWS Machine Learning Foundations\\n, Philosophy of Science , AI, Empathy & Ethics , Philosophy and Cognitive Sciences , \\nGenerative AI with LLMs , Introduction to LangGraph\"), -0.3815025503068199)]\n",
            "  self.vectorstore.similarity_search_with_relevance_scores(\n",
            "WARNING:langchain_core.vectorstores.base:No relevant docs were retrieved using the relevance score threshold 0.1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Relevant Documents ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TTVYzLAaCTzQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}