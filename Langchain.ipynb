{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMoyEAsHQylgBTPZ+Sovacy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ryyhan/RandomCodes/blob/main/Langchain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Basic Chat\n"
      ],
      "metadata": {
        "id": "WLjYXD-eDade"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade langchain langchain_core langchain_openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "3iYxf6v9DsFW",
        "outputId": "bd38f4fa-581b-44b7-9f10-2d2bf1a68db6"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.24)\n",
            "Collecting langchain\n",
            "  Downloading langchain-0.3.25-py3-none-any.whl.metadata (7.8 kB)\n",
            "Requirement already satisfied: langchain_core in /usr/local/lib/python3.11/dist-packages (0.3.56)\n",
            "Collecting langchain_core\n",
            "  Downloading langchain_core-0.3.59-py3-none-any.whl.metadata (5.9 kB)\n",
            "Collecting langchain_openai\n",
            "  Downloading langchain_openai-0.3.16-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.8)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.39)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.11.4)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.40)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain_core) (9.1.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain_core) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain_core) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain_core) (4.13.2)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.68.2 in /usr/local/lib/python3.11/dist-packages (from langchain_openai) (1.76.2)\n",
            "Collecting tiktoken<1,>=0.7 (from langchain_openai)\n",
            "  Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain_core) (3.0.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.18)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.68.2->langchain_openai) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.68.2->langchain_openai) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.68.2->langchain_openai) (0.9.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.68.2->langchain_openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.68.2->langchain_openai) (4.67.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2025.4.26)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken<1,>=0.7->langchain_openai) (2024.11.6)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (0.16.0)\n",
            "Downloading langchain-0.3.25-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_core-0.3.59-py3-none-any.whl (437 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m437.7/437.7 kB\u001b[0m \u001b[31m29.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_openai-0.3.16-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.8/62.8 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m53.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tiktoken, langchain_core, langchain_openai, langchain\n",
            "  Attempting uninstall: langchain_core\n",
            "    Found existing installation: langchain-core 0.3.56\n",
            "    Uninstalling langchain-core-0.3.56:\n",
            "      Successfully uninstalled langchain-core-0.3.56\n",
            "  Attempting uninstall: langchain\n",
            "    Found existing installation: langchain 0.3.24\n",
            "    Uninstalling langchain-0.3.24:\n",
            "      Successfully uninstalled langchain-0.3.24\n",
            "Successfully installed langchain-0.3.25 langchain_core-0.3.59 langchain_openai-0.3.16 tiktoken-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2MmHoZ6hApFa",
        "outputId": "8db87383-374a-4216-ba8f-e56457c8dfbd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Full result:\n",
            "content='81 divided by 9 equals 9.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 16, 'total_tokens': 26, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_0392822090', 'id': 'chatcmpl-BVjeVCFrcLYgIkfBkDgIiXcnQPt3J', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='run--7fcd0b15-6316-4ae9-a25b-02818077fc88-0' usage_metadata={'input_tokens': 16, 'output_tokens': 10, 'total_tokens': 26, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
            "Content only:\n",
            "81 divided by 9 equals 9.\n"
          ]
        }
      ],
      "source": [
        "#imports\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "#environment variables\n",
        "from google.colab import userdata\n",
        "api=userdata.get('OPEN_AI_KEY')\n",
        "\n",
        "\n",
        "# Create a ChatOpenAI model\n",
        "model = ChatOpenAI(model=\"gpt-4o-mini\",api_key=api)\n",
        "\n",
        "# Invoke the model with a message\n",
        "result = model.invoke(\"What is 81 divided by 9?\")\n",
        "print(\"Full result:\")\n",
        "print(result)\n",
        "print(\"Content only:\")\n",
        "print(result.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conversation"
      ],
      "metadata": {
        "id": "RlmNyBYVLjTp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "\n",
        "#environment variables\n",
        "from google.colab import userdata\n",
        "api=userdata.get('OPEN_AI_KEY')\n",
        "\n",
        "\n",
        "# Create a ChatOpenAI model\n",
        "model = ChatOpenAI(model=\"gpt-4o-mini\",api_key=api)\n",
        "\n",
        "\n",
        "messages = [\n",
        "    SystemMessage(content=\"Solve the following math problems\"),\n",
        "    HumanMessage(content=\"What is 81 divided by 9?\"),\n",
        "]\n",
        "\n",
        "# Invoke the model with messages\n",
        "result = model.invoke(messages)\n",
        "print(f\"Answer from AI (Single): {result.content}\")\n",
        "\n",
        "\n",
        "# AIMessage:\n",
        "#   Message from an AI.\n",
        "messages = [\n",
        "    SystemMessage(content=\"Solve the following math problems\"),\n",
        "    HumanMessage(content=\"What is 81 divided by 9?\"),\n",
        "    AIMessage(content=\"81 divided by 9 is 9.\"),\n",
        "    HumanMessage(content=\"What is 10 times 5?\"),\n",
        "]\n",
        "\n",
        "# Invoke the model with messages\n",
        "result = model.invoke(messages)\n",
        "print(f\"Answer from AI (Multiple): {result.content}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Omxka0qoDdD8",
        "outputId": "c374817f-6aa3-491a-e70d-fee996071179"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer from AI (Single): 81 divided by 9 is 9.\n",
            "Answer from AI (Multiple): 10 times 5 is 50.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Chatbot Conversations"
      ],
      "metadata": {
        "id": "baW8PAR8RtT2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.schema import AIMessage, HumanMessage, SystemMessage\n",
        "\n",
        "#environment variables\n",
        "from google.colab import userdata\n",
        "api=userdata.get('OPEN_AI_KEY')\n",
        "\n",
        "\n",
        "# Create a ChatOpenAI model\n",
        "model = ChatOpenAI(model=\"gpt-4o-mini\",api_key=api)\n",
        "\n",
        "\n",
        "chat_history = []  # Use a list to store messages\n",
        "\n",
        "# Set an initial system message (optional)\n",
        "system_message = SystemMessage(content=\"You are a helpful AI assistant.\")\n",
        "chat_history.append(system_message)  # Add system message to chat history\n",
        "\n",
        "# Chat loop\n",
        "while True:\n",
        "    query = input(\"You: \")\n",
        "    if query.lower() == \"exit\":\n",
        "        break\n",
        "    chat_history.append(HumanMessage(content=query))  # Add user message\n",
        "\n",
        "    # Get AI response using history\n",
        "    result = model.invoke(chat_history)\n",
        "    response = result.content\n",
        "    chat_history.append(AIMessage(content=response))  # Add AI message\n",
        "\n",
        "    print(f\"AI: {response}\")\n",
        "\n",
        "\n",
        "print(\"---- Message History ----\")\n",
        "print(chat_history)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dmchOq7YRs85",
        "outputId": "0a24aa07-5237-4f30-8dcb-1a98ef453414"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You: exit\n",
            "---- Message History ----\n",
            "[SystemMessage(content='You are a helpful AI assistant.', additional_kwargs={}, response_metadata={})]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example Source: https://python.langchain.com/v0.2/docs/integrations/memory/google_firestore/\n",
        "\n",
        "from google.cloud import firestore\n",
        "from langchain_google_firestore import FirestoreChatMessageHistory\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "\"\"\"\n",
        "Steps to replicate this example:\n",
        "1. Create a Firebase account\n",
        "2. Create a new Firebase project\n",
        "    - Copy the project ID\n",
        "3. Create a Firestore database in the Firebase project\n",
        "4. Install the Google Cloud CLI on your computer\n",
        "    - https://cloud.google.com/sdk/docs/install\n",
        "    - Authenticate the Google Cloud CLI with your Google account\n",
        "        - https://cloud.google.com/docs/authentication/provide-credentials-adc#local-dev\n",
        "    - Set your default project to the new Firebase project you created\n",
        "5. Enable the Firestore API in the Google Cloud Console:\n",
        "    - https://console.cloud.google.com/apis/enableflow?apiid=firestore.googleapis.com&project=crewai-automation\n",
        "\"\"\"\n",
        "\n",
        "#environment variables\n",
        "from google.colab import userdata\n",
        "api=userdata.get('OPEN_AI_KEY')\n",
        "\n",
        "\n",
        "# Setup Firebase Firestore\n",
        "PROJECT_ID = \"langchain-demo-abf48\"\n",
        "SESSION_ID = \"user_session_new\"  # This could be a username or a unique ID\n",
        "COLLECTION_NAME = \"chat_history\"\n",
        "\n",
        "# Initialize Firestore Client\n",
        "print(\"Initializing Firestore Client...\")\n",
        "client = firestore.Client(project=PROJECT_ID)\n",
        "\n",
        "# Initialize Firestore Chat Message History\n",
        "print(\"Initializing Firestore Chat Message History...\")\n",
        "chat_history = FirestoreChatMessageHistory(\n",
        "    session_id=SESSION_ID,\n",
        "    collection=COLLECTION_NAME,\n",
        "    client=client,\n",
        ")\n",
        "print(\"Chat History Initialized.\")\n",
        "print(\"Current Chat History:\", chat_history.messages)\n",
        "\n",
        "# Create a ChatOpenAI model\n",
        "model = ChatOpenAI(model=\"gpt-4o-mini\",api_key=api)\n",
        "\n",
        "print(\"Start chatting with the AI. Type 'exit' to quit.\")\n",
        "\n",
        "while True:\n",
        "    human_input = input(\"User: \")\n",
        "    if human_input.lower() == \"exit\":\n",
        "        break\n",
        "\n",
        "    chat_history.add_user_message(human_input)\n",
        "\n",
        "    ai_response = model.invoke(chat_history.messages)\n",
        "    chat_history.add_ai_message(ai_response.content)\n",
        "\n",
        "    print(f\"AI: {ai_response.content}\")"
      ],
      "metadata": {
        "id": "ifnx-HPIL8oe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "u8zseiV6hSUd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#prompting"
      ],
      "metadata": {
        "id": "X_Aq47ZAhT_g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "0TDhv16OhXgR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prompt Template Docs:\n",
        "#   https://python.langchain.com/v0.2/docs/concepts/#prompt-templateshttps://python.langchain.com/v0.2/docs/concepts/#prompt-templates\n",
        "\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain_core.messages import HumanMessage\n",
        "\n",
        "# # PART 1: Create a ChatPromptTemplate using a template string\n",
        "template = \"Tell me a joke about {topic}.\"\n",
        "prompt_template = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "print(\"-----Prompt from Template-----\")\n",
        "prompt = prompt_template.invoke({\"topic\": \"cats\"})\n",
        "print(prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WHe2WsKShTHI",
        "outputId": "213af19c-a452-45ae-80c3-4e04352e5d23"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----Prompt from Template-----\n",
            "messages=[HumanMessage(content='Tell me a joke about cats.', additional_kwargs={}, response_metadata={})]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# PART 2: Prompt with Multiple Placeholders\n",
        "template_multiple = \"\"\"You are a helpful assistant.\n",
        "Human: Tell me a {adjective} story about a {animal}.\n",
        "Assistant:\"\"\"\n",
        "prompt_multiple = ChatPromptTemplate.from_template(template_multiple)\n",
        "prompt = prompt_multiple.invoke({\"adjective\": \"funny\", \"animal\": \"panda\"})\n",
        "print(\"\\n----- Prompt with Multiple Placeholders -----\\n\")\n",
        "print(prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mQDYTH4DJkBx",
        "outputId": "633836e3-557d-4001-e207-65adcb2e88ba"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "----- Prompt with Multiple Placeholders -----\n",
            "\n",
            "messages=[HumanMessage(content='You are a helpful assistant.\\nHuman: Tell me a funny story about a panda.\\nAssistant:', additional_kwargs={}, response_metadata={})]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prompt Template with ChatModel"
      ],
      "metadata": {
        "id": "LIFjKTV5LlBR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "#environment variables\n",
        "from google.colab import userdata\n",
        "api=userdata.get('OPEN_AI_KEY')\n",
        "\n",
        "# Create a ChatOpenAI model\n",
        "model = ChatOpenAI(model=\"gpt-4o-mini\",api_key=api)\n",
        "\n",
        "# PART 1: Create a ChatPromptTemplate using a template string\n",
        "print(\"-----Prompt from Template-----\")\n",
        "template = \"Tell me a joke about {topic}.\"\n",
        "prompt_template = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "prompt = prompt_template.invoke({\"topic\": \"cats\"})\n",
        "result = model.invoke(prompt)\n",
        "print(prompt, result.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6TJlO_OTKyT7",
        "outputId": "d7400f9b-fc6e-46e2-b1d2-d2ac5825daf4"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----Prompt from Template-----\n",
            "messages=[HumanMessage(content='Tell me a joke about cats.', additional_kwargs={}, response_metadata={})] Why was the cat sitting on the computer?\n",
            "\n",
            "Because it wanted to keep an eye on the mouse!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# PART 2: Prompt with Multiple Placeholders\n",
        "print(\"\\n----- Prompt with Multiple Placeholders -----\\n\")\n",
        "template_multiple = \"\"\"You are a helpful assistant.\n",
        "Human: Tell me a {adjective} short story about a {animal}.\n",
        "Assistant:\"\"\"\n",
        "prompt_multiple = ChatPromptTemplate.from_template(template_multiple)\n",
        "prompt = prompt_multiple.invoke({\"adjective\": \"funny\", \"animal\": \"cat\"})\n",
        "\n",
        "result = model.invoke(prompt)\n",
        "print(prompt,\"/n\\n\",result.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "usPIM502lUQy",
        "outputId": "118cde7b-45bc-4f79-90cd-8cf94e86f660"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "----- Prompt with Multiple Placeholders -----\n",
            "\n",
            "messages=[HumanMessage(content='You are a helpful assistant.\\nHuman: Tell me a funny short story about a cat.\\nAssistant:', additional_kwargs={}, response_metadata={})] /n\n",
            " Once upon a time in a cozy little town, there lived a clever cat named Whiskers. Whiskers had a knack for mischief and a peculiar obsession with the neighbor’s pet goldfish, Bubbles.\n",
            "\n",
            "One sunny afternoon, as the owner of Bubbles went out for groceries, Whiskers saw his opportunity. He tiptoed through the open window and leaped onto the kitchen counter, where he spotted Bubbles swimming happily in his bowl. Whiskers’s eyes widened as he thought, “Today’s the day I make a splash!”\n",
            "\n",
            "With a swift paw, he nudged the bowl, sending water sloshing everywhere. Bubbles swirled around, unfazed, while Whiskers prepared for his grand entrance. He jumped onto the table, trying to look nonchalant, but his tail had other ideas— it knocked over a stack of newspapers, creating a dramatic crash.\n",
            "\n",
            "The sound startled Bubbles, who swam to the top, and Whiskers, thinking it was time for his “catch of the day,” lunged for the goldfish. But in his excitement, he miscalculated and ended up right in the bowl!\n",
            "\n",
            "The sight of a drenched cat flailing in the water was so hilarious that even Bubbles couldn’t help but gurgle with laughter (or maybe just blow bubbles). A moment later, the owner returned to a scene of chaos: newspapers everywhere, water all over the counter, and Whiskers, dripping wet, sitting proudly in Bubbles’s bowl. \n",
            "\n",
            "The owner burst out laughing, and Whiskers, realizing he was the center of attention, gave an exaggerated shake like a dog, spraying water everywhere. From that day on, Whiskers was known not just as the mischievous cat, but also as “the fishy feline,” and he learned one important lesson: next time, maybe just stick to chasing yarn!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# PART 3: Prompt with System and Human Messages (Using Tuples)\n",
        "print(\"\\n----- Prompt with System and Human Messages (Tuple) -----\\n\")\n",
        "messages = [\n",
        "    (\"system\", \"You are a comedian who tells jokes about {topic}.\"),\n",
        "    (\"human\", \"Tell me {joke_count} jokes.\"),\n",
        "]\n",
        "prompt_template = ChatPromptTemplate.from_messages(messages)\n",
        "prompt = prompt_template.invoke({\"topic\": \"cats\", \"joke_count\": 3})\n",
        "result = model.invoke(prompt)\n",
        "print(prompt,\"\\n\\n\\n\")\n",
        "print(result.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ELOvkj-lgNO",
        "outputId": "5daebdc8-c2a8-4033-a960-32c59842a401"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "----- Prompt with System and Human Messages (Tuple) -----\n",
            "\n",
            "messages=[SystemMessage(content='You are a comedian who tells jokes about cats.', additional_kwargs={}, response_metadata={}), HumanMessage(content='Tell me 3 jokes.', additional_kwargs={}, response_metadata={})] \n",
            "\n",
            "\n",
            "\n",
            "Sure, here are three cat-themed jokes for you:\n",
            "\n",
            "1. Why did the cat sit on the computer?\n",
            "   Because it wanted to keep an eye on the mouse!\n",
            "\n",
            "2. What do you call a pile of cats?\n",
            "   A meow-tain!\n",
            "\n",
            "3. Why was the cat so good at video games?\n",
            "   Because it had nine lives to practice!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FWkafui9lrd-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}