{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOWVY4htLK67wD/ta1H+MLP"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Testing Clustering performance using synthetic numeric data"
      ],
      "metadata": {
        "id": "OavP4Ermbz8W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Silhouette Score:**\n",
        "- Range: Values range from -1 to +1.\n",
        "- +1: Indicates the data point is well-matched to its own cluster and poorly matched to neighboring clusters.\n",
        "- 0: Suggests the data point is on or very close to the decision boundary between two neighboring clusters.\n",
        "- -1: Implies the data point is assigned to the wrong cluster.\n",
        "\n",
        "\n",
        "\n",
        "**Davies-Bouldin Index:**\n",
        "Range: Values range from 0 to infinity.\n",
        "- 0: Represents the ideal scenario with perfectly separated and dissimilar clusters."
      ],
      "metadata": {
        "id": "zt9DQBHecJ0-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.datasets import make_blobs\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score, davies_bouldin_score\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "ZiGg7zicZorR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameters for the synthetic dataset\n",
        "num_samples = 500  # Total samples\n",
        "num_features = 10   # Number of features (dimensionality of each sample)\n",
        "num_clusters = 5    # Number of clusters\n",
        "\n",
        "# Generate synthetic data\n",
        "X, y = make_blobs(n_samples=num_samples, centers=num_clusters, n_features=num_features, random_state=42)\n",
        "df = pd.DataFrame(X, columns=[f'feature_{i}' for i in range(num_features)])\n",
        "df['true_cluster'] = y  # Adding true cluster for validation\n",
        "\n",
        "# Convert to a DataFrame\n",
        "df = pd.DataFrame(X, columns=[f'feature_{i}' for i in range(num_features)])\n",
        "df['cluster'] = y"
      ],
      "metadata": {
        "id": "_7wE9E7uZ22S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
        "df['cluster'] = kmeans.fit_predict(X)\n",
        "\n",
        "# Step 3: Evaluate clustering performance\n",
        "silhouette_avg = silhouette_score(X, df['cluster'])\n",
        "davies_bouldin = davies_bouldin_score(X, df['cluster'])\n",
        "\n",
        "print(f'Silhouette Score: {silhouette_avg}')\n",
        "print(f'Davies-Bouldin Index: {davies_bouldin}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JuRwfyzMZ7Zt",
        "outputId": "a0af47ad-2208-42a0-e4e2-55c9597c7ae5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Silhouette Score: 0.7593905670910638\n",
            "Davies-Bouldin Index: 0.34583710789535893\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing Clustering Performance using textual synthetic data"
      ],
      "metadata": {
        "id": "UT1hmUY2b8Y7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas numpy openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MyAGOoEmcBdS",
        "outputId": "c696f2cc-0e68-408b-f285-e08486315103"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.1.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (0.28.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.5)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai) (3.10.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2024.7.4)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (2.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (4.0.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install openai==0.28"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zuqwM1_9eEr3",
        "outputId": "3f8d146f-0144-403a-cae3-25c8937f5bcd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai==0.28 in /usr/local/lib/python3.10/dist-packages (0.28.0)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (4.66.5)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (3.10.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (2024.7.4)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (2.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (4.0.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Set your OpenAI API key\n",
        "openai.api_key = ''"
      ],
      "metadata": {
        "id": "vTCeqxQycIjm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_synthetic_texts(num_samples, topics):\n",
        "    texts = []\n",
        "    samples_per_topic = num_samples // len(topics)\n",
        "\n",
        "    for topic in topics:\n",
        "        # Generate synthetic texts for each topic\n",
        "        for _ in range(samples_per_topic):\n",
        "            prompt = f\"Generate a short text related to the topic '{topic}'. Generate the text in such a way that a customer is describing about an issue that is related to that topic.\"\n",
        "            response = openai.ChatCompletion.create(\n",
        "                model='gpt-4o-mini',\n",
        "                messages=[{\"role\": \"user\", \"content\": prompt}]\n",
        "            )\n",
        "            texts.append(response.choices[0].message['content'].strip())\n",
        "\n",
        "    return texts"
      ],
      "metadata": {
        "id": "qFx53C-AcqXa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "topics = ['Software Upgrade Assistance', 'Database and Report Issues', 'Technical Issue', 'Email Delivery Problem', 'Financial Reporting Discrepancies']\n",
        "num_samples = 50\n",
        "\n",
        "# Generate synthetic texts\n",
        "synthetic_texts = generate_synthetic_texts(num_samples, topics)"
      ],
      "metadata": {
        "id": "GfZtulmTcybv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_embeddings(texts):\n",
        "    # Use OpenAI's API to get embeddings\n",
        "    responses = openai.Embedding.create(\n",
        "        model=\"text-embedding-3-small\",\n",
        "        input=texts\n",
        "    )\n",
        "    embeddings = [response['embedding'] for response in responses['data']]\n",
        "    return np.array(embeddings)"
      ],
      "metadata": {
        "id": "9yaEsnChiEMd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(synthetic_texts, columns=['text'])\n",
        "df['true_cluster'] = np.repeat(range(len(topics)), num_samples // len(topics))\n",
        "\n",
        "embeddings = get_embeddings(df['text'].tolist())"
      ],
      "metadata": {
        "id": "6h-MnsyVc46A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_clusters = len(topics)\n",
        "kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
        "df['cluster'] = kmeans.fit_predict(embeddings)"
      ],
      "metadata": {
        "id": "aGESP2YLdNrY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "silhouette_avg = silhouette_score(embeddings, df['cluster'])\n",
        "davies_bouldin = davies_bouldin_score(embeddings, df['cluster'])\n",
        "\n",
        "print(f'Silhouette Score: {silhouette_avg}')\n",
        "print(f'Davies-Bouldin Index: {davies_bouldin}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2qsavqWBdUSz",
        "outputId": "6a68a29a-7528-4994-ac03-b49cc8457da2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Silhouette Score: 0.4399515106478122\n",
            "Davies-Bouldin Index: 1.1030071312740273\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score, davies_bouldin_score\n",
        "\n",
        "openai.api_key = ''\n",
        "\n",
        "def generate_synthetic_texts(num_samples, topics):\n",
        "    texts = []\n",
        "    samples_per_topic = num_samples // len(topics)\n",
        "\n",
        "    for topic in topics:\n",
        "        # Generate synthetic texts for each topic\n",
        "        for _ in range(samples_per_topic):\n",
        "            prompt = f\"Generate a short text related to the topic '{topic}'. Generate the text in such a way that a customer is describing about an issue that is related to that topic.\"\n",
        "            response = openai.ChatCompletion.create(\n",
        "                model='gpt-4o-mini',\n",
        "                messages=[{\"role\": \"user\", \"content\": prompt}]\n",
        "            )\n",
        "            texts.append(response.choices[0].message['content'].strip())\n",
        "\n",
        "    return texts\n",
        "\n",
        "def get_embeddings(texts):\n",
        "    # Use OpenAI's API to get embeddings\n",
        "    responses = openai.Embedding.create(\n",
        "        model=\"text-embedding-3-small\",  # Use the desired embedding model\n",
        "        input=texts\n",
        "    )\n",
        "    embeddings = [response['embedding'] for response in responses['data']]\n",
        "    return np.array(embeddings)\n",
        "\n",
        "# Define Topics and Generate Data\n",
        "topics = ['Software Upgrade Assistance', 'Database and Report Issues', 'Technical Issue', 'Email Delivery Problem', 'Financial Reporting Discrepancies']\n",
        "num_samples = 50\n",
        "\n",
        "# Generate synthetic texts\n",
        "synthetic_texts = generate_synthetic_texts(num_samples, topics)\n",
        "\n",
        "# Create DataFrame\n",
        "df = pd.DataFrame(synthetic_texts, columns=['text'])\n",
        "df['true_cluster'] = np.repeat(range(len(topics)), num_samples // len(topics))\n",
        "\n",
        "# Get embeddings for the text data\n",
        "embeddings = get_embeddings(df['text'].tolist())\n",
        "\n",
        "# Cluster the Data Using KMeans\n",
        "num_clusters = len(topics)\n",
        "kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
        "df['cluster'] = kmeans.fit_predict(embeddings)\n",
        "\n",
        "# Evaluate Clustering Performance\n",
        "silhouette_avg = silhouette_score(embeddings, df['cluster'])\n",
        "davies_bouldin = davies_bouldin_score(embeddings, df['cluster'])\n",
        "\n",
        "print(f'Silhouette Score: {silhouette_avg}')\n",
        "print(f'Davies-Bouldin Index: {davies_bouldin}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LHEHOkJdgUTj",
        "outputId": "a44b966f-f989-4f8b-d1d5-c5f9c65c71ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Silhouette Score: 0.4273070698254832\n",
            "Davies-Bouldin Index: 1.1459411652485025\n",
            "                                                text  true_cluster  cluster\n",
            "0  Customer support plays a vital role in enhanci...             0        2\n",
            "1  Customer support is an essential component of ...             0        2\n",
            "2  Customer support is the backbone of any succes...             0        2\n",
            "3  Customer support plays a crucial role in ensur...             0        2\n",
            "4  Customer support is crucial for ensuring the s...             0        2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score, davies_bouldin_score\n",
        "\n",
        "openai.api_key = ''\n",
        "\n",
        "\n",
        "# Function to generate synthetic text data\n",
        "def generate_synthetic_texts(num_samples, topics):\n",
        "    texts = []\n",
        "    samples_per_topic = num_samples // len(topics)\n",
        "\n",
        "    for topic in topics:\n",
        "        # Generate synthetic texts for each topic\n",
        "        for _ in range(samples_per_topic):\n",
        "            prompt = f\"Generate a short text related to the topic '{topic}'. Generate the text in such a way that a customer is describing about an issue that is related to that topic.\"\n",
        "            response = openai.ChatCompletion.create(\n",
        "                model='gpt-4o-mini',\n",
        "                messages=[{\"role\": \"user\", \"content\": prompt}]\n",
        "            )\n",
        "            texts.append(response.choices[0].message['content'].strip())\n",
        "\n",
        "    return texts\n",
        "\n",
        "# Function to get embeddings from OpenAI\n",
        "def get_openai_embeddings(texts, model=\"text-embedding-3-small\"):\n",
        "    responses = openai.Embedding.create(\n",
        "        model=model,\n",
        "        input=texts\n",
        "    )\n",
        "    embeddings = [response['embedding'] for response in responses['data']]\n",
        "    return np.array(embeddings)\n",
        "\n",
        "# Function to evaluate clustering\n",
        "def evaluate_clustering(embeddings, num_clusters):\n",
        "    kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
        "    cluster_labels = kmeans.fit_predict(embeddings)\n",
        "    silhouette_avg = silhouette_score(embeddings, cluster_labels)\n",
        "    davies_bouldin = davies_bouldin_score(embeddings, cluster_labels)\n",
        "    return silhouette_avg, davies_bouldin\n",
        "\n",
        "topics = ['Software Upgrade Assistance', 'Database and Report Issues', 'Technical Issue', 'Email Delivery Problem', 'Financial Reporting Discrepancies']\n",
        "num_samples = 50\n",
        "synthetic_texts = generate_synthetic_texts(num_samples, topics)\n",
        "\n",
        "# Store results in a DataFrame\n",
        "results = pd.DataFrame(columns=['Embedding Method', 'Silhouette Score', 'Davies-Bouldin Index'])\n",
        "\n",
        "# Bag of Words Embedding\n",
        "bov_vectorizer = CountVectorizer(stop_words='english')\n",
        "bow_embeddings = bov_vectorizer.fit_transform(synthetic_texts).toarray()\n",
        "silhouette_avg, davies_bouldin = evaluate_clustering(bow_embeddings, len(topics))\n",
        "results = pd.concat([results, pd.DataFrame({'Embedding Method': 'Bag of Words',\n",
        "                           'Silhouette Score': silhouette_avg,\n",
        "                           'Davies-Bouldin Index': davies_bouldin}, index=[0])], ignore_index=True)\n",
        "\n",
        "# TF-IDF Embedding\n",
        "tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n",
        "tfidf_embeddings = tfidf_vectorizer.fit_transform(synthetic_texts).toarray()\n",
        "silhouette_avg, davies_bouldin = evaluate_clustering(tfidf_embeddings, len(topics))\n",
        "results = pd.concat([results, pd.DataFrame({'Embedding Method': 'TF-IDF',\n",
        "                           'Silhouette Score': silhouette_avg,\n",
        "                           'Davies-Bouldin Index': davies_bouldin}, index=[0])], ignore_index=True)\n",
        "\n",
        "# OpenAI Embeddings (text-embedding-3-small)\n",
        "openai_embeddings_small = get_openai_embeddings(synthetic_texts, model=\"text-embedding-3-small\")\n",
        "silhouette_avg, davies_bouldin = evaluate_clustering(openai_embeddings_small, len(topics))\n",
        "results = pd.concat([results, pd.DataFrame({'Embedding Method': 'OpenAI text-embedding-3-small',\n",
        "                           'Silhouette Score': silhouette_avg,\n",
        "                           'Davies-Bouldin Index': davies_bouldin}, index=[0])], ignore_index=True)\n",
        "\n",
        "\n",
        "# OpenAI Embeddings (text-embedding-3-large)\n",
        "openai_embeddings_large = get_openai_embeddings(synthetic_texts, model=\"text-embedding-3-large\")\n",
        "silhouette_avg, davies_bouldin = evaluate_clustering(openai_embeddings_large, len(topics))\n",
        "results = pd.concat([results, pd.DataFrame({'Embedding Method': 'OpenAI text-embedding-3-large',\n",
        "                           'Silhouette Score': silhouette_avg,\n",
        "                           'Davies-Bouldin Index': davies_bouldin}, index=[0])], ignore_index=True)\n",
        "\n",
        "\n",
        "# OpenAI Embeddings (text-embedding-ada-002)\n",
        "openai_embeddings_ada = get_openai_embeddings(synthetic_texts, model=\"text-embedding-ada-002\")\n",
        "silhouette_avg, davies_bouldin = evaluate_clustering(openai_embeddings_ada, len(topics))\n",
        "results = pd.concat([results, pd.DataFrame({'Embedding Method': 'OpenAI text-embedding-ada-002',\n",
        "                           'Silhouette Score': silhouette_avg,\n",
        "                           'Davies-Bouldin Index': davies_bouldin}, index=[0])], ignore_index=True)\n",
        "\n",
        "\n",
        "# Display the results\n",
        "print(results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pXUBD82Dih26",
        "outputId": "79654c61-cedd-465f-bb38-871300758a74"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                Embedding Method  Silhouette Score  Davies-Bouldin Index\n",
            "0                   Bag of Words          0.189550              1.669081\n",
            "1                         TF-IDF          0.133768              2.008565\n",
            "2  OpenAI text-embedding-3-small          0.323370              1.149484\n",
            "3  OpenAI text-embedding-3-large          0.372534              1.034745\n",
            "4  OpenAI text-embedding-ada-002          0.322881              1.143056\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def append_results(algorithm, silhouette_avg, davies_bouldin):\n",
        "    global results\n",
        "    new_row = {\n",
        "        'Algorithm': algorithm,\n",
        "        'Silhouette Score': silhouette_avg,\n",
        "        'Davies-Bouldin Index': davies_bouldin\n",
        "    }\n",
        "    results = pd.concat([results, pd.DataFrame(new_row, index=[0])], ignore_index=True) # Use concat instead of append\n",
        "    print(new_row)  # Print the results for the individual algorithm"
      ],
      "metadata": {
        "id": "BVDohIntDiZ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_kmeans(embeddings, num_clusters):\n",
        "    from sklearn.cluster import KMeans\n",
        "    kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
        "    labels = kmeans.fit_predict(embeddings)\n",
        "    silhouette_avg = silhouette_score(embeddings, labels)\n",
        "    davies_bouldin = davies_bouldin_score(embeddings, labels)\n",
        "    append_results('KMeans', silhouette_avg, davies_bouldin)\n",
        "\n",
        "def evaluate_dbscan(embeddings):\n",
        "    from sklearn.cluster import DBSCAN\n",
        "    dbscan = DBSCAN(eps=0.5, min_samples=5)\n",
        "    labels = dbscan.fit_predict(embeddings)\n",
        "    num_clusters = len(set(labels)) - (1 if -1 in labels else 0)  # Exclude noise\n",
        "    silhouette_avg = silhouette_score(embeddings, labels) if num_clusters > 1 else -1\n",
        "    davies_bouldin = davies_bouldin_score(embeddings, labels) if num_clusters > 1 else -1\n",
        "    append_results('DBSCAN', silhouette_avg, davies_bouldin)\n",
        "\n",
        "def evaluate_agglomerative(embeddings, num_clusters):\n",
        "    from sklearn.cluster import AgglomerativeClustering\n",
        "    agglomerative = AgglomerativeClustering(n_clusters=num_clusters)\n",
        "    labels = agglomerative.fit_predict(embeddings)\n",
        "    silhouette_avg = silhouette_score(embeddings, labels)\n",
        "    davies_bouldin = davies_bouldin_score(embeddings, labels)\n",
        "    append_results('Agglomerative', silhouette_avg, davies_bouldin)\n",
        "\n",
        "def evaluate_gmm(embeddings, num_clusters):\n",
        "    from sklearn.mixture import GaussianMixture\n",
        "    gmm = GaussianMixture(n_components=num_clusters)\n",
        "    labels = gmm.fit_predict(embeddings)\n",
        "    silhouette_avg = silhouette_score(embeddings, labels)\n",
        "    davies_bouldin = davies_bouldin_score(embeddings, labels)\n",
        "    append_results('Gaussian Mixture', silhouette_avg, davies_bouldin)\n",
        "\n",
        "def evaluate_affinity(embeddings):\n",
        "    from sklearn.cluster import AffinityPropagation\n",
        "    affinity = AffinityPropagation()\n",
        "    labels = affinity.fit_predict(embeddings)\n",
        "    silhouette_avg = silhouette_score(embeddings, labels)\n",
        "    davies_bouldin = davies_bouldin_score(embeddings, labels)\n",
        "    append_results('Affinity Propagation', silhouette_avg, davies_bouldin)\n",
        "\n",
        "def evaluate_spectral(embeddings, num_clusters):\n",
        "    from sklearn.cluster import SpectralClustering\n",
        "    spectral = SpectralClustering(n_clusters=num_clusters, affinity='nearest_neighbors')\n",
        "    labels = spectral.fit_predict(embeddings)\n",
        "    silhouette_avg = silhouette_score(embeddings, labels)\n",
        "    davies_bouldin = davies_bouldin_score(embeddings, labels)\n",
        "    append_results('Spectral Clustering', silhouette_avg, davies_bouldin)"
      ],
      "metadata": {
        "id": "CCT8f2VhCNwR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate each algorithm\n",
        "\n",
        "evaluate_kmeans(embeddings, len(topics))\n",
        "evaluate_dbscan(embeddings)\n",
        "evaluate_agglomerative(embeddings, len(topics))\n",
        "evaluate_gmm(embeddings, len(topics))\n",
        "evaluate_affinity(embeddings)\n",
        "evaluate_spectral(embeddings, len(topics))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KzaQaUUiDvBL",
        "outputId": "f9ca54a0-5c06-4743-c282-76fe90954388"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'Algorithm': 'KMeans', 'Silhouette Score': 0.4273070698254832, 'Davies-Bouldin Index': 1.1459411652485025}\n",
            "{'Algorithm': 'DBSCAN', 'Silhouette Score': 0.2695234602830947, 'Davies-Bouldin Index': 1.4207716851993024}\n",
            "{'Algorithm': 'Agglomerative', 'Silhouette Score': 0.4273070698254832, 'Davies-Bouldin Index': 1.1459411652485023}\n",
            "{'Algorithm': 'Gaussian Mixture', 'Silhouette Score': 0.4273070698254832, 'Davies-Bouldin Index': 1.1459411652485023}\n",
            "{'Algorithm': 'Affinity Propagation', 'Silhouette Score': 0.45128870630634693, 'Davies-Bouldin Index': 1.0313267068679823}\n",
            "{'Algorithm': 'Spectral Clustering', 'Silhouette Score': 0.4102546921658252, 'Davies-Bouldin Index': 1.176047817519145}\n",
            "Final Results:\n",
            "               Algorithm  Silhouette Score  Davies-Bouldin Index\n",
            "0                 kmeans          0.427307              1.145941\n",
            "1                 dbscan          0.269523              1.420772\n",
            "2          agglomerative          0.427307              1.145941\n",
            "3                    gmm          0.410255              1.176048\n",
            "4               affinity          0.451289              1.031327\n",
            "5                 KMeans          0.427307              1.145941\n",
            "6                 DBSCAN          0.269523              1.420772\n",
            "7          Agglomerative          0.427307              1.145941\n",
            "8       Gaussian Mixture          0.427307              1.145941\n",
            "9   Affinity Propagation          0.451289              1.031327\n",
            "10                KMeans          0.427307              1.145941\n",
            "11                DBSCAN          0.269523              1.420772\n",
            "12         Agglomerative          0.427307              1.145941\n",
            "13      Gaussian Mixture          0.408802              1.167911\n",
            "14  Affinity Propagation          0.451289              1.031327\n",
            "15                KMeans          0.427307              1.145941\n",
            "16                DBSCAN          0.269523              1.420772\n",
            "17         Agglomerative          0.427307              1.145941\n",
            "18      Gaussian Mixture          0.427307              1.145941\n",
            "19  Affinity Propagation          0.451289              1.031327\n",
            "20   Spectral Clustering          0.410255              1.176048\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the results DataFrame\n",
        "print(\"Final Results:\")\n",
        "print(results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "obEg5udiD1Rg",
        "outputId": "fa33f837-f048-4961-bcdc-b9aee4e840ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Results:\n",
            "               Algorithm  Silhouette Score  Davies-Bouldin Index\n",
            "0                 kmeans          0.427307              1.145941\n",
            "1                 dbscan          0.269523              1.420772\n",
            "2          agglomerative          0.427307              1.145941\n",
            "3                    gmm          0.410255              1.176048\n",
            "4               affinity          0.451289              1.031327\n",
            "5                 KMeans          0.427307              1.145941\n",
            "6                 DBSCAN          0.269523              1.420772\n",
            "7          Agglomerative          0.427307              1.145941\n",
            "8       Gaussian Mixture          0.427307              1.145941\n",
            "9   Affinity Propagation          0.451289              1.031327\n",
            "10                KMeans          0.427307              1.145941\n",
            "11                DBSCAN          0.269523              1.420772\n",
            "12         Agglomerative          0.427307              1.145941\n",
            "13      Gaussian Mixture          0.408802              1.167911\n",
            "14  Affinity Propagation          0.451289              1.031327\n",
            "15                KMeans          0.427307              1.145941\n",
            "16                DBSCAN          0.269523              1.420772\n",
            "17         Agglomerative          0.427307              1.145941\n",
            "18      Gaussian Mixture          0.427307              1.145941\n",
            "19  Affinity Propagation          0.451289              1.031327\n",
            "20   Spectral Clustering          0.410255              1.176048\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HMjVcQaXEW4-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}